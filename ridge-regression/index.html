<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/blog/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/blog/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav ul li a { background-image: none; } .site-nav > ul.nav-category-list > li > button svg { transform: rotate(-90deg); } .site-nav > ul.nav-category-list > li.nav-list-item > ul.nav-list { display: block; } </style> <script src="/blog/assets/js/vendor/lunr.min.js"></script> <script src="/blog/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="/blog/assets/icons/favicon.ico" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Ridge regression | ML Blog</title> <meta name="generator" content="Jekyll v4.3.3" /> <meta property="og:title" content="Ridge regression" /> <meta name="author" content="Marko Lalovic" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="The ordinary least squares (OLS) method is not suitable to estimate the unknown parameters $\beta$ in the case of highly correlated regressors. As the correlation between regressors in $X$ increases the OLS method becomes unstable. In the limit $|corr(x_{i}, x_{j})| \rightarrow 1$, the OLS objective function is no longer strictly convex and there are infinitely many solutions of OLS problem. The matrix $X$ becomes singular and both the variance of the estimator and the distance of the estimator to the actual $\beta$ go to infinity. (See Multicollinearity effect on OLS regression for more details.)" /> <meta property="og:description" content="The ordinary least squares (OLS) method is not suitable to estimate the unknown parameters $\beta$ in the case of highly correlated regressors. As the correlation between regressors in $X$ increases the OLS method becomes unstable. In the limit $|corr(x_{i}, x_{j})| \rightarrow 1$, the OLS objective function is no longer strictly convex and there are infinitely many solutions of OLS problem. The matrix $X$ becomes singular and both the variance of the estimator and the distance of the estimator to the actual $\beta$ go to infinity. (See Multicollinearity effect on OLS regression for more details.)" /> <link rel="canonical" href="http://localhost:4000/blog/ridge-regression/" /> <meta property="og:url" content="http://localhost:4000/blog/ridge-regression/" /> <meta property="og:site_name" content="ML Blog" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2019-08-01T00:00:00+02:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Ridge regression" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Marko Lalovic"},"dateModified":"2019-08-01T00:00:00+02:00","datePublished":"2019-08-01T00:00:00+02:00","description":"The ordinary least squares (OLS) method is not suitable to estimate the unknown parameters $\\beta$ in the case of highly correlated regressors. As the correlation between regressors in $X$ increases the OLS method becomes unstable. In the limit $|corr(x_{i}, x_{j})| \\rightarrow 1$, the OLS objective function is no longer strictly convex and there are infinitely many solutions of OLS problem. The matrix $X$ becomes singular and both the variance of the estimator and the distance of the estimator to the actual $\\beta$ go to infinity. (See Multicollinearity effect on OLS regression for more details.)","headline":"Ridge regression","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/ridge-regression/"},"url":"http://localhost:4000/blog/ridge-regression/"}</script> <!-- End Jekyll SEO tag --> <!-- Load dark color theme --> <link id="dark-theme" rel="stylesheet" href="/blog/assets/css/just-the-docs-dark.css" media="(prefers-color-scheme: dark)" /> <!-- Fonts: post-font-family: "Noto Sans" heading-font-family: "Lato" mono-font-family: "Fira Code" --> <link rel="preconnect" href="https://fonts.googleapis.com" /> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /> <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet"> <!-- MathJax config --> <script type="text/javascript"> window.MathJax = { tex: { packages: ["base", "ams", "newcommand", "boldsymbol"], inlineMath: [ ["$", "$"], ["\\(", "\\)"], ], tags: "ams" }, loader: { load: ["ui/menu", "[tex]/ams", "[tex]/newcommand", "[tex]/boldsymbol"], versionWarnings: false, }, }; </script> <!-- MathJax snippet --> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" ></script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div id="side-bar-component" class="side-bar"> <div class="site-header" role="banner"> <a href="/blog/" class="site-title lh-tight"> <p class="my-site-title">ML Blog</p> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"> <!-- Added `reversed` so that posts are sorted by date in reverse --><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --> <a href="/blog/" class="nav-list-link fs-5">Index</a><!-- changed typography: fw-400 fs-4 --></li></ul> <!-- Don't show my Posts collection name --><ul class="nav-list"> <!-- Added `reversed` so that posts are sorted by date in reverse --><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/max-cut-sdp/" class="nav-list-link fs-3 before-pseudo-class">Maximum cut and Goemans-Williamson</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/layout-optimization/" class="nav-list-link fs-3 before-pseudo-class">Layout optimization for a solar tower power plant</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/lasso-dual/" class="nav-list-link fs-3 before-pseudo-class">Lasso dual</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/tda-digits/" class="nav-list-link fs-3 before-pseudo-class">Topological features applied to the MNIST data set</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/water-towers/" class="nav-list-link fs-3 before-pseudo-class">Some math behind water towers</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/covid-calc/" class="nav-list-link fs-3 before-pseudo-class">Covid Calculator</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/direct-approach-to-fdr/" class="nav-list-link fs-3 before-pseudo-class">On Storey's direct approach to false discovery rates</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/ridge-regression/" class="nav-list-link fs-3 before-pseudo-class">Ridge regression</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/ols-regression/" class="nav-list-link fs-3 before-pseudo-class">Multicollinearity effect on OLS regression</a></li></ul> </nav> <footer class="site-footer"></footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search ML Blog" aria-label="Search ML Blog" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <button id="toggle-theme-button" name="Swich Theme" class="toggle-btn site-button fw-500 fs-5" onclick="toggleTheme()"> <!-- Sun --> <svg width="24" height="24" id="light-icon" display="none"> <g transform="translate(0,1) scale( 0.9 )"> <circle cx="12" cy="12" r="6" fill="rgb(110, 110, 110)" /> <line id="ray" stroke="rgb(110, 110, 110)" stroke-width="2" stroke-linecap="round" x1="12" y1="1" x2="12" y2="3.3" ></line> <!-- rest of rays are just rotated top ray --> <use href="#ray" transform="rotate(45 12 12)" /> <use href="#ray" transform="rotate(90 12 12)" /> <use href="#ray" transform="rotate(135 12 12)" /> <use href="#ray" transform="rotate(180 12 12)" /> <use href="#ray" transform="rotate(225 12 12)" /> <use href="#ray" transform="rotate(270 12 12)" /> <use href="#ray" transform="rotate(315 12 12)" /> </g> </svg> <!-- Moon --> <svg width="24" height="24" id="dark-icon" display="none"> <g transform="translate(0,1) scale( 0.9 )"> <path fill="rgb(110, 110, 110)" d=" M 18, 4 A 10 10 0 1 0 18, 20 A 10 10 0 0 1 18, 4" /> </g> </svg> </button> </li> <li class="aux-nav-list-item"> <a href="https://lalovic.io" class="site-button fw-500 fs-5"> About </a> </li> </ul> </nav> <!-- Light and dark theme toggle --> <script> var currentTheme = ""; const toggleThemeButton = document.getElementById("toggle-theme-button"); const lightIcon = document.getElementById("light-icon"); const darkIcon = document.getElementById("dark-icon"); const defaultTheme = document.getElementsByTagName("link")[0]; const lightThemeHTML = '<link id="light-theme" rel="stylesheet" href="/blog/assets/css/just-the-docs-light.css"/>'; const darkThemeHTML = '<link id="dark-theme" rel="stylesheet" href="/blog/assets/css/just-the-docs-dark.css"/>'; if ( window.matchMedia && window.matchMedia("(prefers-color-scheme: dark)").matches ) { currentTheme = "dark"; toggleThemeButton.setAttribute("title", "Switch to light mode"); darkIcon.setAttribute("display", "none"); lightIcon.setAttribute("display", "block"); } else { currentTheme = "light"; toggleThemeButton.setAttribute("title", "Switch to dark mode"); darkIcon.setAttribute("display", "block"); lightIcon.setAttribute("display", "none"); } function removeBothThemes() { const lightTheme = document.getElementById("light-theme"); const darkTheme = document.getElementById("dark-theme"); if (lightTheme !== null) { lightTheme.remove(); } if (darkTheme !== null) { darkTheme.remove(); } } function darkOn() { removeBothThemes(); defaultTheme.insertAdjacentHTML("afterend", darkThemeHTML); currentTheme = "dark"; localStorage.setItem("currentThemeLS", "dark"); toggleThemeButton.setAttribute("title", "Switch to light mode"); lightIcon.setAttribute("display", "block"); darkIcon.setAttribute("display", "none"); } function lightOn() { removeBothThemes(); defaultTheme.insertAdjacentHTML("afterend", lightThemeHTML); currentTheme = "light"; localStorage.setItem("currentThemeLS", "light"); toggleThemeButton.setAttribute("title", "Switch to dark mode"); lightIcon.setAttribute("display", "none"); darkIcon.setAttribute("display", "block"); } function toggleTheme() { if (currentTheme == "dark") { lightOn(); } else { darkOn(); } } if (localStorage.getItem("currentThemeLS") !== null) { const currentThemeLS = localStorage.getItem("currentThemeLS"); if (currentThemeLS !== currentTheme) { if (currentThemeLS == "dark") { darkOn(); } else { lightOn(); } } } </script> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 class="fs-7">Ridge regression</h1> <p class="my-post-meta"> Marko Lalovic &nbsp; Aug 1, 2019 </p> <p class="post-content"> <strong>TLDR:</strong> Derivation of ridge regression estimator and it's properties, the role of the penalty function and the analysis of regularization parameter. </p> <div class="post-content"> <p>The ordinary least squares (OLS) method is not suitable to estimate the unknown parameters $\beta$ in the case of highly correlated regressors. As the correlation between regressors in $X$ increases the OLS method becomes unstable. In the limit $|corr(x_{i}, x_{j})| \rightarrow 1$, the OLS objective function is no longer strictly convex and there are infinitely many solutions of OLS problem. The matrix $X$ becomes singular and both the variance of the estimator and the distance of the estimator to the actual $\beta$ go to infinity. (See <a href="/blog/ols-regression">Multicollinearity effect on OLS regression</a> for more details.)</p> <p>Ridge regression is an effective approach to solve such problems. Regardless of data $X$, unique solution to ridge regression always exists. By adding the ridge (vector of $\alpha$’s) on the diagonal of $X$, the ridge regression produces stable estimates of the coefficients in $\beta$ (<a href="#figure1">Figure 1</a>).</p> <div id="figure1" class="imgcap-nbd"> <img src="/blog/assets/posts/ridge-regression/problem.svg" width="100%" class="invertImg" /> <div class="thecap"> Figure 1: As the correlation between regressors increases the OLS method becomes unstable while the ridge regression method produces stable estimates. </div> </div> <p>In this blog post, penalized regression is introduced, and the ridge regression estimator is derived. A simple example in R is used to illustrate the method. After that, some properties of the estimator are outlined, and the role of the penalty function is explained. Finally, an analysis of the regularization parameter $\alpha$ is performed.</p> <h2 class="no_toc text-delta" id="table-of-contents">Table of contents</h2> <ol id="markdown-toc"> <li><a href="#penalized-regression" id="markdown-toc-penalized-regression">Penalized regression</a></li> <li><a href="#derivation-of-ridge-regression-estimator" id="markdown-toc-derivation-of-ridge-regression-estimator">Derivation of ridge regression estimator</a> <ol> <li><a href="#example" id="markdown-toc-example">Example</a></li> </ol> </li> <li><a href="#properties-of-ridge-regression-estimator" id="markdown-toc-properties-of-ridge-regression-estimator">Properties of ridge regression estimator</a></li> <li><a href="#the-role-of-the-penalty-function" id="markdown-toc-the-role-of-the-penalty-function">The role of the penalty function</a></li> <li><a href="#regularization-parameter-alpha" id="markdown-toc-regularization-parameter-alpha">Regularization parameter $\alpha$</a></li> <li><a href="#references" id="markdown-toc-references">References</a></li> </ol> <h2 id="penalized-regression">Penalized regression</h2> <p>In penalized regression, for $n &gt; p$ and given $X: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$ and $y \in \mathbb{R}^{n}$, we minimize the functional</p> \[\newcommand{\norm}[1]{\left\lVert#1\right\rVert} J_{\alpha}(\beta) = \norm{ y - X\beta }_{2}^{2} + \alpha P(\beta)\] <p>over $\beta \in \mathbb{R}^{p}$, where:</p> <ul> <li>$J_{\alpha}: \mathbb{R}^{p} \rightarrow \mathbb{R}$ is the objective function;</li> <li>$P: \mathbb{R}^{p} \rightarrow \mathbb{R}$ is a penalty function that penalizes unrealistic values in $\beta$;</li> <li>parameter $\alpha &gt; 0$ controls the trade-off between the penalty and the fit of the loss function.</li> </ul> <p>There are many different possibilities for the penalty function $P$. The main idea that determines the choice of the penalty function is that we would prefer a simple model to a more complex one. For example, if we want a smoother fit, then the penalty function to consider is a measure of the curvature. In the case of correlated regressors, the estimated coefficients can become too large and $P$ is a measure of the distance of the coefficients from the origin. In this case, the main penalty function to consider is</p> \[P(\beta) = \norm{\beta}_{2}^{2}.\] <p>This type of penalized regression is called <em>ridge regression</em> introduced in <a class="citation" href="#Hoerl1970">(Hoerl &amp; Kennard, 1970)</a>. It belongs to a more general type of regularization known as Tikhonov regularization <a class="citation" href="#Tikhonov1977">(Tikhonov &amp; Arsenin, 1977)</a> where for some suitably chosen matrix $T: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$, the penalty function is</p> \[P(\beta) = \norm{T \beta}_{2}^{2}.\] <h2 id="derivation-of-ridge-regression-estimator">Derivation of ridge regression estimator</h2> <p>In order to simplify the derivation, assume that $X: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$ is linear and continuous with full column rank $p$. The objective function we want to minimize, written in a matrix form, is</p> \[\begin{align*} \norm{ y - X \beta }_{2}^{2} + \alpha \norm{\beta}_{2}^2 &amp;= (y - X \beta)^{T} (y - X \beta) + \alpha \beta^{T} \beta \nonumber \\ &amp;= y^{T}y - 2 y^{T} X \beta + \beta^{T} X^{T}X \beta + \alpha \beta^{T} \beta \end{align*}\] <p>Taking a partial derivative with respect to $\beta$ and setting it to zero</p> \[-2 X^{T} y + 2 X^{T} X \hat{\beta} + 2 \alpha \hat{\beta} = 0\] <p>gives a regularized normal equation</p> \[(X^{T}X + \alpha I) \hat{\beta} = X^{T}y\] <p>and express $\hat{\beta}$ as</p> \[\hat{\beta} = (X^{T}X + \alpha I)^{-1} X^{T} y.\] <p>Since $\text{rank}(X) = p$</p> \[X z \neq 0 \quad \text{for each} \quad z \neq 0\] <p>For the Hessian</p> \[2X^{T}X + 2 \alpha\] <p>it holds that</p> \[\begin{align*} 2 z^{T} X^{T} X z + 2 \alpha z^{T} z &amp;= 2 (Xz)^{T} (Xz) + 2 \alpha z^{T} z \nonumber \\[1em] &amp;= 2 \norm{Xz}_{2}^{2} + 2 \alpha \norm{z}_{2}^{2} &gt; 0 \quad \text{for all} \quad z \neq 0 \end{align*}\] <p>Therefore, the expressed $\hat{\beta}$ is an estimator, denoted by:</p> \[\begin{equation} \hat{\beta}_{\text{RR}} = (X^{T}X + \alpha I)^{-1} X^{T} y. \label{eq: rr} \end{equation}\] <h4 id="example">Example</h4> <p>We can demonstrate how the ridge regression method estimates the unknown parameters $\beta$ in the case of correlated regressors on a simple example using R. Suppose, we have the following model</p> \[y \sim \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2}.\] <p>More specifically, let</p> \[\beta_{0} = 3, \quad \beta_{1} = \beta_{2} = 1.\] <p>and let the sample contain 100 elements</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span><span class="w">
</span></code></pre></div></div> <p>Then, introduce some highly correlated regressors</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">x1</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w">
</span><span class="n">x2</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>with correlation coefficient almost 1</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">0.999962365268769</span><span class="w">
</span></code></pre></div></div> <p>into the model</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>and calculate the estimate $\hat{\beta}_{\text{RR}}$ for $\alpha = 0.3$</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alpha</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.3</span><span class="w">
</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">))</span><span class="w">

</span><span class="n">beta.ridge</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">xx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">solve</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="m">3</span><span class="p">))</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">as.vector</span><span class="p">(</span><span class="n">xx</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">y</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">beta.ridge</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">2.98537494896842</span><span class="w"> </span><span class="m">0.815120466450887</span><span class="w"> </span><span class="m">1.04146900239714</span><span class="w">
</span></code></pre></div></div> <h2 id="properties-of-ridge-regression-estimator">Properties of ridge regression estimator</h2> <p>The unique solution to \ref{eq: rr} of ridge regression estimator $\hat{\beta}_{\text{RR}}$ always exists, since $X^{T}X + \alpha I$ is always rank $p$.</p> <p>Let’s derive the relationship between ridge and OLS estimators for the case when matrix $X$ is orthogonal. Using $X^{T}X = I$ twice and since $\hat{\beta}_{\text{OLS}} = (X^{T}X)^{-1} X^{T} y$, we get</p> \[\begin{align*} \hat{\beta}_{\text{RR}} &amp;= (X^{T}X + \alpha I)^{-1} X^{T} y \nonumber \\[1em] &amp;= (I + \alpha I)^{-1} X^{T} y \nonumber \\[1em] &amp;= (1 + \alpha)^{-1} I X^{T} y \nonumber \\[1em] &amp;= (1 + \alpha)^{-1} (X^{T}X)^{-1} X^{T} y \nonumber \\[1em] &amp;= (1 + \alpha)^{-1} \hat{\beta}_{\text{OLS}} \end{align*}\] <p>Ridge regression estimator $\hat{\beta}_{\text{RR}}$ is biased since, for any value of $\alpha &gt; 0$, its expected value is not equal to $\beta$:</p> \[\begin{align*} \mathbb{E}[\hat{\beta}_{ridge}] &amp;= \mathbb{E}[(X^{T}X + \alpha I)^{-1} X^{T} y] \nonumber \\[1em] &amp;= \mathbb{E}[(X^{T}X + \alpha I)^{-1} (X^{T}X) (X^{T}X)^{-1} X^{T} y] \nonumber \\[1em] &amp;= \mathbb{E}[(X^{T}X + \alpha I)^{-1} (X^{T}X) \hat{\beta}_{\text{OLS}}] \nonumber \\[1em] &amp;= (X^{T}X + \alpha I)^{-1} (X^{T}X) \mathbb{E}[\hat{\beta}_{\text{OLS}}] \nonumber \\[1em] &amp;= (X^{T}X + \alpha I)^{-1} (X^{T}X) \beta. \end{align*}\] <p>As $\alpha \rightarrow 0$, ridge estimator tends to OLS estimator. This can be seen from</p> \[\begin{align*} \lim_{\alpha \to 0} \hat{\beta}_{\text{RR}} &amp;= \lim_{\alpha \to 0} (X^{T}X + \alpha I)^{-1} (X^{T}X) \hat{\beta}_{\text{OLS}} \nonumber \\[1em] &amp;= (X^{T}X)^{-1} (X^{T}X) \hat{\beta}_{\text{OLS}} \nonumber \\[1em] &amp;= \hat{\beta}_{\text{OLS}}. \end{align*}\] <h2 id="the-role-of-the-penalty-function">The role of the penalty function</h2> <p>The role of the penalty function can be shown conveniently using singular value decomposition. Let</p> \[X = U \Sigma V^{T}\] <p>be the singular value decomposition of $X$ where $\Sigma$ contains all the singular values</p> \[\sigma_{1} \geq \sigma_{2} \geq \dots \geq \sigma_{p} &gt; 0.\] <p>The regularized normal equation</p> \[( X^{T} X + \alpha I ) \hat{\beta} = X^{T} y\] <p>can be rewritten as</p> \[(V \Sigma^{T} U^{T}U \Sigma V^{T} + \alpha I) \hat{\beta} = V \Sigma^{T} U^{T} y\] <p>Since $U^{T}U = I$ and $V^{T}V = I$, we have</p> \[(V \Sigma^{T} \Sigma V^{T} + \alpha V^{T}V) \hat{\beta} = V (\Sigma^{T} \Sigma + \alpha I) V^{T} \hat{\beta} = V \Sigma^{T} U^{T} y\] <p>Multiplying by $V^{T}$ from the left and setting $z = V^{T} \hat{\beta}$, we get</p> \[(\Sigma^{T} \Sigma + \alpha I) z = \Sigma^{T} U^{T} y\] <p>Therefore</p> \[z_{i} = \frac{\sigma_{i} (u_{i}^{T} y)}{\sigma_{i}^{2} + \alpha} \quad \text{for} \quad i = 1, \dots, p\] <p>For minimum norm solution, let</p> \[z_{i} = 0 \quad \text{for} \quad i = p + 1, \dots, n.\] <p>Finally, from $\hat{\beta} = V z$ and since $V$ is orthogonal</p> \[\norm{\hat{\beta}} = \norm{VV^{T} \hat{\beta}} = \norm{V^{T}\hat{\beta}} = \norm{z}\] <p>we get</p> \[\begin{equation} \hat{\beta}_{i} = \frac{\sigma_{i} (u_{i}^{T} y)}{\sigma_{i}^{2} + \alpha} v_{i}. \label{eq: beta_i} \end{equation}\] <p>From</p> \[\begin{equation} \hat{\beta}_{i} \approx \begin{cases} 0, &amp; \text{if } \sigma_{i} &lt;&lt; \alpha \\ \frac{u_{i}^{T} y}{\sigma_{i}}v_{i}, &amp; \text{if } \sigma_{i} &gt;&gt; \alpha \end{cases} \end{equation}\] <p>it can be seen that the penalty function $\alpha \norm{\beta}_{2}^{2}$ acts as a filter since the contributions</p> <ul> <li>from $\sigma_{i}$ that is small relative to the regularization parameter $\alpha$ are almost eliminated;</li> <li>from $\sigma_{i}$ that is large relative to the regularization parameter $\alpha$ are left almost unchanged.</li> </ul> <p>By defining a filter</p> \[\begin{equation} F_{\alpha}(\xi) = \frac{1}{\xi + \alpha} \end{equation}\] <p>the solution of ridge regression can be expressed as</p> \[\hat{\beta}_{\text{RR}} = F_{\alpha}(X^{T}X) X^{T}y.\] <h2 id="regularization-parameter-alpha">Regularization parameter $\alpha$</h2> <p>The solution of ridge regression is monotonically decreasing in $\alpha$. To see this, let</p> \[\psi(\alpha) = \norm{\hat{\beta}_{\text{RR}}}_{2}^{2}.\] <p>From derived equation for $\hat{\beta}_{i}$ in \ref{eq: beta_i}, we have that</p> \[\psi(\alpha) = \sum_{i = 1}^{p} \frac{\sigma_{i}^{2} (u_{i}^{T} y)^{2}}{ (\sigma_{i}^{2} + \alpha)^{2} } v_{i}^{2}\] <p>and the first derivative</p> \[\psi'(\alpha) = -2 \sum_{i = 1}^{p} \frac{\sigma_{i}^{2} (u_{i}^{T} y)^{2}}{ (\sigma_{i}^{2} + \alpha)^{3} } v_{i}^{2} &lt; 0.\] <p>As $\alpha \rightarrow \infty$ the solution of ridge regression goes to 0</p> \[\lim_{\alpha \rightarrow \infty} \psi(\alpha) = \lim_{\alpha \rightarrow \infty} \sum_{i = 1}^{p} \frac{\sigma_{i}^{2} (u_{i}^{T} y)^{2}}{ (\sigma_{i}^{2} + \alpha)^{2} } v_{i}^{2} = 0\] <p>In the limit $\alpha \rightarrow 0$, the solution of ridge regression goes to ordinary least squares solution. Furthermore, if $\sigma_{p} \rightarrow 0$ where $X$ is no longer full column rank, then $\psi(\alpha) \rightarrow \infty$.</p> <p><a href="#figure2">Figure 2</a> shows how the estimates $\beta_{0}, \beta_{1}$ and $\beta_{2}$ change depending on the value of parameter $\alpha$ for the data from <a href="#example">Example</a> above.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alphas</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="n">seq</span><span class="p">(</span><span class="m">-5</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">0.1</span><span class="p">))</span><span class="w">
</span><span class="n">betas</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">beta.ridge</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w">
</span><span class="p">})</span><span class="w">

</span><span class="n">library</span><span class="p">(</span><span class="n">latex2exp</span><span class="p">)</span><span class="w"> </span><span class="c1"># for annotation</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">alphas</span><span class="p">),</span><span class="w"> </span><span class="n">betas</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="p">],</span><span class="w">
     </span><span class="n">type</span><span class="o">=</span><span class="s2">"l"</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">,</span><span class="w">
     </span><span class="n">xlab</span><span class="o">=</span><span class="n">TeX</span><span class="p">(</span><span class="n">r</span><span class="s1">'($\log(\alpha)$)'</span><span class="p">),</span><span class="w">
     </span><span class="n">ylab</span><span class="o">=</span><span class="n">TeX</span><span class="p">(</span><span class="n">r</span><span class="s1">'($\hat{\beta}$)'</span><span class="p">),</span><span class="w">
     </span><span class="n">cex.lab</span><span class="o">=</span><span class="m">1.5</span><span class="p">,</span><span class="w"> </span><span class="n">cex.axis</span><span class="o">=</span><span class="m">1.5</span><span class="p">,</span><span class="w"> </span><span class="n">cex.main</span><span class="o">=</span><span class="m">1.5</span><span class="p">,</span><span class="w"> </span><span class="n">cex.sub</span><span class="o">=</span><span class="m">1.5</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">alphas</span><span class="p">),</span><span class="w"> </span><span class="n">betas</span><span class="p">[</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="p">],</span><span class="w">
      </span><span class="n">type</span><span class="o">=</span><span class="s2">"l"</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">alphas</span><span class="p">),</span><span class="w"> </span><span class="n">betas</span><span class="p">[</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="p">],</span><span class="w">
      </span><span class="n">type</span><span class="o">=</span><span class="s2">"l"</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span><span class="w">
</span><span class="n">legend</span><span class="p">(</span><span class="m">7.73</span><span class="p">,</span><span class="w"> </span><span class="m">3.12</span><span class="p">,</span><span class="w">
       </span><span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="w">
           </span><span class="n">TeX</span><span class="p">(</span><span class="n">r</span><span class="s1">'($\hat{\beta}_{1}(\alpha)$)'</span><span class="p">),</span><span class="w">
           </span><span class="n">TeX</span><span class="p">(</span><span class="n">r</span><span class="s1">'($\hat{\beta}_{2}(\alpha)$)'</span><span class="p">),</span><span class="w">
           </span><span class="n">TeX</span><span class="p">(</span><span class="n">r</span><span class="s1">'($\hat{\beta}_{3}(\alpha)$)'</span><span class="p">)),</span><span class="w">
       </span><span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"red"</span><span class="p">,</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">,</span><span class="w"> </span><span class="s2">"black"</span><span class="p">),</span><span class="w">
       </span><span class="n">lwd</span><span class="o">=</span><span class="nf">rep</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">3</span><span class="p">),</span><span class="w"> </span><span class="n">lty</span><span class="o">=</span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">cex</span><span class="o">=</span><span class="m">1.5</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <div id="figure2" class="imgcap-nbd"> <img src="/blog/assets/posts/ridge-regression/ridge-solution-path.svg" width="80%" class="invertImg" /> <div class="thecap"> Figure 2: The solution of ridge regression as a function of the regularization parameter $\alpha$. </div> </div> <p>The selection of $\alpha$ is usually done by cross-validation as follows. First, randomly partition the data into $K$ equally sized sets. For some value of $\alpha$, build a model (calculate estimates for the coefficients) on the data from $K - 1$ sets (learning set) and test it on the rest of the data (test set) by calculating the mean square error (MSE). Then, repeat this process for the remaining values of $\alpha$ and select the value of $\alpha$ with the smallest MSE. Typical values for $K$ are $5, 10$, and $n$ (sample size).</p> <p>Let’s find the optimal value of parameter $\alpha$ for the data in <a href="#example">Example</a> using 10-fold cross-validation:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">K</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="n">folds</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cut</span><span class="p">(</span><span class="n">seq</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span><span class="w"> </span><span class="n">breaks</span><span class="o">=</span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">labels</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">cv.matrix</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="o">=</span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="o">=</span><span class="nf">length</span><span class="p">(</span><span class="n">alphas</span><span class="p">))</span><span class="w">

</span><span class="n">mse</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="nf">length</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">sum</span><span class="p">((</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">test.i</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">which</span><span class="p">(</span><span class="n">folds</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">k</span><span class="p">)</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">j</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">alphas</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">br</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">beta.ridge</span><span class="p">(</span><span class="n">alphas</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="n">test.i</span><span class="p">,</span><span class="w"> </span><span class="p">],</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="n">test.i</span><span class="p">])</span><span class="w">
        </span><span class="n">cv.matrix</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mse</span><span class="p">(</span><span class="n">br</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">test.i</span><span class="p">,</span><span class="w"> </span><span class="p">],</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">test.i</span><span class="p">])</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">avgs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">cv.matrix</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">)</span><span class="w">
</span><span class="n">best.alpha</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">alphas</span><span class="p">[</span><span class="n">avgs</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">avgs</span><span class="p">)]</span><span class="w">
</span><span class="n">best.alpha</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">0.246596963941606</span><span class="w">
</span></code></pre></div></div> <h2 id="references">References</h2> <ol class="bibliography"><li><span id="Hoerl1970">Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. <i>Technometrics</i>, <i>12</i>(1), 55–67. https://doi.org/<a href="https://doi.org/10.1080/00401706.1970.10488634">10.1080/00401706.1970.10488634</a></span></li> <li><span id="Tikhonov1977">Tikhonov, A. N., &amp; Arsenin, V. Y. (1977). <i>Solutions of ill-posed problems</i> (p. xiii+258). V. H. Winston \&amp; Sons. <a href="https://catalogue.nla.gov.au/catalog/720231">https://catalogue.nla.gov.au/catalog/720231</a></span></li></ol> </div> <div> <section class="comments"> <!-- Add comment form --> <div id="respond" class="comment-new"> <h2>Leave a comment</h2> <form id="input-form" method="post" action="https://api.staticforms.xyz/submit"> <input type="hidden" name="accessKey" value="4e5a5ee1-5fd9-47ed-9b85-aecdabd0c006"> <input type="hidden" name="$Date" value="1716071731"> <input type="hidden" name="$BlogPost" value="Ridge regression"> <input type="hidden" name="subject" value="Comment to blog post Sun, 19 May 2024 00:35:31 +0200"> <!-- DEBUG: hard coded redirect_url --> <input type="hidden" name="redirectTo" value="https://lalovic.io/blog/thanks"> <textarea class="wide-field" id="message" name="message" rows="8" cols="68" placeholder="Your comment (LaTeX markup accepted)" required></textarea> <div style="margin-top: 5px; margin-bottom: 5px;"> <input class="input-field" id="name" name="name" type="text" placeholder="Your name (optional)" /> <button class="btn btn-blue" id="submit-button" type="submit">Add Comment</button> </div> </form> <div id="preview-div"></div> <!-- Comment preview and rerenderMath function to apply MathJax to preview --> <script> function previewComment() { const content = document.getElementById("message").value; document.getElementById("comment-result-title").innerText = "Your comment:"; document.getElementById("comment-result").innerText = content; rerenderMath(); } function rerenderMath() { MathJax.typeset(); } var textareaInput = document.getElementById("message"); textareaInput.onkeyup = textareaInput.onkeypress = function() { document.getElementById("preview-div").innerText = this.value; rerenderMath(); } </script> </div> </section> </div> </main> <hr> <footer class="my-footer"> <p><a class="fs-5" href="#top" id="back-to-top">Back to top</a></p> <p class="fs-3 text-grey-dk-000"> CC-BY 2024, Marko Lalovic. Theme is based on <a href="https://just-the-docs.com/">Just the Docs</a>. </p> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
