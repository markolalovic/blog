{"0": {
    "doc": "Index",
    "title": "Index",
    "content": " ",
    "url": "/blog/",
    
    "relUrl": "/"
  },"1": {
    "doc": "Multicollinearity effect on OLS regression",
    "title": "Why does this happen?",
    "content": "In general, we have a linear model . \\begin{equation} \\label{eq: model} y = X \\beta + \\epsilon \\end{equation} . and let for errors $\\epsilon$ hold the assumption (Gauss–Markov): . \\[\\mathbb{E}[\\epsilon \\epsilon^{T}] = \\sigma^{2} I.\\] The estimator according to the OLS method is . \\begin{equation} \\label{eq: betahat} \\hat{\\beta} = (X^{T}X)^{-1} X^{T} y. \\end{equation} . From \\ref{eq: model} and \\ref{eq: betahat}, we get . \\[\\hat{\\beta} - \\beta = (X^{T}X)^{-1} X^{T} \\epsilon\\] Therefore, the covariance matrix for $\\hat{\\beta}$ is . \\[\\begin{align} \\mathbb{E}[(\\hat{\\beta} - \\beta) (\\hat{\\beta} - \\beta)^{T}] \\nonumber &amp;= \\mathbb{E}[\\left((X^{T}X)^{-1} X^{T} \\epsilon\\right) \\left((X^{T}X)^{-1} X^{T} \\epsilon\\right)^{T}] \\nonumber \\\\[1em] &amp;= \\mathbb{E}[(X^{T}X)^{-1} X^{T} \\epsilon \\epsilon^{T} X (X^{T}X)^{-1}] \\nonumber \\\\[1em] &amp;= (X^{T}X)^{-1} X^{T} \\mathbb{E}[\\epsilon \\epsilon^{T}] X (X^{T}X)^{-1} \\nonumber \\\\[1em] &amp;= \\sigma^{2} (X^{T}X)^{-1} \\label{eq: variance} \\end{align}\\] In the derivation, we took into account that the matrix $X^{T}X$ is symmetric and assumed that it is not stochastic and it is independent of $\\epsilon$. The variance for the coefficient $\\hat{\\beta}_{k}$ is the $(k, k)$-th element of the covariance matrix. The average distance between the estimator $\\hat{\\beta}$ and the actual $\\beta$ is . \\[\\begin{align} \\mathbb{E}[(\\hat{\\beta} - \\beta)^{T} (\\hat{\\beta} - \\beta)] \\nonumber &amp;= \\mathbb{E}[((X^{T}X)^{-1} X^{T} \\epsilon)^{T} ((X^{T}X)^{-1} X^{T} \\epsilon)] \\nonumber \\\\[1em] &amp;= \\mathbb{E}[\\epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} \\epsilon] \\nonumber \\\\[1em] &amp;= \\mathbb{E}[\\text{tr}(\\epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} \\epsilon)] \\nonumber \\\\[1em] &amp;= \\mathbb{E}[\\text{tr}(\\epsilon \\epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} )] \\nonumber \\\\[1em] &amp;= \\sigma^{2} \\text{tr}( X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} )] \\nonumber \\\\[1em] &amp;= \\sigma^{2} \\text{tr}( X^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1})] \\nonumber \\\\[1em] &amp;= \\sigma^{2} \\text{tr}((X^{T}X)^{-1}) \\label{eq: distance} \\end{align}\\] In the derivation, we used that the distance is a scalar, so its expected value will be equal to its trace. We then used the fact that the trace is invariant with respect to cyclic permutations: . \\[\\text{tr}(ABCD) = \\text{tr}(DABC).\\] From \\ref{eq: variance} and \\ref{eq: distance}, we see that both the variance of the estimator and the distance of the estimator from the actual $\\beta$ depend on the matrix $(X^{T}X)^{-1}$. The reason why the variance of the estimator and the distance of the estimator from the actual $\\beta$ become large, can be shown conveniently using singular value decomposition. Let . \\[X = U \\Sigma V^{T}\\] be the singular value decomposition of $X$ where $\\Sigma$ contains all the singular values: . \\[\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{p} &gt; 0.\\] Then . \\[\\begin{align} (X^{T}X)^{-1} &amp;= (V \\Sigma^{T} \\Sigma V^{T})^{-1} \\nonumber \\\\[1em] &amp;= (V^{T})^{-1} (\\Sigma^{T} \\Sigma)^{-1} V^{-1} \\nonumber \\\\[1em] &amp;= V (\\Sigma^{T} \\Sigma)^{-1} V^{T} \\nonumber \\\\[1em] &amp;= \\sum_{j = 1}^{p} \\frac{1}{\\sigma_{j}^{2}} v_{j} v_{j}^{T} \\label{eq: svd} \\end{align}\\] In the limit . \\[| corr(x_{i}, x_{j}) | \\rightarrow 1\\] matrix $X$ becomes singular and the smallest singular value vanishes: . \\[\\sigma_{p} \\rightarrow 0\\] and from \\ref{eq: svd}: . \\[(X^{T}X)^{-1} \\rightarrow \\infty\\] Therefore, from \\ref{eq: variance} and \\ref{eq: distance}, both the variance of the $\\hat{\\beta}$ and the distance of $\\hat{\\beta}$ to the actual $\\beta$ go to infinity as \\(| corr(x_{i}, x_{j}) | \\rightarrow 1.\\) . ",
    "url": "/blog/ols-regression/#why-does-this-happen",
    
    "relUrl": "/ols-regression/#why-does-this-happen"
  },"2": {
    "doc": "Multicollinearity effect on OLS regression",
    "title": "Multicollinearity effect on OLS regression",
    "content": "In ordinary least squares (OLS) regression, for given $X: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{n}$ and $y \\in \\mathbb{R}^{n}$, we minimize over $\\beta \\in \\mathbb{R}^{p}$ the sum of squared residuals . \\[\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} S(\\beta) = \\norm{ y - X\\beta }_{2}^{2}\\] This blog post illustrates the problem (Figure 1) of using OLS method to estimate the unknown parameters $\\beta$ in the case of highly correlated regressors on a simple example using R. Figure 1: As the correlation between regressors increases, the OLS method becomes unstable. Suppose we have a model . \\[y \\sim \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2}\\] where . \\[\\beta_{0} = 3, \\quad \\beta_{1} = \\beta_{2} = 1.\\] Let the sample contain 100 elements: . n &lt;- 100 . and let’s introduce some highly correlated regressors: . set.seed(42) x1 &lt;- rnorm(n) x2 &lt;- rnorm(n, mean = x1, sd = 0.01) . with correlation coefficient almost 1: . cor(x1, x2) [1] 0.999962365268769 . We can run the OLS method 1000 times to get a sense of the effect of highly correlated regressors: . x &lt;- as.matrix(cbind(intr = 1, x1, x2)) nsim &lt;- 1000 betas &lt;- sapply(1:nsim, function(i) { y &lt;- rnorm(n, mean = 3 + x1 + x2, sd = 1) xx &lt;- solve(t(x) %*% x) beta.ols &lt;- as.vector(xx %*% t(x) %*% y) return(beta.ols) }) . The estimator for $\\beta$, obtained by the OLS method, is still unbiased . round(apply(betas, 1, mean), 3) [1] 2.996 1.006 0.993 . But the variance is large . round(apply(betas, 1, sd), 3) [1] 0.101 11.110 11.103 . The estimated coefficients can be very large, some can even have the wrong sign: . round(betas[, 1], 3) [1] 3.002 -7.673 9.529 . The problem can be seen by drawing a contour plot (shown in Figure 1) of the objective function $S(\\beta)$: . ssr &lt;- function(b) { return(sum((y - x %*% b)^2)) } xlen &lt;- 100 ylen &lt;- 100 xgrid &lt;- seq(-10.1, 10.1, length.out = xlen) ygrid &lt;- seq(-10.1, 10.1, length.out = ylen) zvals &lt;- matrix(NA, ncol = xlen, nrow = ylen) for (i in 1:xlen) { for (j in 1:ylen) { zvals[i, j] &lt;- ssr(c(3, xgrid[i], ygrid[j])) } } contour(x = xgrid, y = ygrid, z = zvals, levels = c(1e3, 3e3, 6e3, 1e4)) . As the correlation between regressors increases, matrix $X$ becomes nearly singular and OLS method becomes unstable. In the limit . \\[|corr(x_{i}, x_{j})| \\rightarrow 1\\] the dimension of the column space decreases . \\[\\text{rank}(X) &lt; p.\\] The objective function $S(\\beta)$ is no longer strictly convex, and there are infinitely many solutions of OLS. Intuitively, we would like to estimate the coefficient $\\beta_{1}$ as the influence of $x_{1}$ on $y$ without the influence of $x_{2}$. Since the regressors $x_{1}$ and $x_{2}$ are highly correlated, they vary together and the coefficient $\\beta_{1}$ is difficult to estimate. The OLS method does not accomplish this, as it only minimizes the sum of squared residuals, i.e. the objective function $S(\\beta)$. ",
    "url": "/blog/ols-regression/",
    
    "relUrl": "/ols-regression/"
  },"3": {
    "doc": "Ridge regression",
    "title": "Table of contents",
    "content": ". | Penalized regression | Derivation of ridge regression estimator . | Example | . | Properties of ridge regression estimator | The role of the penalty function | Regularization parameter $\\alpha$ | References | . ",
    "url": "/blog/ridge-regression/#table-of-contents",
    
    "relUrl": "/ridge-regression/#table-of-contents"
  },"4": {
    "doc": "Ridge regression",
    "title": "Penalized regression",
    "content": "In penalized regression, for $n &gt; p$ and given $X: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{n}$ and $y \\in \\mathbb{R}^{n}$, we minimize the functional . \\[\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} J_{\\alpha}(\\beta) = \\norm{ y - X\\beta }_{2}^{2} + \\alpha P(\\beta)\\] over $\\beta \\in \\mathbb{R}^{p}$, where: . | $J_{\\alpha}: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}$ is the objective function; | $P: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}$ is a penalty function that penalizes unrealistic values in $\\beta$; | parameter $\\alpha &gt; 0$ controls the trade-off between the penalty and the fit of the loss function. | . There are many different possibilities for the penalty function $P$. The main idea that determines the choice of the penalty function is that we would prefer a simple model to a more complex one. For example, if we want a smoother fit, then the penalty function to consider is a measure of the curvature. In the case of correlated regressors, the estimated coefficients can become too large and $P$ is a measure of the distance of the coefficients from the origin. In this case, the main penalty function to consider is . \\[P(\\beta) = \\norm{\\beta}_{2}^{2}.\\] This type of penalized regression is called ridge regression introduced in (Hoerl &amp; Kennard, 1970). It belongs to a more general type of regularization known as Tikhonov regularization (Tikhonov &amp; Arsenin, 1977) where for some suitably chosen matrix $T: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{n}$, the penalty function is . \\[P(\\beta) = \\norm{T \\beta}_{2}^{2}.\\] ",
    "url": "/blog/ridge-regression/#penalized-regression",
    
    "relUrl": "/ridge-regression/#penalized-regression"
  },"5": {
    "doc": "Ridge regression",
    "title": "Derivation of ridge regression estimator",
    "content": "In order to simplify the derivation, assume that $X: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{n}$ is linear and continuous with full column rank $p$. The objective function we want to minimize, written in a matrix form, is . \\[\\begin{align*} \\norm{ y - X \\beta }_{2}^{2} + \\alpha \\norm{\\beta}_{2}^2 &amp;= (y - X \\beta)^{T} (y - X \\beta) + \\alpha \\beta^{T} \\beta \\nonumber \\\\ &amp;= y^{T}y - 2 y^{T} X \\beta + \\beta^{T} X^{T}X \\beta + \\alpha \\beta^{T} \\beta \\end{align*}\\] Taking a partial derivative with respect to $\\beta$ and setting it to zero . \\[-2 X^{T} y + 2 X^{T} X \\hat{\\beta} + 2 \\alpha \\hat{\\beta} = 0\\] gives a regularized normal equation . \\[(X^{T}X + \\alpha I) \\hat{\\beta} = X^{T}y\\] and express $\\hat{\\beta}$ as . \\[\\hat{\\beta} = (X^{T}X + \\alpha I)^{-1} X^{T} y.\\] Since $\\text{rank}(X) = p$ . \\[X z \\neq 0 \\quad \\text{for each} \\quad z \\neq 0\\] For the Hessian . \\[2X^{T}X + 2 \\alpha\\] it holds that . \\[\\begin{align*} 2 z^{T} X^{T} X z + 2 \\alpha z^{T} z &amp;= 2 (Xz)^{T} (Xz) + 2 \\alpha z^{T} z \\nonumber \\\\[1em] &amp;= 2 \\norm{Xz}_{2}^{2} + 2 \\alpha \\norm{z}_{2}^{2} &gt; 0 \\quad \\text{for all} \\quad z \\neq 0 \\end{align*}\\] Therefore, the expressed $\\hat{\\beta}$ is an estimator, denoted by: . \\[\\begin{equation} \\hat{\\beta}_{\\text{RR}} = (X^{T}X + \\alpha I)^{-1} X^{T} y. \\label{eq: rr} \\end{equation}\\] Example . We can demonstrate how the ridge regression method estimates the unknown parameters $\\beta$ in the case of correlated regressors on a simple example using R. Suppose, we have the following model . \\[y \\sim \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2}.\\] More specifically, let . \\[\\beta_{0} = 3, \\quad \\beta_{1} = \\beta_{2} = 1.\\] and let the sample contain 100 elements . n &lt;- 100 . Then, introduce some highly correlated regressors . set.seed(42) x1 &lt;- rnorm(n) x2 &lt;- rnorm(n, mean = x1, sd = 0.01) . with correlation coefficient almost 1 . cor(x1, x2) [1] 0.999962365268769 . into the model . y &lt;- rnorm(n, mean = 3 + x1 + x2, sd = 1) . and calculate the estimate $\\hat{\\beta}_{\\text{RR}}$ for $\\alpha = 0.3$ . alpha &lt;- 0.3 x &lt;- as.matrix(cbind(int = 1, x1, x2)) beta.ridge &lt;- function(alpha, x, y) { xx &lt;- solve(t(x) %*% x + alpha * diag(3)) return(as.vector(xx %*% t(x) %*% y)) } beta.ridge(alpha, x, y) [1] 2.98537494896842 0.815120466450887 1.04146900239714 . ",
    "url": "/blog/ridge-regression/#derivation-of-ridge-regression-estimator",
    
    "relUrl": "/ridge-regression/#derivation-of-ridge-regression-estimator"
  },"6": {
    "doc": "Ridge regression",
    "title": "Properties of ridge regression estimator",
    "content": "The unique solution to \\ref{eq: rr} of ridge regression estimator $\\hat{\\beta}_{\\text{RR}}$ always exists, since $X^{T}X + \\alpha I$ is always rank $p$. Let’s derive the relationship between ridge and OLS estimators for the case when matrix $X$ is orthogonal. Using $X^{T}X = I$ twice and since $\\hat{\\beta}_{\\text{OLS}} = (X^{T}X)^{-1} X^{T} y$, we get . \\[\\begin{align*} \\hat{\\beta}_{\\text{RR}} &amp;= (X^{T}X + \\alpha I)^{-1} X^{T} y \\nonumber \\\\[1em] &amp;= (I + \\alpha I)^{-1} X^{T} y \\nonumber \\\\[1em] &amp;= (1 + \\alpha)^{-1} I X^{T} y \\nonumber \\\\[1em] &amp;= (1 + \\alpha)^{-1} (X^{T}X)^{-1} X^{T} y \\nonumber \\\\[1em] &amp;= (1 + \\alpha)^{-1} \\hat{\\beta}_{\\text{OLS}} \\end{align*}\\] Ridge regression estimator $\\hat{\\beta}_{\\text{RR}}$ is biased since, for any value of $\\alpha &gt; 0$, its expected value is not equal to $\\beta$: . \\[\\begin{align*} \\mathbb{E}[\\hat{\\beta}_{ridge}] &amp;= \\mathbb{E}[(X^{T}X + \\alpha I)^{-1} X^{T} y] \\nonumber \\\\[1em] &amp;= \\mathbb{E}[(X^{T}X + \\alpha I)^{-1} (X^{T}X) (X^{T}X)^{-1} X^{T} y] \\nonumber \\\\[1em] &amp;= \\mathbb{E}[(X^{T}X + \\alpha I)^{-1} (X^{T}X) \\hat{\\beta}_{\\text{OLS}}] \\nonumber \\\\[1em] &amp;= (X^{T}X + \\alpha I)^{-1} (X^{T}X) \\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] \\nonumber \\\\[1em] &amp;= (X^{T}X + \\alpha I)^{-1} (X^{T}X) \\beta. \\end{align*}\\] As $\\alpha \\rightarrow 0$, ridge estimator tends to OLS estimator. This can be seen from . \\[\\begin{align*} \\lim_{\\alpha \\to 0} \\hat{\\beta}_{\\text{RR}} &amp;= \\lim_{\\alpha \\to 0} (X^{T}X + \\alpha I)^{-1} (X^{T}X) \\hat{\\beta}_{\\text{OLS}} \\nonumber \\\\[1em] &amp;= (X^{T}X)^{-1} (X^{T}X) \\hat{\\beta}_{\\text{OLS}} \\nonumber \\\\[1em] &amp;= \\hat{\\beta}_{\\text{OLS}}. \\end{align*}\\] ",
    "url": "/blog/ridge-regression/#properties-of-ridge-regression-estimator",
    
    "relUrl": "/ridge-regression/#properties-of-ridge-regression-estimator"
  },"7": {
    "doc": "Ridge regression",
    "title": "The role of the penalty function",
    "content": "The role of the penalty function can be shown conveniently using singular value decomposition. Let . \\[X = U \\Sigma V^{T}\\] be the singular value decomposition of $X$ where $\\Sigma$ contains all the singular values . \\[\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{p} &gt; 0.\\] The regularized normal equation . \\[( X^{T} X + \\alpha I ) \\hat{\\beta} = X^{T} y\\] can be rewritten as . \\[(V \\Sigma^{T} U^{T}U \\Sigma V^{T} + \\alpha I) \\hat{\\beta} = V \\Sigma^{T} U^{T} y\\] Since $U^{T}U = I$ and $V^{T}V = I$, we have . \\[(V \\Sigma^{T} \\Sigma V^{T} + \\alpha V^{T}V) \\hat{\\beta} = V (\\Sigma^{T} \\Sigma + \\alpha I) V^{T} \\hat{\\beta} = V \\Sigma^{T} U^{T} y\\] Multiplying by $V^{T}$ from the left and setting $z = V^{T} \\hat{\\beta}$, we get . \\[(\\Sigma^{T} \\Sigma + \\alpha I) z = \\Sigma^{T} U^{T} y\\] Therefore . \\[z_{i} = \\frac{\\sigma_{i} (u_{i}^{T} y)}{\\sigma_{i}^{2} + \\alpha} \\quad \\text{for} \\quad i = 1, \\dots, p\\] For minimum norm solution, let . \\[z_{i} = 0 \\quad \\text{for} \\quad i = p + 1, \\dots, n.\\] Finally, from $\\hat{\\beta} = V z$ and since $V$ is orthogonal . \\[\\norm{\\hat{\\beta}} = \\norm{VV^{T} \\hat{\\beta}} = \\norm{V^{T}\\hat{\\beta}} = \\norm{z}\\] we get . \\[\\begin{equation} \\hat{\\beta}_{i} = \\frac{\\sigma_{i} (u_{i}^{T} y)}{\\sigma_{i}^{2} + \\alpha} v_{i}. \\label{eq: beta_i} \\end{equation}\\] From . \\[\\begin{equation} \\hat{\\beta}_{i} \\approx \\begin{cases} 0, &amp; \\text{if } \\sigma_{i} &lt;&lt; \\alpha \\\\ \\frac{u_{i}^{T} y}{\\sigma_{i}}v_{i}, &amp; \\text{if } \\sigma_{i} &gt;&gt; \\alpha \\end{cases} \\end{equation}\\] it can be seen that the penalty function $\\alpha \\norm{\\beta}_{2}^{2}$ acts as a filter since the contributions . | from $\\sigma_{i}$ that is small relative to the regularization parameter $\\alpha$ are almost eliminated; | from $\\sigma_{i}$ that is large relative to the regularization parameter $\\alpha$ are left almost unchanged. | . By defining a filter . \\[\\begin{equation} F_{\\alpha}(\\xi) = \\frac{1}{\\xi + \\alpha} \\end{equation}\\] the solution of ridge regression can be expressed as . \\[\\hat{\\beta}_{\\text{RR}} = F_{\\alpha}(X^{T}X) X^{T}y.\\] ",
    "url": "/blog/ridge-regression/#the-role-of-the-penalty-function",
    
    "relUrl": "/ridge-regression/#the-role-of-the-penalty-function"
  },"8": {
    "doc": "Ridge regression",
    "title": "Regularization parameter $\\alpha$",
    "content": "The solution of ridge regression is monotonically decreasing in $\\alpha$. To see this, let . \\[\\psi(\\alpha) = \\norm{\\hat{\\beta}_{\\text{RR}}}_{2}^{2}.\\] From derived equation for $\\hat{\\beta}_{i}$ in \\ref{eq: beta_i}, we have that . \\[\\psi(\\alpha) = \\sum_{i = 1}^{p} \\frac{\\sigma_{i}^{2} (u_{i}^{T} y)^{2}}{ (\\sigma_{i}^{2} + \\alpha)^{2} } v_{i}^{2}\\] and the first derivative . \\[\\psi'(\\alpha) = -2 \\sum_{i = 1}^{p} \\frac{\\sigma_{i}^{2} (u_{i}^{T} y)^{2}}{ (\\sigma_{i}^{2} + \\alpha)^{3} } v_{i}^{2} &lt; 0.\\] As $\\alpha \\rightarrow \\infty$ the solution of ridge regression goes to 0 . \\[\\lim_{\\alpha \\rightarrow \\infty} \\psi(\\alpha) = \\lim_{\\alpha \\rightarrow \\infty} \\sum_{i = 1}^{p} \\frac{\\sigma_{i}^{2} (u_{i}^{T} y)^{2}}{ (\\sigma_{i}^{2} + \\alpha)^{2} } v_{i}^{2} = 0\\] In the limit $\\alpha \\rightarrow 0$, the solution of ridge regression goes to ordinary least squares solution. Furthermore, if $\\sigma_{p} \\rightarrow 0$ where $X$ is no longer full column rank, then $\\psi(\\alpha) \\rightarrow \\infty$. Figure 2 shows how the estimates $\\beta_{0}, \\beta_{1}$ and $\\beta_{2}$ change depending on the value of parameter $\\alpha$ for the data from Example above. alphas &lt;- exp(seq(-5, 10, 0.1)) betas &lt;- sapply(alphas, function(alpha) { beta.ridge(alpha, x, y) }) library(latex2exp) # for annotation plot(log(alphas), betas[1, ], type=\"l\", lty=1, lwd=3, col=\"red\", xlab=TeX(r'($\\log(\\alpha)$)'), ylab=TeX(r'($\\hat{\\beta}$)'), cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5) lines(log(alphas), betas[2, ], type=\"l\", lty=2, lwd=3, col=\"blue\") lines(log(alphas), betas[3, ], type=\"l\", lty=3, lwd=3, col=\"black\") legend(7.73, 3.12, legend=c( TeX(r'($\\hat{\\beta}_{1}(\\alpha)$)'), TeX(r'($\\hat{\\beta}_{2}(\\alpha)$)'), TeX(r'($\\hat{\\beta}_{3}(\\alpha)$)')), col=c(\"red\", \"blue\", \"black\"), lwd=rep(3,3), lty=1:3, cex=1.5) . Figure 2: The solution of ridge regression as a function of the regularization parameter $\\alpha$. The selection of $\\alpha$ is usually done by cross-validation as follows. First, randomly partition the data into $K$ equally sized sets. For some value of $\\alpha$, build a model (calculate estimates for the coefficients) on the data from $K - 1$ sets (learning set) and test it on the rest of the data (test set) by calculating the mean square error (MSE). Then, repeat this process for the remaining values of $\\alpha$ and select the value of $\\alpha$ with the smallest MSE. Typical values for $K$ are $5, 10$, and $n$ (sample size). Let’s find the optimal value of parameter $\\alpha$ for the data in Example using 10-fold cross-validation: . K &lt;- 10 folds &lt;- cut(seq(1, nrow(x)), breaks=K, labels=FALSE) cv.matrix &lt;- matrix(NA, nrow=K, ncol=length(alphas)) mse &lt;- function(b, x, y) { return(1/length(y) * sum((y - x %*% b)^2)) } for (k in 1:K) { test.i &lt;- which(folds == k) for (j in 1:length(alphas)) { br &lt;- beta.ridge(alphas[j], x[-test.i, ], y[-test.i]) cv.matrix[k, j] &lt;- mse(br, x[test.i, ], y[test.i]) } } avgs &lt;- apply(cv.matrix, 2, mean) best.alpha &lt;- alphas[avgs == min(avgs)] best.alpha [1] 0.246596963941606 . ",
    "url": "/blog/ridge-regression/#regularization-parameter-alpha",
    
    "relUrl": "/ridge-regression/#regularization-parameter-alpha"
  },"9": {
    "doc": "Ridge regression",
    "title": "References",
    "content": ". | Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 | Tikhonov, A. N., &amp; Arsenin, V. Y. (1977). Solutions of ill-posed problems (p. xiii+258). V. H. Winston \\&amp; Sons. https://catalogue.nla.gov.au/catalog/720231 | . ",
    "url": "/blog/ridge-regression/#references",
    
    "relUrl": "/ridge-regression/#references"
  },"10": {
    "doc": "Ridge regression",
    "title": "Ridge regression",
    "content": "The ordinary least squares (OLS) method is not suitable to estimate the unknown parameters $\\beta$ in the case of highly correlated regressors. As the correlation between regressors in $X$ increases the OLS method becomes unstable. In the limit $|corr(x_{i}, x_{j})| \\rightarrow 1$, the OLS objective function is no longer strictly convex and there are infinitely many solutions of OLS problem. The matrix $X$ becomes singular and both the variance of the estimator and the distance of the estimator to the actual $\\beta$ go to infinity. (See Multicollinearity effect on OLS regression for more details.) . Ridge regression is an effective approach to solve such problems. Regardless of data $X$, unique solution to ridge regression always exists. By adding the ridge (vector of $\\alpha$’s) on the diagonal of $X$, the ridge regression produces stable estimates of the coefficients in $\\beta$ (Figure 1). Figure 1: As the correlation between regressors increases the OLS method becomes unstable while the ridge regression method produces stable estimates. In this blog post, penalized regression is introduced, and the ridge regression estimator is derived. A simple example in R is used to illustrate the method. After that, some properties of the estimator are outlined, and the role of the penalty function is explained. Finally, an analysis of the regularization parameter $\\alpha$ is performed. ",
    "url": "/blog/ridge-regression/",
    
    "relUrl": "/ridge-regression/"
  },"11": {
    "doc": "On Storey's direct approach to false discovery rates",
    "title": "Table of contents",
    "content": ". | Introduction . | Single-hypothesis testing | Multiple-hypothesis testing | Bonferroni correction | Benjamini-Hochberg procedure | Storey’s approach | . | References | . ",
    "url": "/blog/direct-approach-to-fdr/#table-of-contents",
    
    "relUrl": "/direct-approach-to-fdr/#table-of-contents"
  },"12": {
    "doc": "On Storey's direct approach to false discovery rates",
    "title": "Introduction",
    "content": " ",
    "url": "/blog/direct-approach-to-fdr/#introduction",
    
    "relUrl": "/direct-approach-to-fdr/#introduction"
  },"13": {
    "doc": "On Storey's direct approach to false discovery rates",
    "title": "Single-hypothesis testing",
    "content": "Following the scientific method, researchers usually want to establish the truth of a statement by demonstrating that the opposite appears to be false. A hypothesis is a proposed explanation for a phenomenon that can be tested. In statistics, hypotheses involve a parameter $\\theta$ whose value is unknown but must fall within a certain parameter space $\\Omega$. Our assumption is that $\\Omega$ can be partitioned into two disjoint subsets, $\\Omega_0$ and $\\Omega_1$, and define two hypotheses: . \\[\\begin{align*} H_{0}\\text{: } \\theta &amp;\\in \\Omega_0, \\\\[.5em] H_{1}\\text{: } \\theta &amp;\\in \\Omega_1. \\end{align*}\\] The hypothesis $H_{1}$ is the statement that confirms the theory and it is called an alternative hypothesis. The opposite $H_{0}$ is called a null hypothesis. The task is to decide which of the hypotheses appears to be true. A procedure for deciding this is called a test. Before deciding which hypothesis to choose, we assume that a random sample $\\mathbf{X} = (X_{1}, …, X_{n})$ is drawn from a distribution that involves the unknown parameter $\\theta$. Let $T = r(\\mathbf{X})$ be a statistic and let $\\Gamma$ be a subset of the real line and suppose that the test procedure is of the form: reject $H_{0}$ if $T \\in \\Gamma$. Then $T$ is called a test statistic and $\\Gamma$ a rejection region. A problem of this type is called single-hypothesis testing. In single-hypothesis testing there are only two kinds of errors. A type I error occurs when rejecting the null $H_{0}$ when in fact $H_{0}$ is true. And type II error occurs when failing to reject the null when in fact the null hypothesis $H_{0}$ is false. The significance level $\\alpha$ is the upper bound on the probability of type I error: . \\[\\begin{equation} \\label{eq: condition1} \\text{Pr}(T \\in \\Gamma \\mid H_{0} \\text{ is true }) \\leq \\alpha. \\end{equation}\\] The power $\\pi$ of a test is the probability of rejecting $H_{0}$ when in fact $H_{0}$ is false. Practice is to choose the value of $\\alpha$, say $0.01$, and find the rejection region $\\Gamma_{\\alpha}$ that maximizes the power and satisfies (\\ref{eq: condition1}), that probability of type I error is at most $\\alpha$. The detection power $\\pi$ is a function of rejection region $\\Gamma_{\\alpha}$: . \\[\\pi(\\Gamma_{\\alpha}) = \\text{Pr}(T \\in \\Gamma_{\\alpha} \\mid H_{0} \\text{ is false }).\\] Typically, rejection regions $\\Gamma_{\\alpha}$ and $\\Gamma_{\\alpha’}$ for different values $\\alpha$ and $\\alpha’$, are nested in the following sense: . \\[\\begin{equation} \\label{eq: nesting} \\text{if} \\quad \\alpha &lt; \\alpha' \\quad \\text{then} \\quad \\Gamma_{\\alpha} \\subset \\Gamma_{\\alpha'}. \\end{equation}\\] For the given observation $T = t$, the smallest significance level $\\alpha$ at which a null hypothesis would be rejected, is called $p$-value: . \\[p(t) = \\inf \\lbrace \\alpha : t \\in \\Gamma_{\\alpha} \\rbrace.\\] ",
    "url": "/blog/direct-approach-to-fdr/#single-hypothesis-testing",
    
    "relUrl": "/direct-approach-to-fdr/#single-hypothesis-testing"
  },"14": {
    "doc": "On Storey's direct approach to false discovery rates",
    "title": "Multiple-hypothesis testing",
    "content": "Now consider simultaneous testing a collection of $m$ hypotheses, called a family of hypotheses $\\mathcal{F}$, where for $i = 1, \\dots, m$, we test $H_{0i}$ versus $H_{1i}$ on the basis of test statistics given in the form of p-values $p_{i}$. Let $H_{1}, …,H_{m}$ be random variables, where . \\[H_{i} = \\begin{cases} 0 &amp; \\text{if $H_{0i}$ is true,}\\\\ 1 &amp; \\text{if $H_{1i}$ is true.} \\end{cases}\\] A multiple-testing procedure is a decision function $\\phi$ that given p-values $p_{1}, …, p_{m}$ assigns decision values $D_{1}, …, D_{m}$: . \\[\\phi(p_{i}) = D_{i}, \\quad i = 1,..., m,\\] where $D_{i} = 1$ if hypothesis $H_{0i}$ is rejected and $D_{i} = 0$ otherwise, for $i = 1, \\dots, m$. Table 1 describes the possible outcomes from $m$ hypothesis tests, where $I_{0} \\subset \\lbrace 1, \\dots, m \\rbrace$ is the index of true null hypotheses, $m_{0} = |I_{0}|$ is the number of true null hypotheses, $V = \\sum_{i=1}^{m} (1 - H_{i}) D_{i}$ is the number of true null hypotheses which are rejected (type I errors), $R = \\sum_{i=1}^{m} D_{i}$ is the number of all rejected null hypotheses, etc. | | Accepted | Rejected | Total | . | $I_{0}$ | $U$ | $V$ | $m_{0}$ | . | $\\lbrace 1, \\dots, m \\rbrace \\setminus I_{0}$ | $T$ | $S$ | $m - m_{0}$ | . | Total | $W$ | $R$ | $m$ | . Table 1: All possible outcomes from $m$ hypothesis tests. If the decision function $\\phi$ is based on rejection region of the form $[0, \\gamma]$ for some threshold $\\gamma &gt; 0$: . \\[\\phi(p_{i}) = \\mathbb{1}{\\lbrace p_{i} \\leq \\gamma\\rbrace},\\] then the random variables $U, V, …, R$ depend only on this threshold $\\gamma$, for example $R(\\gamma) = \\sum_{i=1}^{m}{\\mathbb{1}{\\lbrace p_{i} \\leq \\gamma\\rbrace}}$. Good scientific practice requires the specification of certain type I error measure control to be done prior to the data analysis. The problem is to find a multiple-testing procedure which maximizes detection power and satisfies certain conditions involving type I error measures. A problem of this type is called a multiple-testing problem. It is described in (Heller, 2013) or in the context of assessing the feature significance in (Hastie et al., 2009). The multiple-testing procedure $\\phi$ is said to have a strong level $\\alpha$ control of some error measure EM, if it satisfies the condition: . \\[\\text{EM}_{\\phi} \\leq \\alpha\\] for any configuration of true and false hypotheses in the family of $m$ hypotheses $\\mathcal{F}$. The first type I error measure that was suggested is the family-wise error rate (FWER). This is the probability that we make at least one type I error in the family $\\mathcal{F}$ of $m$ hypotheses using a multiple-testing procedure $\\phi$: . \\[\\text{FWER}_{\\phi} = \\text{Pr}(V \\geq 1).\\] I'm switching the theme of this blog and this post still needs some work. See the PDF version: . PDF version ",
    "url": "/blog/direct-approach-to-fdr/#multiple-hypothesis-testing",
    
    "relUrl": "/direct-approach-to-fdr/#multiple-hypothesis-testing"
  },"15": {
    "doc": "On Storey's direct approach to false discovery rates",
    "title": "Bonferroni correction",
    "content": " ",
    "url": "/blog/direct-approach-to-fdr/#bonferroni-correction",
    
    "relUrl": "/direct-approach-to-fdr/#bonferroni-correction"
  },"16": {
    "doc": "On Storey's direct approach to false discovery rates",
    "title": "Benjamini-Hochberg procedure",
    "content": " ",
    "url": "/blog/direct-approach-to-fdr/#benjamini-hochberg-procedure",
    
    "relUrl": "/direct-approach-to-fdr/#benjamini-hochberg-procedure"
  },"17": {
    "doc": "On Storey's direct approach to false discovery rates",
    "title": "Storey’s approach",
    "content": " ",
    "url": "/blog/direct-approach-to-fdr/#storeys-approach",
    
    "relUrl": "/direct-approach-to-fdr/#storeys-approach"
  },"18": {
    "doc": "On Storey's direct approach to false discovery rates",
    "title": "References",
    "content": ". | Storey, J. D. (2002). A direct approach to false discovery rates. Journal of the Royal Statistical Society: Series B (Statistical Methodology). Vol. 64(3). | Benjamini, Y., &amp; Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society, Series B. 57(1). | Black, M. A. (2004). A note on the adaptive control of false discovery rates. Journal of the Royal Statistical Society Series B (Statistical Methodology) 66(2):297-304. | Storey, J. D., &amp; Tibshirani, R. (2001). Estimating the positive false discovery rate under dependence, with applications to DNA microarrays. Stanford Statistics Technical Report. No. 2001-28. | Heller, R. (2013). False Discovery Rate Control in multiple testing problems. http://www.statistics.org.il/wp-content/uploads/2013/04/FDR-Ruth-Heller.pdf | Hastie, T., Tibshirani, R., &amp; Friedman, J. H. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer. | . ",
    "url": "/blog/direct-approach-to-fdr/#references",
    
    "relUrl": "/direct-approach-to-fdr/#references"
  },"19": {
    "doc": "On Storey's direct approach to false discovery rates",
    "title": "On Storey's direct approach to false discovery rates",
    "content": "Storey (Storey, 2002) shed a new light on the multiple-testing problem by defining a positive false discovery rate measure and providing a new perspective with a direct approach to false discovery rates. Overview. After an introduction to multiple-testing and Storey’s direct approach the duality between Storey’s direct approach and Benjamini-Hochberg (BH) procedure (Benjamini &amp; Hochberg, 1995) is explored. It follows that the estimate of the proportion of null hypotheses plays a key role in the multiple-testing problem, and the approach we take doesn’t matter much. This is confirmed by simulations where performances were practically identical for both methods. When the distance between distributions under the null hypothesis and alternative hypotheses is small, the Storey’s estimator can have a strong upward bias. By increasing the tuning parameter value, the upward bias in the simulations was reduced. However, this can result in underestimation, as already observed in (Black, 2004). In case of dependence, Storey suggested the same approach (Storey &amp; Tibshirani, 2001). From the simulation, it can be observed that dependence can lead to a very high overestimation of the false discovery rate. ",
    "url": "/blog/direct-approach-to-fdr/",
    
    "relUrl": "/direct-approach-to-fdr/"
  },"20": {
    "doc": "Covid Calculator",
    "title": "Table of contents",
    "content": ". | Introduction | Age structure | Fatality rates | Proportion of infected | Probability of eliminating Covid-19 | Expected number of infected | Expected number of deaths | Expected years of life lost | Actual years of life lost . | Example | . | Projected costs | Projected poverty . | Example | . | References | . ",
    "url": "/blog/covid-calc/#table-of-contents",
    
    "relUrl": "/covid-calc/#table-of-contents"
  },"21": {
    "doc": "Covid Calculator",
    "title": "Introduction",
    "content": "At the time of writing, the impacts of Covid-19 remain largely uncertain and depend on a whole range of possibilities. Organizing the overwhelming mass of the available information and coming up with a reasonable estimates can be challenging. As an attempt to address this problem I used publicly available data to create an online tool called Covid Calculator that allows users to derive their own country-specific estimates. The tool is focused on simple presentation and pedagogical aspects and only offers crude estimates. It uses relatively simplistic methodology outlined below. There are lots of improvements possible and more things to consider. One is to also include estimated fatality rates of Covid-19 by pre-existing health conditions. Having time to event data and applying survival analysis techniques would result in a more sensible estimates of expected years of life lost. Allowing parameters to evolve over time and comparing different time spans is another improvement. ",
    "url": "/blog/covid-calc/#introduction",
    
    "relUrl": "/covid-calc/#introduction"
  },"22": {
    "doc": "Covid Calculator",
    "title": "Age structure",
    "content": "We denote the number of something with $n(\\cdot)$, for example . \\[n(\\text{people in the world}) \\approx 7.59B.\\] For selected location population we use data about age from (Pyramids, 2020) and divide it in 9 intervals or age groups: . \\[\\begin{align*} G = \\{ \\text{0-9}, \\text{10-19}, \\ldots, \\text{70-79}, \\text{80+} \\}. \\end{align*}\\] The size of population $N(g)$ by age group $g$ is estimated by counting how many people fall into each age group $g$: . \\[N(g) = n(\\text{people in age group g})\\] and total population size $N$ is . \\[N = \\sum_{g \\text{ in } G} n(\\text{people in age group g}).\\] For a more detailed analysis, age groups are divided into two sets . \\[\\begin{align*} G_{&lt;60} &amp;= \\{ \\text{0-9}, \\text{10-19}, \\ldots, \\text{50-59} \\} \\\\[.5em] G_{60+} &amp;= \\{ \\text{60-69}, \\text{70-79}, \\text{80+} \\}. \\end{align*}\\] The proportion of people over 60 in selected population is calculated as . \\[d_{60+} = \\sum_{g \\text{ in } G_{60+}} N(g) / N.\\] ",
    "url": "/blog/covid-calc/#age-structure",
    
    "relUrl": "/covid-calc/#age-structure"
  },"23": {
    "doc": "Covid Calculator",
    "title": "Fatality rates",
    "content": "Infection Fatality Rate (IFR), see (N Ferguson &amp; et al., 2020), represents the proportion of deaths among all the infected individuals: . \\[\\text{IFR} = n(\\text{deaths}) / n(\\text{infected}).\\] Case Fatality Rate (CFR), see (Wikipedia, 2020), represents the proportion of confirmed deaths among all confirmed infected individuals: . \\[\\text{CFR} = n( \\text{confirmed cases of deaths} ) / n( \\text{confirmed cases of infected} ).\\] By default, the estimates $\\text{IFR}(g)$ by age groups are from (N Ferguson &amp; et al., 2020). Users can also choose to use estimates of $\\text{CFR}(g)$ by age groups based on data from different countries. There is a big difference between the two measures. For example if in some particular time frame we have 5 confirmed cases of people infected and 2 confirmed deaths, then $CFR = 2/5 = 0.4$. However, if based on some other data and not only on confirmed cases, we know that there are actually more people infected, than our estimated IFR will be smaller than $\\text{CFR}$. Users can also adjust the fatality rate of each age group by input parameter $F$. It represents percent of increase or decrease. Fatality Rates $\\text{FR}(g)$ by age groups are calculated by multiplying selected estimates of fatality rate for each age group by $1 + F/100$ and used to estimate actual $\\text{IFR}$: . \\[\\text{FR}(g) = \\text{*FR}(g) \\cdot (1 + F/100).\\] Here, $\\text{*FR}$ is estimated $\\text{IFR}$ or user selected $\\text{CFR}$ estimate for a particular country. Note: . | Since “confirmed cases of infected” is a subset of “infected”, wider testing can reduce $\\text{CFR}$ estimates. | When using $\\text{CFR}$, the expected number of deaths in age group 0-9 is always 0 since no children under 10 appear to have died from Covid-19 when this data was aquired. | Our proposed approach for later estimation assumes that the fatality rate by age in selected location has distribution similar to that estimated by (N Ferguson &amp; et al., 2020) or observed in the country of selected $\\text{CFR}$. | . ",
    "url": "/blog/covid-calc/#fatality-rates",
    
    "relUrl": "/covid-calc/#fatality-rates"
  },"24": {
    "doc": "Covid Calculator",
    "title": "Proportion of infected",
    "content": "The selected proportion of infected $H$ is: . \\[H = n(\\text{infected}) \\cdot 100 / N.\\] Users can adjust the proportion of infected people over 60 using $H_{60+}$. The overall $H$ can be decomposed as: . \\[\\begin{equation} H = (1 - d_{60+}) \\cdot H_{&lt;60} + d_{60+} \\cdot H_{60+} \\label{eq: decomposition} \\end{equation}\\] where $H_{&lt;60}$ is proportion of infected people below 60. ",
    "url": "/blog/covid-calc/#proportion-of-infected",
    
    "relUrl": "/covid-calc/#proportion-of-infected"
  },"25": {
    "doc": "Covid Calculator",
    "title": "Probability of eliminating Covid-19",
    "content": "Let $A$ be the event of achieving complete elimination of Covid-19 disease before it manages to infect proportion of population $H$. Let $I_{A}$ be the indicator variable for event $A$. Let . \\[\\begin{align*} E &amp;= \\text{Pr}(I_{A} = 1) \\cdot 100 \\\\[.5em] U &amp;= n(\\text{infected until elimination}) \\cdot 100 / N. \\end{align*}\\] The proportion of people over 60 infected until elimination $U_{60+}$ is calculated from . \\[U_{60+} / U = H_{60+} / H\\] and proportion of people below 60 infected until elimination $U_{&lt;60}$ from decomposition (\\ref{eq: decomposition}): . \\[U = (1 - d_{60+}) \\cdot U_{&lt;60} + d_{60+} \\cdot U_{60+}.\\] ",
    "url": "/blog/covid-calc/#probability-of-eliminating-covid-19",
    
    "relUrl": "/covid-calc/#probability-of-eliminating-covid-19"
  },"26": {
    "doc": "Covid Calculator",
    "title": "Expected number of infected",
    "content": "The expected number of infected in age group $g$ is estimated as . \\[\\mathbb{E} \\left[ n(\\text{infected in age group g}) \\right] = (1 - E/100) \\cdot N(g) \\cdot H_{\\text{*}} + E/100 \\cdot N(g) \\cdot U_{\\text{*}}.\\] Here, * in $U_{\\text{*}}$ is either &lt;60 or 60+. ",
    "url": "/blog/covid-calc/#expected-number-of-infected",
    
    "relUrl": "/covid-calc/#expected-number-of-infected"
  },"27": {
    "doc": "Covid Calculator",
    "title": "Expected number of deaths",
    "content": "Expected number of deaths in age group $g$ in $G$ is estimated as . \\[\\mathbb{E} \\left[ n(\\text{deaths in age group g}) \\right] = \\mathbb{E} \\left[ n(\\text{infected in age group g}) \\right] \\cdot \\text{FR}(g).\\] Total expected numbers are simply sums over all age groups . \\[\\begin{align*} \\mathbb{E} \\left[ n(\\text{infected}) \\right] &amp;= \\sum_{g \\text{ in } G} \\mathbb{E} \\left[ n(\\text{infected in age group g}) \\right] \\\\ \\mathbb{E} \\left[ n(\\text{deaths}) \\right] &amp;= \\sum_{g \\text{ in } G} \\mathbb{E} \\left[ n(\\text{deaths in age group g}) \\right]. \\end{align*}\\] ",
    "url": "/blog/covid-calc/#expected-number-of-deaths",
    
    "relUrl": "/covid-calc/#expected-number-of-deaths"
  },"28": {
    "doc": "Covid Calculator",
    "title": "Expected years of life lost",
    "content": "For this, the life table for global population (WHO, 2020) with estimates about expected number of life years left for all ages in 2016 was used. For example, a person at the age of 60 had 20.5 expected number of life years left in 2016. Next, by taking the average by gender and by age in a specific 10 year age group as an estimate of life expectancy of individual from a particular age group, calculate $\\text{life exp}_g$. Finally, estimate expected years of life lost (EYLL) due to Covid-19 as: . \\[\\text{EYLL} = \\sum_{g \\text{ in } G} \\mathbb{E} \\left[ n(\\text{deaths in age group g}) \\right] \\cdot \\text{life exp}_g.\\] ",
    "url": "/blog/covid-calc/#expected-years-of-life-lost",
    
    "relUrl": "/covid-calc/#expected-years-of-life-lost"
  },"29": {
    "doc": "Covid Calculator",
    "title": "Actual years of life lost",
    "content": "Years of life lost are estimated from confirmed deaths due to Covid-19 in the following way. First, calculate mean $\\text{CFR}$ by age group from all the $\\text{CFR}$ estimates by age group from countries as presented in Table 1. | CFR | 0-9 | 10-19 | 20-29 | 30-39 | 40-49 | 50-59 | 60-69 | 70-79 | 80+ | . | China | 0.0 | 0.2 | 0.20 | 0.20 | 0.40 | 1.3 | 3.6 | 8.0 | 14.8 | . | Korea | 0.0 | 0.0 | 0.00 | 0.11 | 0.08 | 0.5 | 1.8 | 6.3 | 13.0 | . | Italy | 0.0 | 0.0 | 0.00 | 0.30 | 0.40 | 1.0 | 3.5 | 12.8 | 20.2 | . | Spain | 0.0 | 0.0 | 0.22 | 0.14 | 0.30 | 0.4 | 1.9 | 4.8 | 15.6 | . | Mean | 0.0 | 0.05 | 0.105 | 0.1875 | 0.295 | 0.8 | 2.7 | 7.975 | 15.9 | . Table 1: CFR's by age group from different countries and mean CFR. Next, let $D$ be the event that a person died from Covid-19 and let $A_g$ be the event that this persons age is in the age group $g$. If there is $n(\\text{deaths})$ in selected country from Covid-19, estimate years of life lost (YLL) due to Covid-19 as: . \\[\\text{YLL} = n(\\text{deaths}) \\cdot \\sum_{g \\text{ in } G} Pr(A_g | D) \\cdot \\text{life exp}_g\\] The probability of a person who died in a particular country being in age group $g$ is estimated as . \\[Pr(A_g | D) = \\frac{Pr(D | A_g) Pr(A_g)}{P(D)}.\\] Here, . \\[\\begin{align*} Pr(D | A_g) &amp;= \\text{estimated mean } \\text{CFR}_g \\\\[.5em] Pr(A_g) &amp;= \\text{proportion of people in age group } g \\text{ in particular population} \\\\[.5em] Pr(D) &amp;= \\sum_{g \\text{ in } G} Pr(D | A_g) Pr(A_g). \\end{align*}\\] Example . For example, in Italy there were 24114 confirmed deaths from Covid-19 until the end of May. Based on the demographics and life expectancies of Italy, we estimate 294369.66 years of life lost in Italy from Covid-19. ",
    "url": "/blog/covid-calc/#actual-years-of-life-lost",
    
    "relUrl": "/covid-calc/#actual-years-of-life-lost"
  },"30": {
    "doc": "Covid Calculator",
    "title": "Projected costs",
    "content": "A figure of $129,000 represents what it would cost to give a person an additional quality-of-life adjusted year of life (Staff, 2020). This figure is multiplied with years of life lost to get estimated costs or money saved. ",
    "url": "/blog/covid-calc/#projected-costs",
    
    "relUrl": "/covid-calc/#projected-costs"
  },"31": {
    "doc": "Covid Calculator",
    "title": "Projected poverty",
    "content": "Take the difference between the IMF’s GDP growth forecasts (Fund, 2020) for April 2020 and forecasts from October 2019. We take estimates of how many people lived in extreme poverty (income below $1.9 per day) by country from 2015 from World Bank (Bank, 2020). Then estimate the projected poverty increases by country due to Covid-19 by multiplying this number and subtract the difference in GDB growth forecasts. Example . For example in Nigeria the difference between IMF’s GDP growth forecasts is –5.9 percent and World Bank estimated that there were 85.15 million people living in extreme poverty. So we estimate there will be . \\[5.15M (5.9 / 100) \\approx 5.02M\\] additional people living in extreme poverty in Nigeria due to Covid-19 pandemic. Note: This are crude estimats of Covid-19 effect on poverty because not only Covid-19 pandemic has caused the IMF to alter its forecasts. ",
    "url": "/blog/covid-calc/#projected-poverty",
    
    "relUrl": "/covid-calc/#projected-poverty"
  },"32": {
    "doc": "Covid Calculator",
    "title": "References",
    "content": ". | N Ferguson, D. L., &amp; et al., G. N. G. (2020). Report 9: Impact of non-pharmaceutical interventions (NPIs) to reduce COVID19 mortality and healthcare demand. https://doi.org/10.25561/77482 | Pyramids, P. (2020). Population Pyramids of the World from 1950 to 2100. https://www.populationpyramid.net/ | Wikipedia. (2020). Case fatality rate. https://en.wikipedia.org/wiki/Case_fatality_rate | WHO. (2020). Life tables. https://apps.who.int/gho/data/view.main.LIFEREGIONGLOBAL | Staff, S. G. S. B. (2020). How much will we pay for a year of life? https://www.gsb.stanford.edu/insights/how-much-will-we-pay-year-life | Fund, I. M. (2020). World Economic Outlook, April 2020: The Great Lockdown. https://www.imf.org/en/Publications/WEO/Issues/2020/04/14/weo-april-2020 | Bank, T. W. (2020). Poverty headcount ratio. https://data.worldbank.org/indicator/SI.POV.DDAY | . ",
    "url": "/blog/covid-calc/#references",
    
    "relUrl": "/covid-calc/#references"
  },"33": {
    "doc": "Covid Calculator",
    "title": "Covid Calculator",
    "content": "This blog post introduces Covid Calculator. This tool allows for interactive data visualization to explore and analyze the potential effects of Covid-19. The tool has been translated into multiple languages and allows users to derive their own country-specific estimates based on current data. It is accessible at: . https://markolalovic.github.io/covid-calc A screenshot of the Covid Calculator is shown in Figure 1. It shows the estimated number of deaths from Covid-19 based on the selected infection rate (30%) and infection fatality rates (N Ferguson &amp; et al., 2020), and how they compare to other major causes of deaths in the US. Figure 1: A screenshot of the Covid Calculator. It began as a simple bar chart that displayed mortality by age for a selected country. Over time, by adding new features, it evolved into a more complex tool. This blog post is meant to describe technical details. Disclaimer . The author is not a health expert or an epidemiologist and disclaims responsibility for any adverse effect resulting, directly or indirectly, from information contained in this website. For health, safety, and medical emergencies or updates on the novel coronavirus pandemic, you can get the latest information from WHO or search for official public health information for your country. ",
    "url": "/blog/covid-calc/",
    
    "relUrl": "/covid-calc/"
  },"34": {
    "doc": "Some math behind water towers",
    "title": "Optimal arrangement",
    "content": "To derive the optimal arrangement of the metal rings shown in Figure 2B, let’s assume that the water temperature in the tank is constant and the same everywhere. Also, the influence of depth on water density is negligible. Then the water pressure $p$ at depth $x$ is equal to: . \\[p(x) = p_{0} + \\rho \\cdot g \\cdot x\\] where $p_{0}$ is the pressure at the surface, $\\rho$ is the density of water and $g$ is the gravitational acceleration. The important part to notice for this derivation, is that the pressure exerted by a static liquid increases linearly with depth . \\[p(x) \\propto C \\cdot x\\] for some constant $C &gt; 0$. Choose the constant $C = 2/9$ and define the density . \\[f_{X}(x) = \\frac{2}{9} \\cdot x\\] of some continuously distributed random variable $X$ that represents the pressure in the tank defined on the interval $[0, 3]$. The corresponding distribution function is then: . \\[F_{X}(x) = \\frac{1}{9} \\cdot x^{2}.\\] Define the limits of the interval $[0, 3]$ to be . \\[x_{1} = 0 \\quad \\text{ and } \\quad x_{6} = 3\\] and divide the interval $[0, 3]$ into 5 subintervals with boundaries: . \\[x_{2} = x_{0.2}, \\quad x_{3} = x_{0.4}, \\quad x_{4} = x_{0.6}, \\quad x_{5} = x_{0.8}\\] where $x_{q}$ are quantiles, that is . \\[F_{X}(x_{q}) = q\\] The boundaries are marked as dashed red lines in Figure 2B. At each subinterval, the integral of the density is equal to 0.2. This method, of dividing a continuous distribution into subintervals with equal density, is called equal frequency discretization. Finally, we arrange the metal rings so that each ring is exactly in the centroid $h_{k}$​ of density $f_{X}$ at the subinterval $k$ . \\[h_{k} = \\frac{\\int_{x_{k}}^{x_{k+1}} x \\cdot f(x) dx}{\\int_{x_{k}}^{x_{k+1}} f(x) dx}\\] This way, each metal ring needs to hold the same amount of pressure. Therefore the structural strength is the same everywhere and this arrangement of metal rings is optimal. In the optimal arrangement, the calculated heights of the metal rings from the bottom of the tank are approximately . \\[h_{1} = 0.16, \\quad h_{2} = 0.49, \\quad h_{3} = 0.88, \\quad h_{4} = 1.36, \\quad h_{5} = 2.11.\\] Using this procedure, it is possible to calculate the optimal arrangement for any number of metal rings. For example, in Figure 1, the water tower in the front uses 11 metal rings. ",
    "url": "/blog/water-towers/#optimal-arrangement",
    
    "relUrl": "/water-towers/#optimal-arrangement"
  },"35": {
    "doc": "Some math behind water towers",
    "title": "Some math behind water towers",
    "content": "The structure of a water tower reveals some math behind how they are build. Figure 1 shows some water towers reinforced with metal rings to hold the cylindrical structure of the water tanks together. But what is the optimal arrangement of metal rings? . Figure 1: Water towers in New York. Image credit: © takomabibelot licensed CC-BY 2.0. Since I didn’t find a derivation anywhere on the internet, I will describe my solution in this blog post. Example . Suppose we have a wooden water tower that we want to reinforce with $K=5$ equal metal rings so that the wood does not give in to the water pressure. Let $d=3$ be the depth of the water tank. Denote by $h_{k}$ for $k=1, …, 5​​$ the height of each metal ring from the bottom of the tank. Two examples of arrangements are shown in Figure 2. Figure 2: Two examples of arrangements of metal rings. Figure 2A shows a naïve approach, where the metal rings are spaced evenly at heights from the bottom of the tank: . \\[h_{1} = 0.5, \\quad h_{2} = 1, \\quad h_{3} = 1.5, \\quad h_{4} = 2, \\quad h_{5} = 2.5.\\] The pressure that the water exerts increases with depth and the metal rings at the bottom of the tank have to hold a lot more pressure than the metal rings at the top of the tank. Therefore the structural strength of the lower part of the tank is lower than that of the upper part of the tank. Hence, this arrangement of metal rings is a poor choice. In Figure 2B, the metal rings are closer to each other at the bottom of the tank where the pressure that the water exerts on the tank is the highest. ",
    "url": "/blog/water-towers/",
    
    "relUrl": "/water-towers/"
  },"36": {
    "doc": "Topological features applied to the MNIST data set",
    "title": "Table of contents",
    "content": ". | Introduction | Terminology | Extracting the graph structure | Extracting the topological features | Empirical results | Source code | References | . ",
    "url": "/blog/tda-digits/#table-of-contents",
    
    "relUrl": "/tda-digits/#table-of-contents"
  },"37": {
    "doc": "Topological features applied to the MNIST data set",
    "title": "Introduction",
    "content": "Persistent homology is a fascinating mathematical tool that continues to be studied, developed, and applied. Topology applied to real-world data sets using persistent homology has begun to look for applications in machine learning, including deep learning (Brüel-Gabrielsson et al., 2020). Topology is mainly used in a pre-processing step to provide robust features for learning. The developed topological techniques, mostly deal with point clouds, i.e. finite sets of data points in space. Point clouds are typically captured by a variety of imaging devices, such as MRI or CT scanners. With the greater availability of such devices, this type of data is being generated at an increasing rate. Our data is often a finite set of noisy samples from some underlying space. The data sets are often very noisy and contain a lot of missing information, especially biological data sets. Our ability to analyze this data, both in terms of the amount and the nature of the data, is getting out of step with the data we generate (Carlsson, 2009). Topology can be used to make a useful contribution to the analysis of such data sets and it is especially helpful in studying them qualitatively. The problem we are trying to solve in this tutorial is how to extract the topological features that can be used as an input to standard machine learning algorithms. We will use a similar approach as described in (Adcock et al., 2013). From each image, we first construct a graph, where pixels of the image correspond to vertices of the graph and we add edges between adjacent pixels (Figure 2A,B). We then extract 0- and 1-dimensional topological features called Betti numbers denoted by $\\beta_{0}$ and $\\beta_{1}$. For example, a torus has one connected component giving $\\beta_{0} = 1$, and two cycles or loops, giving $\\beta_{1} = 2$ (Figure 2C). Figure 2: (A) handwritten digits (B) preprocessing and (C) a torus with two cycles on its surface. ",
    "url": "/blog/tda-digits/#introduction",
    
    "relUrl": "/tda-digits/#introduction"
  },"38": {
    "doc": "Topological features applied to the MNIST data set",
    "title": "Terminology",
    "content": ". | Topology is a branch of mathematics that deals with qualitative geometric information. This includes the classification of loops and higher-dimensional surfaces. | Topological data analysis and computational topology deal with the study of topology using a computer. | Persistent homology is an algebraic method for discerning topological features of data. | Connected component (or connected cluster of points) is a 0-dimensional feature and cycle (or loop) is a 1-dimensional feature. | Simplicial complex is a set composed of points, line segments, triangles, and their n-dimensional counterparts. | Filtration is the sequence of simplicial complexes, with an inclusion map from each simplicial complex to the next. | Barcode is a visual representation of the persistence of the topological features. Longer bars represent significant features of the data. Shorter bars are due to irregularities or noise. | . ",
    "url": "/blog/tda-digits/#terminology",
    
    "relUrl": "/tda-digits/#terminology"
  },"39": {
    "doc": "Topological features applied to the MNIST data set",
    "title": "Extracting the graph structure",
    "content": "The pre-processing steps to expose the topology of the digits, shown in the image of a handwritten digit 8 in Figure 3, are as follows. | Load the MNIST image of a handwritten digit. | Produce a binary image by thresholding. | Reduce the image to a skeleton of 1-pixel width using the popular Zhang-Suen thinning algorithm. | Construct a graph $G$ from the skeleton. | . Figure 3: Pre-procesing steps: the original image, binary image, skeleton and extracted graph. To construct the graph $G$ from the 1-pixel width skeleton, we treat the pixels of the skeleton as vertices of $G$ and add the edges between adjacent vertices and then remove all created cycles of length 3. Intuitively, we connect the points while trying not to create new topological features. The result for the image of a handwritten digit 8 is a graph in Figure 3. The resulting graph in this example consists of one connected component with two cycles, so no new topological features were added. ",
    "url": "/blog/tda-digits/#extracting-the-graph-structure",
    
    "relUrl": "/tda-digits/#extracting-the-graph-structure"
  },"40": {
    "doc": "Topological features applied to the MNIST data set",
    "title": "Extracting the topological features",
    "content": "Construct a simplex stream for computing the persistent homology using the following filtration on the graph $G$ embedded in the plane. Sweep across the plane while adding the vertices and edges of graph $G$. From the simplex stream, compute the persistent homology to get the Betti barcodes. The Dionysus 2 package can be used for the computation of persistent homology. See the package documentation (Morozov, Accessed on Sep 25, 2020) for details. Figure 4 shows the result of this technique applied on the image of a handwritten digit 8. In this example, we sweep across the plane to the top in a vertical direction $y \\in (-\\infty, \\infty)$. Figure 4: Result of applying this technique on (A) an image of a handwritten digit 8, (B) extracted embedded graph and (C) the resulting Betti barcodes at $y = 22$. Interactive example: https://markolalovic.github.io/tda-digits Betti 0 barcode $\\beta_{0}$ consists of one interval $[2, \\infty)$, which clearly shows the single connected component with the birth time of 2 when the first vertex of $G$ is added. Betti 1 barcode $\\beta_{1}$ consists of two intervals $[10, \\infty)$ and $[18, \\infty)$, with birth times of 10 and 18 corresponding to the births of the two cycles. The birth of the cycle is the value of $y$ when the loop closes in the embedded graph $G$ as we sweep to the top. Finally, extract the following features from each of the k-dimensional Betti barcodes in the following way. Denote the endpoints of Betti barcode intervals with: . \\[x_{1}, y_{1}, ..., x_{n}, y_{n}\\] where $x_{i}$ represents the beginning and $y_{i}$ the end of each interval. From the endpoints compute 4 features from the invariants discussed in (Adcock et al., 2013) that take into account all of the bars lengths and endpoints: . \\[\\begin{align*} f_{1}&amp;=\\sum_{i} x_{i} (y_{i} - x_{i}) \\\\ f_{2}&amp;=\\sum_{i} (y_{\\max} - y_{i})(y_{i} - x_{i}) \\\\ f_{3}&amp;=\\sum_{i} (y_{\\max} - y_{i})^{2} (y_{i} - x_{i})^{4} \\\\ f_{4}&amp;=\\sum_{i} x_{i}^{2} (y_{i} - x_{i})^{4} \\\\ \\end{align*}\\] For each of the 4 sweep directions: top, bottom, right, left and for each $k$-dimensional Betti barcode, $k = 0, 1$, we compute the defined 4 features $f_{1}, \\dots, f_{4}$. This gives us a total of $4 \\cdot 2 \\cdot 4 = 32$ features per image. ",
    "url": "/blog/tda-digits/#extracting-the-topological-features",
    
    "relUrl": "/tda-digits/#extracting-the-topological-features"
  },"41": {
    "doc": "Topological features applied to the MNIST data set",
    "title": "Empirical results",
    "content": "Data set consisted of extracted topological features of 10000 images of handwritten digits from MNIST database. Data set was split 50:50 in train and test set so that each had 5000 examples. SVM with RBF kernel was used for classification of the images based on the extracted topological features. The empirical classification results are as follows. Accuracy on the train set using 10-fold cross-validation was $0.88 (\\pm 0.05)$. Accuracy on the test set was 0.89. We now examine the common misclassifications. There were only 3 examples of the number 2 being mistaken for number 0, they are shown in Figure 5. The reason is that the number 2 was written with a loop that appears in the region that is close to the loop in number 0. Figure 5: Examples of number 2 being mistaken for number 0. For number 5 we got the lowest F1 score of 0.75. It was misclassified as number 2 in 32 examples in the test set. The first three examples are shown in Figure 6. This was expected since these two numbers are topologically the same with no topological features (e.g. loops) appearing in different regions. Figure 6: Examples of number 5 being mistaken for number 2. Number 8 was misclassified as number 4 in 21 examples from the test set. The first three examples are shown in Figure 7. We see the stylistic problems that caused the misclassifications. The top loop of number 8 was not closed which made it topologically more similar to number 4 written with a loop. Figure 7: Examples of number 8 being mistaken for number 4. ",
    "url": "/blog/tda-digits/#empirical-results",
    
    "relUrl": "/tda-digits/#empirical-results"
  },"42": {
    "doc": "Topological features applied to the MNIST data set",
    "title": "Source code",
    "content": "The repository containing all Python scripts for this tutorial including the processed version of the data set is available here: . https://github.com/markolalovic/tda-digits ",
    "url": "/blog/tda-digits/#source-code",
    
    "relUrl": "/tda-digits/#source-code"
  },"43": {
    "doc": "Topological features applied to the MNIST data set",
    "title": "References",
    "content": ". | Adcock, A., Carlsson, E., &amp; Carlsson, G. (2013). The Ring of Algebraic Functions on Persistence Bar Codes. https://arxiv.org/abs/1304.0530 | Brüel-Gabrielsson, R., Nelson, B. J., Dwaraknath, A., Skraba, P., Guibas, L. J., &amp; Carlsson, G. (2020). A Topology Layer for Machine Learning. https://arxiv.org/abs/1905.12200 | Carlsson, G. E. (2009). Topology and data. Bulletin of the American Mathematical Society, 46, 255–308. https://api.semanticscholar.org/CorpusID:1472609 | Morozov, D. (Accessed on Sep 25, 2020). Dionysus 2 documentation. https://mrzv.org/software/dionysus2/ | . ",
    "url": "/blog/tda-digits/#references",
    
    "relUrl": "/tda-digits/#references"
  },"44": {
    "doc": "Topological features applied to the MNIST data set",
    "title": "Topological features applied to the MNIST data set",
    "content": "The purpose of this tutorial is to give a friendly introduction on how to use the persistent homology that does not require substantial knowledge of topological methods. To illustrate the use of persistent homology in machine learning we apply it to the MNIST data set of handwritten digits. A very similar approach can be applied to any point cloud data and can be generalized to higher dimensions. A pure topological classification cannot distinguish between individual numbers, as the numbers are topologically too similar. For example, numbers 6 and 9 are topologically the same if we use this style for writing numbers. Persistent homology, however, gives us more information. The process is described as follows and illustrated in Figure 1. Define a filtration on the vertices of the graph corresponding to the image pixels, adding vertices and edges as we sweep across the image (Figure 1A,B). This adds spatial information to the topological features. For example, though 6 and 9 both have a single loop, it will appear at different locations in the filtration. Then, compute the persistent homology given the simplex stream from the filtration to get the finite set of intervals called Betti barcodes (Figure 1C). Click to animate the diagram: RUN . 0 . 2 . 4 . 6 . 8 . 10 . 12 . 14 . 16 . 18 . 20 . 22 . 0 . 2 . 4 . 6 . 8 . 10 . 12 . 14 . 16 . 18 . 20 . 22 . 0 . 2 . 4 . 6 . 8 . 10 . 12 . 14 . 16 . 18 . 20 . 22 . 0 . 2 . 4 . 6 . 8 . 10 . 12 . 14 . 16 . 18 . 20 . 22 . Figure 1: Applying this technique on (A) an image of a handwritten digit 8, (B) extracted embedded graph and (C) the resulting Betti barcodes. Finally, extract 4 features from the k-dimensional barcode from the invariants discussed in (Adcock et al., 2013). For each of 4 sweep directions: top, bottom, right, left and dimensions: 0 and 1, compute 4 features. This gives a total of 32 features per image. On the extracted features from a set of images, we then apply a support vector machine (SVM) learning algorithm to classify the images. This blog post starts by a short introduction to Topological Data Analysis and Computational Topology. Next, the extraction of topological features is explained. Finally, the empirical classification results on a subset of the MNIST database are presented including the evaluation. The aim is to demonstrate the classification potential of the technique and not to outperform the existing models. For a more interesting example of using this technique on a clinical data set to classify hepatic lesions, see (Adcock et al., 2013). All scripts are available on GitHub repository including a processed version of the dataset. Also, freely available computational topology package Dionysus 2 is used for the computation of persistent homology. ",
    "url": "/blog/tda-digits/",
    
    "relUrl": "/tda-digits/"
  },"45": {
    "doc": "Lasso dual",
    "title": "Table of contents",
    "content": ". | Formulation of the dual problem | Solution of the dual problem | Solution of the primal problem | References | . ",
    "url": "/blog/lasso-dual/#table-of-contents",
    
    "relUrl": "/lasso-dual/#table-of-contents"
  },"46": {
    "doc": "Lasso dual",
    "title": "Formulation of the dual problem",
    "content": "To derive the dual problem, introduce a dummy variable $z \\in \\mathbb{R}^{n}$ . \\[z = Ax\\] and reformulate the minimization problem in \\eqref{eq:min-problem} as a constrained problem P: . \\[\\begin{align*} \\label{eq:primal-problem} \\tag{P} &amp;\\underset{z \\in \\mathbb{R}^{n}, \\, x \\in \\mathbb{R}^{p}}{\\text{minimize}} \\quad \\frac{1}{2} \\norm{y - z}_{2}^{2} + \\alpha \\norm{x}_{1} \\\\ &amp;\\text{subject to } \\quad z = Ax \\end{align*}\\] Then, construct the Lagrangian by introducing the dual variable $p \\in \\mathbb{R}^{n}$, containing $n$ Lagrange multipliers: . \\[L(x, z, p) = \\frac{1}{2} \\norm{y - z}_{2}^{2} + \\alpha \\norm{x}_{1} + p^{T} (z - Ax).\\] The dual objective function is: . \\[g(p) = \\min_{z \\in \\mathbb{R}^{n}, \\, x \\in \\mathbb{R}^{p}} \\left\\lbrace \\frac{1}{2} \\norm{y - z}_{2}^{2} + \\alpha \\norm{x}_{1} + p^{T} (z - Ax) \\right\\rbrace\\] We will split the terms depending on $z$ and $x$ and minimize each part separately: . \\[\\begin{align*} g(p) &amp;= \\min_{z \\in \\mathbb{R}^{n}, \\, x \\in \\mathbb{R}^{p}} \\left\\lbrace \\frac{1}{2} \\norm{y}_{2}^{2} - y^{T}z + \\frac{1}{2} \\norm{z}_{2}^{2} + p^{T}z + \\alpha \\norm{x}_{1} - p^{T}Ax \\right\\rbrace \\\\[.5em] &amp;= \\min_{z \\in \\mathbb{R}^{n}} \\left\\lbrace \\frac{1}{2} \\norm{y}_{2}^{2} - (y - p)^{T}z + \\frac{1}{2} \\norm{z}_{2}^{2} \\right\\rbrace + \\max_{x \\in \\mathbb{R}^{p}} \\left\\lbrace \\alpha \\norm{x}_{1} - (A^{T}p)^{T}x \\right\\rbrace \\end{align*}\\] The stationarity condition says that at the optimal point, the subgradient of $L(x, z, p)$ with respect to $x$ and $z$ must contain 0. For the first part, since $L(x, z, p)$ is differentiable in $z$, the subgradient with respect to $z$ equals the gradient. By taking $\\frac{\\partial}{\\partial z} L(x, z, p)$ and setting it to $0$, we get the stationarity condition: . \\[z = y - p^{*}\\] Plugging this into the first part, yields: . \\[\\frac{1}{2}\\norm{y}_{2}^{2} - (y - p^{*})^{T}(y - p^{*}) + \\frac{1}{2}\\norm{y - p^{*}}_{2}^{2} = \\frac{1}{2}\\norm{y}_{2}^{2} - \\frac{1}{2}\\norm{y - p^{*}}_{2}^{2}.\\] For the second part, because $\\alpha \\norm{x}_1$ is a non-differentiable function of $x$, we need to compute the subdifferential $\\partial (\\alpha \\norm{x}_1)$. From the rules for the subgradient, see (Boyd &amp; Vandenberghe, 2008), we can derive that the $\\partial \\norm{x}_{1}$ can be expressed as follows: . \\[\\partial (\\norm{x}_{1}) = \\lbrace g : \\norm{g}_{\\infty} \\leq 1, g^{T} x = \\norm{x}_{1} \\rbrace.\\] Furthermore, by using the rule for scalar multiplication: . \\[\\partial (\\alpha\\norm{x}_{1}) = \\lbrace g : \\norm{g}_{\\infty} \\leq \\alpha, g^{T} x = \\alpha\\norm{x}_{1} \\rbrace\\] we get the following stationarity condition: . \\[g = A^{T}p \\in \\partial \\alpha \\norm{x}_{1}\\] when . \\[\\begin{align*} \\norm{A^{T}p}_{\\infty} &amp;\\leq \\alpha \\\\[.5em] \\alpha \\norm{x}_{1} &amp;= (A^{T}p)^{T}x. \\end{align*}\\] Therefore, the dual problem D is: . \\[\\begin{align*} \\label{eq:dual-problem} \\tag{D} &amp;\\max_{p \\in \\mathbb{R}^{n} } \\, \\frac{1}{2}\\norm{y}_{2}^{2} - \\frac{1}{2}\\norm{y - p}_{2}^{2} \\\\[.5em] &amp;\\text{subject to } \\, \\norm{A^{T}p}_{\\infty} \\leq \\alpha \\end{align*}\\] ",
    "url": "/blog/lasso-dual/#formulation-of-the-dual-problem",
    
    "relUrl": "/lasso-dual/#formulation-of-the-dual-problem"
  },"47": {
    "doc": "Lasso dual",
    "title": "Solution of the dual problem",
    "content": "The solution $p^{*}$ of the dual problem will be determined with the help of a projection operator. Looking at the dual problem formulation \\eqref{eq:dual-problem}, we see that $\\frac{1}{2}\\norm{y}_{2}^{2}$ can be omitted, since it doesn’t depend on $p$. Then, multiplying it by 2 and reversing the sign, we arive at equivalent dual problem: . \\[\\begin{align*} \\label{eq:dual-problem-prime} \\tag{D'} &amp;\\min_{p \\in \\mathbb{R}^{n} } \\, \\norm{y - p}_{2}^{2} \\\\[.5em] &amp;\\text{subject to } \\, \\norm{A^{T}p}_{\\infty} \\leq \\alpha. \\end{align*}\\] Let $C \\subset \\mathbb{R}^{n}$ be closed and convex set. Define a projection operator $P_{C}$ as: . \\[\\DeclareMathOperator*{\\argmin}{arg\\,min} \\begin{align*} P_{C} \\text{: } &amp; \\mathbb{R}^{n} \\rightarrow C \\\\[.5em] &amp; y \\mapsto P_{C}(y) := \\argmin_{p \\in C} \\norm{y - p}_{2}. \\end{align*}\\] Looking at \\eqref{eq:dual-problem-prime}, we see that $p^{*}$ is a projection . \\[p^{*} = P_{C}(y)\\] where $C$ is equal to: . \\[C = \\lbrace p \\in \\mathbb{R}^{n} : \\norm{A^{T}p}_{\\infty} \\leq \\alpha \\rbrace.\\] Notice that $C$ is indeed closed and convex. It can be expressed as: . \\[\\begin{equation}\\nonumber C = (A^{T})^{-1}(D) \\end{equation}\\] where $D$ is equal to: . \\[D = \\lbrace d \\in \\mathbb{R}^{p} : \\norm{d}_{\\infty} \\leq \\alpha \\rbrace.\\] For example, let $n = 2, p = 2$ and . \\[A^{T} = \\begin{pmatrix} a_{11} &amp; a_{21}\\\\ a_{12} &amp; a_{22} \\end{pmatrix}.\\] Then, . \\[\\begin{equation}\\nonumber C = \\lbrace -\\alpha \\leq a_{11} p_{1} + a_{12} p_{2} \\leq \\alpha \\rbrace \\cap \\lbrace -\\alpha \\leq a_{21} p_{1} + a_{22} p_{2} \\leq \\alpha \\rbrace \\end{equation}\\] and . \\[D = \\lbrace -\\alpha \\leq d_{1} \\leq \\alpha \\rbrace \\cap \\lbrace -\\alpha \\leq d_{2} \\leq \\alpha \\rbrace.\\] This is illustrated in Figure 2. Figure 2: Illustration when $n=p=2$ of primal and dual admissible sets $C$ and $D$. ",
    "url": "/blog/lasso-dual/#solution-of-the-dual-problem",
    
    "relUrl": "/lasso-dual/#solution-of-the-dual-problem"
  },"48": {
    "doc": "Lasso dual",
    "title": "Solution of the primal problem",
    "content": "From the optimality condition of the dual problem, we can infer the primal solution given the assumption of the existence of $A^{-1}$. During the formulation of the dual problem, we introduced the dummy variable $z = Ax$ and derived the stationary condition $z = y - p^{*}$. This implies that every solution $x^{*}$ of \\eqref{eq:primal-problem} should satisfy” . \\[A x^{*} = y - p^{*}\\] where $p^{*}$ is a solution of the dual problem \\eqref{eq:dual-problem}. Therefore, if $A^{-1}$ exists, the solution is: . \\[x^{*} = A^{-1} \\left( y - P_{C}(y) \\right).\\] Since $P_{C}$ is a projection onto a convex set $C$, it follows that $x^{*}$ is also nonexpansive mapping as a function of $y$. This is not obvious and would probably be hard to show without the dual formulation. The nonexpansive property: . \\[\\norm{P_{C}(y) - P_{C}(\\tilde{y})} \\leq \\norm{y - \\tilde{y}}.\\] is demonstrated in Figure 3A. For constrast, the projection onto a non-convex set $N$ is depiced in Figure 3B. Figure 3: Lasso solution $x^{*}$ is non-expansive as a function of $y$. ",
    "url": "/blog/lasso-dual/#solution-of-the-primal-problem",
    
    "relUrl": "/lasso-dual/#solution-of-the-primal-problem"
  },"49": {
    "doc": "Lasso dual",
    "title": "References",
    "content": ". | Tibshirani, R. (1996). Regression Shrinkage and Selection Via the Lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267–288. https://doi.org/https://doi.org/10.1111/j.2517-6161.1996.tb02080.x | Tikhonov, A. N., &amp; Arsenin, V. Y. (1977). Solutions of ill-posed problems (p. xiii+258). V. H. Winston \\&amp; Sons. https://catalogue.nla.gov.au/catalog/720231 | Boyd, S., &amp; Vandenberghe, L. (2008). Subgradients Notes. https://see.stanford.edu/materials/lsocoee364b/01-subgradients_notes.pdf | . ",
    "url": "/blog/lasso-dual/#references",
    
    "relUrl": "/lasso-dual/#references"
  },"50": {
    "doc": "Lasso dual",
    "title": "Lasso dual",
    "content": "Let $A: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{n}$ and $y \\in \\mathbb{R}^{p}$. Consider the following regularization problem: . \\[\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\begin{equation} \\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\norm{Ax - y}_{2}^{2} + \\alpha \\norm{x}_{1}. \\label{eq:min-problem} \\end{equation}\\] This type of penalized regression is known as Lasso, see Tibshirani’s original paper (Tibshirani, 1996). It belongs to a more general type of regularization known as Tikhonov regularization (Tikhonov &amp; Arsenin, 1977). In this post, we first derive the dual problem, then show that the solution $x^{*}$ can be determined with the help of a projection operator. Under the assumption that $A^{-1}$ exists, the solution $x^{*}$ can be further expressed in terms of a mapping $\\left(A^{T}\\right)^{-1}$ from the dual space as a function of $y$. This is illustrated in Figure 1. Figure 1: Illustration when $n=p=3$ of primal and dual admissible sets $C$ and $D$. Solution to (1) can be determined by a projection of $y$. Furthermore, if $A^{-1}$ exists, then the primal solution can be expressed as a function of $y$. ",
    "url": "/blog/lasso-dual/",
    
    "relUrl": "/lasso-dual/"
  },"51": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "Table of contents",
    "content": ". | Introduction | Optical model for a solar tower power plant | Cosine effect | Spillage effect | Atmospheric attenuation | Optimal position for a single heliostat | Multiple heliostats . | Shading and blocking effects | Discretization and ray tracing | The layout optimization problem | Solving the layout optimization problem . | Example | . | . | Source code | References | . ",
    "url": "/blog/layout-optimization/#table-of-contents",
    
    "relUrl": "/layout-optimization/#table-of-contents"
  },"52": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "Introduction",
    "content": "Concentrating solar power systems such as solar tower power plants concentrate sunlight to produce electricity, offering a renewable energy production. However, the cost of electricity from these systems remain high compared to other renewable sources, as indicated in Figure 3. Figure 3: Global weighted average cost of electricity. Data from: IRENA (2022). Efforts in research and development are focused on improving efficiency, cutting construction and operation expenses, and ultimately reducing electricity production costs. Figure 4 shows an experimental solar power tower plant at the German Aerospace Center. Figure 4: Experimental solar power plant near Jülich in Germany. Source: DLR (CC-BY 3.0). ",
    "url": "/blog/layout-optimization/#introduction",
    
    "relUrl": "/layout-optimization/#introduction"
  },"53": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "Optical model for a solar tower power plant",
    "content": "We begin by examining a solar power tower plant employing a single heliostat and determining its optimal position. Although actual plants employ thousands of heliostats, focusing on a single one makes it easier to explain the model. Furthermore, the derived solution for a single heliostat allows us to verify the accuracy of the optimization methods to find the optimal layout for multiple heliostats. The model introduced here operates in two dimensions to simplify complexity, but a more realistic approach would involve replacing all definitions with their three-dimensional counterparts. The normal $n(t)$ of heliostat at time $t$ is given by . \\[\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} n(t) = \\frac{r + s(t)}{\\norm{r + s(t)}}.\\] Here, $r$ denotes the direction from the center $H$ of the heliostat to the center of the receiver $R$, while $s(t)$ is the normalized solar vector at time $t$, as illustrated in Figure 5. Figure 5: Vectors $r, s(t)$, and normal $n(t)$ of a heliostat. ",
    "url": "/blog/layout-optimization/#optical-model-for-a-solar-tower-power-plant",
    
    "relUrl": "/layout-optimization/#optical-model-for-a-solar-tower-power-plant"
  },"54": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "Cosine effect",
    "content": "The primary energy loss in a solar power plant occurs when sunlight does not strike the heliostat perpendicularly. The effective reflection area of the heliostat is reduced by the cosine of one-half of the angle between vectors $r$ and $s$. This phenomenon is known as the cosine effect: . \\[\\eta_{cos} = \\cos{\\left( \\frac{\\gamma}{2} \\right)}\\] where . \\[\\gamma = \\angle (r, s).\\] As depicted in Figure 6A, an $\\eta_{cos}$ value approximately 0 indicates minimal power output, while an $\\eta_{cos}$ value close to 1 signifies maximum power output (Figure 6B). Figure 6: Cosine effect on the power output. No power (A) when $\\eta_{cos} \\approx 0$. Maximum power (B) when $\\eta_{cos} \\approx 1.$ In order to determine the optimal position of the heliostat to maximize the power output, we will express the cosine effect $\\eta_{cos}$ in terms of the heliostat’s center $H$. Let $\\rho$ denote the angle between a line passing through the receiver’s center $R$ and the heliostat’s center $H$, and the horizontal line passing through $R$. Additionally, let $\\phi$ denote the angle between the direction of sunlight and the ground’s horizontal plane, as shown in Figure 7. Figure 7: Angles $\\gamma$, $\\rho$ and $\\phi.$ In this simplified model, as the sun’s position changes throughout the day, the solar angle $\\phi$ ranges from 0 to $\\pi$. The angle $\\gamma = \\angle (r, s)$ can be expressed in terms of $\\phi$ and $\\rho$ as: . \\[\\phi = |\\pi - \\phi - \\rho|\\] and cumulative daily cosine effect can be expressed in terms of $\\rho$ as: . \\[\\begin{equation} \\eta_{cos}(\\rho) = \\int_{0}^{\\pi} \\cos \\left( \\frac{|\\pi - \\phi - \\rho |}{2} \\right) d\\phi = 2 \\left( \\sin \\left( \\frac{\\rho}{2} \\right) + \\cos \\left(\\frac{\\rho}{2}\\right) \\right). \\label{eq: cosine-effect} \\end{equation}\\] ",
    "url": "/blog/layout-optimization/#cosine-effect",
    
    "relUrl": "/layout-optimization/#cosine-effect"
  },"55": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "Spillage effect",
    "content": "Solely focusing on the cumulative daily cosine effect, the optimal position of the heliostat to maximize power output is directly underneath the receiver. This corresponds to $\\rho = \\pi/2$, as indicated by equation \\ref{eq: cosine-effect}. However, in this position, many reflected rays from the heliostat fail to reach the receiver at various times throughout the day as illustrated in Figure 8A. Figure 8: Spillage effect on the power output. No power (A) when $\\eta_{spill} \\approx 0$. Maximum power (B) when $\\eta_{spill} = 1.$ This effect on the energy loss is known as spillage effect and can be defined as follows: . \\[\\eta_{spill} = \\text{the proportion of rays that reach the receiver.}\\] To derive the spillage effect $\\eta_{spill}$, let $\\tau$ be the angle between the line passing through the receiver’s surface and the ground’s horizontal plane, as shown in Figure 9. Figure 9: Notation in the derivation of the spillage effect. Note that, . \\[\\begin{align*} \\alpha &amp;= \\pi - \\tau - \\rho \\\\[.5em] \\beta &amp;= (\\phi + \\rho) / 2. \\end{align*}\\] By similarity and sine rule, . \\[\\begin{align*} \\frac{|AQ|}{|AR|} &amp;= \\frac{|AP|}{|AH|} \\\\[.5em] \\frac{|AR|}{\\sin{\\alpha}} &amp;= \\frac{|AH|}{\\sin{\\beta}} \\end{align*}\\] we can derive the spillage effect in terms of angles $\\rho, \\tau$ and $\\phi$ as . \\[\\begin{equation} \\eta_{spill} = \\min \\left( 1, \\frac{|HP|}{|RQ|} \\right) = \\min \\left( 1, \\frac{\\sin{(\\pi - \\tau - \\rho)} }{ \\sin{\\left((\\phi + \\rho)/2\\right)} } \\right). \\label{eq: spillage-effect} \\end{equation}\\] Note that, in our model, the angle $\\tau \\approx 80^{\\circ}$ and it is a fixed constant because it is part of plant’s specification. Therefore $\\eta_{spill}$ only varies with respect to $\\phi$ and $\\rho$. ",
    "url": "/blog/layout-optimization/#spillage-effect",
    
    "relUrl": "/layout-optimization/#spillage-effect"
  },"56": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "Atmospheric attenuation",
    "content": "Another loss of energy is due to atmospheric attenuation. This loss depends on the distance $d$ between a heliostat and the receiver and assumes a visibility of 40 km. See (Richter, 2017) for more details. Denote by $d$ the distance in meters from the center $H$ of the heliostat to the center of the receiver $R$: . \\[\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} d = \\norm{H - R}\\] The atmospheric attenuation effect, shown in Figure 10, is given by the formula: . \\[\\eta_{aa}(d) = \\begin{cases} 0.99321 - 1.176 \\cdot 10^{-4} d + 1.97 \\cdot 10^{-8} d^{2}, \\quad d \\leq 1000 \\, m\\\\ \\exp(-1.106 \\cdot 10^{-4} d), \\quad d &gt; 1000 \\, m \\end{cases}\\] Figure 10: Atmospheric attenuation effect. ",
    "url": "/blog/layout-optimization/#atmospheric-attenuation",
    
    "relUrl": "/layout-optimization/#atmospheric-attenuation"
  },"57": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "Optimal position for a single heliostat",
    "content": "To determine the optimal position for the center $H(\\rho, d)$ of a single heliostat we define the following objective function called daily energy production of a power plant: . Daily Energy Production (DEP) . \\[\\text{DEP}(\\rho, d) = \\frac{1}{\\pi} \\, \\eta_{aa}(d) \\, \\int_{0}^{\\pi} \\eta_{cos}(\\rho, \\phi) \\, \\eta_{spill}(\\rho, \\phi) \\, d\\phi\\] Note: . | The normalizing constant is $1/\\pi$ because when there are no losses, all effects are 1 and $\\int_{0}^{\\pi} d\\phi = \\pi$. | In this simplified model, as the sun’s position changes throughout the day, the solar angle $\\phi$ ranges from 0 to $\\pi$. This is why we integrate over $\\phi$ from 0 to $\\pi$. | . To determine the optimal position for $H(\\rho, d)$ that maximizes DEP, we need to find the optimal distance $d^{*}$ and optimal angle $\\rho^{*}$. Since $\\eta_{aa}(d)$ is a monotonically decreasing function, . \\[\\DeclareMathOperator*{\\argmin}{arg\\,min} d^{*} = \\argmin_{d} \\norm{ H - R } \\quad \\text{ subject to: } \\norm{ H - R } &gt; 4 \\, m.\\] In our model, the heliostat and receiver are both 4 meters in size and the rotation of heliostats adds a constraint $\\norm{ H - R } &gt; 4 \\, m.$ (See Figure 11B.) Therefore, the optimal distance is $d^{*} = 4 \\, m$. Plugging into DEP the derived equations \\ref{eq: cosine-effect} and \\ref{eq: spillage-effect} for $\\eta_{cos}$ and $\\eta_{spill}$ and integrating, we get DEP with respect to $\\rho$ shown in Figure 11A. The optimal angle $\\rho$ is approximately $30^{\\circ}$. The optimal position for a single heliostat is shown in Figure 11B. Figure 11: (A) Optimal angle $\\rho$ and (B) optimal position for a single heliostat. ",
    "url": "/blog/layout-optimization/#optimal-position-for-a-single-heliostat",
    
    "relUrl": "/layout-optimization/#optimal-position-for-a-single-heliostat"
  },"58": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "Multiple heliostats",
    "content": "Shading and blocking effects . In case of multiple heliostats, individual heliostats can be blocked or shaded by neighboring heliostats, which affects the efficiency of the power plant (Figure 12). The effects of shading and blocking are defined as . \\[\\begin{align*} \\eta_{shade} &amp;= \\text{ proportion of incoming rays not shaded by neighboring heliostats} \\\\[.5em] \\eta_{block} &amp;= \\text{ proportion of reflected rays not blocked by neighboring heliostats}. \\end{align*}\\] Figure 12: The effects of shading (A) and blocking (B). Discretization and ray tracing . Finding the optimal layout using only geometry is challenging when there are multiple heliostats.Therefore, discretization and ray tracing are used to estimate the reflected power of each heliostat that is hitting the receiver. For each heliostat, we divide the surface of the heliostat into equal-area regions. We then trace a ray from each region in the direction of the sun and in the direction of the receiver. Figure 13 shows ray tracing for $m = 5$ rays. Figure 13: Tracing $m = 5$ rays from the surface of the heliostat towards the sun and towards the receiver. Denote by $m$ the number of rays per heliostat and by $\\eta_{ray, i}$ the proportion of rays that are received at the surface of heliostat $i$ and hit the surface of the receiver. The layout optimization problem . For a given plant with $n$ heliostats with centers: . \\[H_{1}, H_{2}, \\dots, H_{n}\\] we approximate the daily energy production by . \\[\\widehat{\\text{DEP}}(H_{1}, H_{2}, \\dots, H_{n}) = \\frac{1}{n \\, m} \\sum_{i = 1}^{n} \\eta_{aa, i} \\, \\sum_{k = 1}^{m} \\eta_{cos, i}(\\phi_{k}) \\, \\eta_{ray, i}.\\] We can now define the layout optimization problem in a more formal manner. Layout optimization problem . Determine the locations $H_{i}$ of heliostats $i = 1, 2, \\dots, n$, that maximize the $\\widehat{\\text{DEP}}$ subject to: . | all $H_{i}$ are in the specified field area, | all $H_{i}, H_{j}$ where $i \\neq j$ are at least $4$ meters appart, | $H_{i}, R$ are at least $4$ meters appart. | . An example of a valid layout for $n = 5$ heliostats is in Figure 14. Here the field area is specified as follows. Maximum distance from receiver’s center to a heliostat’s center is 35 meters and maximum height of heliostat’s center is 10 meters. Figure 14: Field area and a valid layout for $n = 5$ heliostats. Solving the layout optimization problem . Note that the objective function $\\widehat{\\text{DEP}}$ is not convex as indicated by Figure 15. It shows the plot of $\\widehat{\\text{DEP}}$ as we vary the location of $H_{3}$ in the layout shown in Figure 14. Figure 15: Plot of $\\widehat{\\text{DEP}}(H_{3})$ as we vary the location of $H_{3}$. Starting far from the global optimum and running a gradient ascent can lead us to a sub-optimal solution. Similarly with higher order methods. Despite this, we can still observe some interesting results. Example . Running Sequential Quadratic Programming (SQP) method from a class of Lagrange-Newton methods starting from some initial layout with 3 heliostats, the method converges to the layout show in Figure 16. See the Source code for more details. Figure 16: Converged layout using SQP for $n = 3$ heliostats. It places the heliostats some distance appart on the parabola with focal point $R$: . \\[y = \\frac{1}{48} x^{2}.\\] The layout we get for $n=5$ heliostats is shown in Figure 1. Intuitively this seems to be the best option. Generally speaking, the heliostats do not need to rotate so much to reflect the rays towards the receiver. Furthermore, the heliostats should be far apart in order to minimize the effects of shading and blocking. In particular, the effects of shading and blocking are most noticable when the cosine effect is minimized, that is when $\\phi$ is roughly 90 degrees. In Figure 17 is sketch of the shadow from heliostat $H_{j}$ considering the values of $\\phi$ close to 90 degrees. Figure 17: Sketch of the shadow from heliostat $H_{j}$ for values of $\\phi$ close to 90 degrees. ",
    "url": "/blog/layout-optimization/#multiple-heliostats",
    
    "relUrl": "/layout-optimization/#multiple-heliostats"
  },"59": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "Source code",
    "content": "The repository containing all Python scripts is available here: . https://github.com/markolalovic/math-mods-camp The code for Example above: . import sys import numpy as np from scipy import optimize # load the specifications of the plant hypo_plant = utils.load(\"../data/plants/tiny-plant.json\") # set the initial layout basic_layout = np.array([[10, 10], [20, 5], [30, 10]]) x0 = basic_layout.flatten() # objective function def f(x): plant.layout = x.reshape((3, 2)) plant.set_layout() return -utils.get_energy(plant) # set the bounds and constraints bounds = [] for i in range(3): bounds.append((0, 35)) bounds.append((0, 10)) def g01(x): i, j = 0, 1 layout = x.reshape(3, 2) distance = np.linalg.norm(layout[i] - layout[j]) return distance - 4 def g02(x): i, j = 0, 2 layout = x.reshape(3, 2) distance = np.linalg.norm(layout[i] - layout[j]) return distance - 4 def g12(x): i, j = 1, 2 layout = x.reshape(3, 2) distance = np.linalg.norm(layout[i] - layout[j]) return distance - 4 constraints = [{\"type\": \"ineq\", \"fun\": g01}, {\"type\": \"ineq\", \"fun\": g02}, {\"type\": \"ineq\", \"fun\": g12}] result = optimize.minimize(f, x0, method=\"SLSQP\", bounds=bounds, constraints=constraints, options={'disp': True, 'maxiter': 100}) # Optimization terminated successfully (Exit mode 0) # Current function value: -41.033956692960274 # Iterations: 28 # Function evaluations: 317 # Gradient evaluations: 28 . ",
    "url": "/blog/layout-optimization/#source-code",
    
    "relUrl": "/layout-optimization/#source-code"
  },"60": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "References",
    "content": ". | Richter, P. (2017). Simulation and optimization of solar thermal power plants [PhD thesis, RWTH Aachen University]. https://doi.org/10.18154/RWTH-2017-05351 | . ",
    "url": "/blog/layout-optimization/#references",
    
    "relUrl": "/layout-optimization/#references"
  },"61": {
    "doc": "Layout optimization for a solar tower power plant",
    "title": "Layout optimization for a solar tower power plant",
    "content": "A solar power tower plant uses flat, rotating mirrors called heliostats to reflect and concentrate solar radiation onto a tower-mounted receiver. Figure 1 shows heliostats reflecting sunlight during solar noon, when the sun is at its zenith in the sky. Figure 1: Schematic of a solar power tower plant. Throughout the day, each heliostat rotates to ensure that the sunlight remains focused on the receiver by using a tracking system as shown in Figure 2. Click to animate the diagram: RUN . Figure 2: Heliostat rotation. Note that the position of the sun varies, and individual heliostats can be blocked or shaded by neighboring heliostats, which affects the efficiency of the power plant. In addition, there are other factors that affect the power plant’s efficiency as described in Table 1. | Name | Description | . | Cosine effect | Loss due to the sun's rays not being perpendicular to the heliostat. | . | Spillage effect | Loss due to reflected rays missing the receiver. | . | Atmospheric attenuation | Loss due to atmospheric absorption. | . | Shading effect | Sun rays being blocked by neighboring heliostats heliostats. | . | Blocking effect | Reflected rays being blocked by neighboring heliostats. | . Table 1: Key factors influencing plant efficiency. Given the specifications of the power plant, including the tower height and receiver dimensions, the task is to determine the optimal locations of heliostats that maximize the plant’s efficiency. This optimal arrangement of heliostats is referred to as optimal layout. Following a brief introduction, a simplified model is presented for determining the optimal layout. The task of finding an optimal layout for a single heliostat is solved using only geometry. In the case of multiple heliostats, discretization and ray tracing are used to estimate the efficiency of a solar power plant taking into account shading and blocking effects. Then, the layout is optimized using Sequential Quadratic Programming (SQP). ",
    "url": "/blog/layout-optimization/",
    
    "relUrl": "/layout-optimization/"
  },"62": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "Table of contents",
    "content": ". | Introduction . | Example | . | Preliminaries | SDP relaxation . | Example | . | Hyperplane rounding | Dual problem | SoS relaxation | Gaussian rounding | Generalizations | Source code | References | . ",
    "url": "/blog/max-cut-sdp/#table-of-contents",
    
    "relUrl": "/max-cut-sdp/#table-of-contents"
  },"63": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "Introduction",
    "content": "Let $G = (V, E)$ be a simple undirected graph with $V := \\lbrace 1, \\dots, n \\rbrace$ and $|E| = m$. For a subset $S \\subset V$, we define a $cut(S)$ as a subset of edges $E$ having one vertex in $S$ and the other one in $V \\setminus S$. We denote the cut size by $f(S) = |cut(S)|$. The MaxCut problem is to find a subset $S \\subseteq V$ that maximizes the cut size $f(S)$. MaxCut problem is NP-complete (Karp, 1972). Figure 1 shows a cycle graph $C_{5}$ where a subset $S = \\lbrace{v_1, v_3\\rbrace}$ gives a cut of maximum size 4. Figure 1: Cycle graph $C_5$ and a maximum cut of size 4. Given a graph $G=(V,E)$, denote the maximum cut size by $mc(G)$. For a given algorithm that returns the subset $S \\subseteq V$, we say that it is an $\\alpha$-approximation algorithm for MaxCut if . \\[\\begin{equation} f(S) \\geq \\alpha \\cdot mc(G) \\label{eq: alpha} \\end{equation}\\] for all graphs $G = (V, E)$ and some approximation ratio $\\alpha \\in [0, 1]$. If algorithm employed is randomized, meaning $S$ that it returns is a random variable, then we say the same, if (\\ref{eq: alpha}) holds with an expectation taken on the left-hand side. Example . An algorithm, that assigns each vertex to $S$ or $V \\setminus S$ independently uniformly at random, is a $1/2-$approximation for MaxCut: . \\[\\DeclareMathOperator{\\Ex}{\\mathbb{E}} \\DeclareMathOperator{\\Prob}{\\mathbb{P}} \\mathbb{E}[f(S)] = \\sum_{i, j} \\Prob[(i, j) \\in \\text{cut}(S)] = \\frac{1}{2} \\, m \\geq \\frac{1}{2} \\cdot mc(G).\\] This approximation was first proposed and analyzed by (Erdős, 1967). In (Goemans &amp; Williamson, 1995) they improved this by proposing $\\alpha_{GW}$-approximation algorithm with $\\alpha_{GW} \\geq 0.878$ using semidefinite programming (SDP) and hyperplane rounding technique. The proposed algorithm is easy to implement using off-the-shelf SDP solver, see the Source code for more details. ",
    "url": "/blog/max-cut-sdp/#introduction",
    
    "relUrl": "/max-cut-sdp/#introduction"
  },"64": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "Preliminaries",
    "content": "A real symmetric matrix $X$ is positive definite, denoted $X \\succeq 0$ if the following equivalent conditions hold: . | All eigenvalues of $X$ are non-negative. | Quadratic form $v^{T}Xv \\geq 0$ for all $v \\in \\mathbb{R}^{n}$. | There exists $Q \\in \\mathbb{R}^{n \\times r}$ with | . \\[X = QQ^{T} = \\sum_{i = 1}^{r} v_{i} v_{i}^{T}\\] where $v_{1}, \\dots, v_{r}$ are columns of $Q$. In the set of all symmetric matrices $S^{n}$, denote the convex cone of positive semidefinite matrices by . \\[S^{n}_{+}.\\] A spectrahedron is the intersection of $S^{n}_{+}$ with an affine linear space $A \\bullet X = b$. Semidefinite programming (SDP) solves the problem (P) of maximizing (or minimizing) a linear objective function $C \\bullet X$ over the spectrahedron: . \\[\\begin{align*} &amp; \\text{maximize} \\ C \\bullet X \\\\ &amp; \\text{subject to: } \\tag{P}\\\\ &amp; \\qquad A_{i} \\bullet X = b_{i} \\quad i = 1, \\dots, m \\\\ &amp; \\qquad X \\succeq 0 \\end{align*}\\] The corresponding dual problem (D) is: . \\[\\begin{align*} &amp; \\text{minimize} \\ b^{T}y \\\\ &amp; \\text{subject to: } \\label{eq: P} \\tag{D}\\\\ &amp; \\qquad \\sum_{i=1}^{m} A_{i} y_{i} - C \\succeq 0 \\end{align*}\\] Properties: . | Every convex polyhedron is a spectrahedron. | Spectrahedron is a closed convex set. | Weak duality always holds: . | . \\[C \\bullet X \\geq b^{T} y.\\] . | Under primal and dual feasibility, also strong duality holds: | . \\[C \\bullet X = b^{T} y.\\] More on semidefinite programming can be found in Chapter 12 (Michalek &amp; Sturmfels, 2021). ",
    "url": "/blog/max-cut-sdp/#preliminaries",
    
    "relUrl": "/max-cut-sdp/#preliminaries"
  },"65": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "SDP relaxation",
    "content": "Cuts in the complete graph $K_n$ can be represented by a set of $2^{n - 1}$ matrices $x x^{T}$ of rank one with $x \\in \\lbrace -1, 1 \\rbrace^{n}$. The $n \\times n$ spectrahedron: . \\[\\mathcal{E}_{n} := \\lbrace X \\in S^{n}_{+} : X_{ii} = 1, \\, \\, \\forall i \\rbrace\\] is called an elliptope. It is a set of all $n \\times n$ correlation matrices. It gives the following formulation of maximum cut problem: . \\[mc(G) = \\max_{X \\in \\mathcal{E}_{n}, rk(X) = 1 } \\sum_{i, j} \\frac{1 - X_{ij}}{2} .\\] Letting go of rank one constraint $rk(X) = 1$, we get an SDP problem: . \\[sdp(G) := \\max_{X \\in \\mathcal{E}_{n}} \\sum_{i, j} \\frac{1 - X_{ij}}{2}.\\] Since $\\mathcal{E}_{n}$ includes all the matrices of rank one, . \\[mc(G) \\leq sdp(G).\\] Therefore $sdp(G)$ is a relaxation. Example . Cuts in complete graph $K_{3}$ can be represented by a set of matrices of rank one: . \\[\\lbrace x x^{T} : x \\in \\lbrace -1, 1 \\rbrace^{3} \\rbrace.\\] These matrices are contained in $3 \\times 3$ elliptope . \\[\\mathcal{E}_{3} = \\lbrace (x_1, x_2, x_3)^{T} \\in \\mathbb{R}^{3} : \\begin{pmatrix} 1 &amp; x_1 &amp; x_2 \\\\ x_1 &amp; 1 &amp; x_3 \\\\ x_2 &amp; x_3 &amp; 1 \\end{pmatrix} \\succeq 0 \\rbrace.\\] The boundary of $\\mathcal{E}_{3}$ consists of all $(x_1, x_2, x_3)^{T} \\in \\mathbb{R}^{3}$ where determinant is 0: . \\[(x_2 x_3 - x_1) x_1 + (x_1 x_3 - x_2) x_2 - x_{3}^2 + 1 = 0.\\] This is a cubic surface drawn in Figure 2 using Python and SageMath: . from sage.all import * var('x,y,z') f(x, y, z) = (y*z - x)*x + (x*z - y)*y - z^2 + 1 p = implicit_plot3d(f(x,y,z) == 0, (x,-1,1), (y,-1,1), (z,-1,1), opacity=0.9, plot_points=50, smooth=True) p . Figure 2: Complete graph $K_3$ and surface of elliptope $\\mathcal{E}_{3}$. ",
    "url": "/blog/max-cut-sdp/#sdp-relaxation",
    
    "relUrl": "/max-cut-sdp/#sdp-relaxation"
  },"66": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "Hyperplane rounding",
    "content": "Solution $X$ that realizes the maximum $sdp(G)$ is a positive semidefinite matrix that can be decomposed as $X=\\sum_{i=1} v_{i}v_{i}^T$ for some unit vectors $v_{i}$. To construct a subset $S \\subset V$, select a random unit vector $r \\in \\mathbb{R}^{n}$ and let $S = \\lbrace i \\in V : v_{i}^{T}r \\geq 0 \\rbrace$. Denote the angle between vectors $v_i, v_j$ by $\\theta_{i, j}$. Since $v_{i}$ are unit vectors $v_{i}^{T} v_{j} = \\cos{\\theta_{i, j}}$, we can bound the contribution of an edge $(i, j) \\in E(G)$ by . \\[\\Prob[(i, j) \\in cut(S)] = \\frac{\\theta_{i, j}}{\\pi} \\geq \\alpha_{GW} \\cdot \\frac{1 - v_{i}^{T} v_{j} }{2}\\] where . \\[\\alpha_{GW} = \\min_{\\theta_{i, j} \\in [0, \\pi]} \\lbrace \\frac{2}{\\pi} \\cdot \\frac{\\theta_{i, j}}{ { 1 - \\cos(\\theta_{i, j}) }} \\rbrace \\geq 0.878.\\] Summing up the contributions over all edges, we arrive at the famous result . \\[\\Ex[f(S)] = \\sum_{i, j} \\Prob[(i, j) \\in cut(S)] \\geq \\alpha_{GW} \\cdot sdp(G).\\] ",
    "url": "/blog/max-cut-sdp/#hyperplane-rounding",
    
    "relUrl": "/max-cut-sdp/#hyperplane-rounding"
  },"67": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "Dual problem",
    "content": "Let $L$ be the Laplacian matrix of a graph $G = (V, E)$. The dual problem of SDP relaxation is equivalent to finding maximum eigenvalue of $L$ with smallest added correction $u \\in \\mathbb{R}^{n}$, under constraint that $\\sum_{i} u_{i} = 0$ . \\[\\newcommand{\\vect}[1]{\\boldsymbol{#1}} \\frac{n}{4} \\min_{u: \\vect{1}^{T} u = 0} \\lambda_{\\max}(L + diag(u))\\] Setting $u = \\vect{0}$ and by weak duality we get an upper bound . \\[mc(G) \\leq sdp(G) \\leq \\frac{n}{4} \\lambda_{\\max}(L)\\] given earlier in (Mohar &amp; Poljak, 1990). For 5-cycle $C_{5}$, $\\lambda_{\\max} = \\frac{1}{5}(5 + \\sqrt{5})$, giving an upper bound . \\[\\frac{1}{2}(5 + \\sqrt{5})/4 \\geq 0.9 \\cdot mc(C_{5}).\\] This bound was studied in (Delorme &amp; Poljak, 1993). ",
    "url": "/blog/max-cut-sdp/#dual-problem",
    
    "relUrl": "/max-cut-sdp/#dual-problem"
  },"68": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "SoS relaxation",
    "content": "Now let $x \\in \\lbrace 0, 1 \\rbrace^{n}$ and let $\\mathcal{P}_{n}$ be a set of real valued polynomials $p(x)$ of degree at most $d/2$. Let . \\[\\tilde{\\Ex}: \\mathcal{P}_{n} \\rightarrow \\mathbb{R}\\] be some operator over the probability distribution on $x$. We formulate the following optimization problem . \\[\\begin{align*} sos(G) := \\ &amp; \\max_{\\tilde{\\Ex}} \\ \\tilde{\\Ex}[ \\sum_{i, j} (x_{i} - x_{j})^{2} ] \\\\ &amp; \\text{subject to: } \\\\[.4em] &amp; \\qquad \\text{(1) $\\tilde{\\Ex}$ is linear} \\qquad \\text{(2) $\\tilde{\\Ex}[1] = 1$} \\\\[.4em] &amp; \\qquad \\text{(3) $\\tilde{\\Ex}[p^2] \\geq 0$} \\qquad \\, \\, \\, \\text{(4) $\\tilde{\\Ex}[x_{i}^{2}p] = \\tilde{\\Ex}[x_{i}p]$} \\\\[.4em] &amp; \\qquad \\forall p \\in \\mathcal{P}_{n} \\end{align*}\\] An operator $\\tilde{\\Ex}$ that realizes the maximum $sos(G)$ is called pseudo-expectation, because it only meets a subset of constraints (1)-(3) required to be an actual expectation. The fourth constraint requires $x$ to be a vector $x \\in \\lbrace 0, 1 \\rbrace^{n}$. Given a subset $S$ with maximum cut size $mc(G)$, let $\\vect{a}$ be the indicator vector of $S$ and set $\\tilde{\\Ex} = \\vect{a}$. Then $\\tilde{\\Ex}$ satisfies the constraints (1)-(4) and achieves objective value $mc(G)$. In other words, given an expectation $a$, we can build a pseudo-expectation $\\tilde{\\Ex}$ just by setting $\\tilde{\\Ex} = \\vect{a}$. Therefore $mc(G) \\leq sos(G)$. This framework is called Sum-of-Squares hierarchy introduced by (Parrilo, 2000) and (Lasserre, 2001). Increasing the degree $d$, increases the size of the corresponding SDP problem. For $d = n$, the relaxation is exact. We can choose $d=2$ and show that we can get the same $\\alpha_{GW}$-approximation as before by rounding the solution of SoS relaxation. ",
    "url": "/blog/max-cut-sdp/#sos-relaxation",
    
    "relUrl": "/max-cut-sdp/#sos-relaxation"
  },"69": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "Gaussian rounding",
    "content": "Let $\\tilde{\\Ex}$ be solution that realizes the maximum $sos(G)$. Select $\\vect{y}$ to be a Gaussian vector with the mean $\\mu = \\tilde{\\Ex}[x] = \\frac{1}{2} \\vect{1}$ and covariance matrix $\\Sigma = \\tilde{\\Ex}[(x - \\mu)(x - \\mu)^{T}]$ and construct a subset $S = \\lbrace i \\in V : y_{i} \\leq \\frac{1}{2} \\rbrace$. For each edge $(i, j) \\in E(G)$, define . \\[\\rho_{i, j} = 4 \\tilde{\\Ex}[x_{i} x_{j}] - 1.\\] From $(s, t) \\overset{\\text{i.i.d.}}{\\sim} N(0, I)$, setting . \\[u = \\rho_{i, j} s + \\sqrt{1 - \\rho_{i, j}^{2}} t\\] gives $\\rho_{i, j} = \\Ex[u s] = \\cos(\\theta_{i, j})$. Summing up the contributions, we can show that . \\[\\Ex[f(S)] = \\sum_{i, j} \\Prob[(i, j) \\in cut(S)] \\geq \\alpha_{GW} \\cdot sos(G)\\] ",
    "url": "/blog/max-cut-sdp/#gaussian-rounding",
    
    "relUrl": "/max-cut-sdp/#gaussian-rounding"
  },"70": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "Generalizations",
    "content": "Given a non-negative weight function on the edges, $w \\in \\mathbb{R}^{E}_{+}$, the cut size becomes . \\[f(S) = \\sum_{e \\in \\text{cut}(S)} w_e\\] with no effect on analysis of approximation algorithms presented here. Given arbitrary weight function $w \\in \\mathbb{R}^{E}$, the analysis has to be adjusted but it is possible to derive a generalization of guarantees presented here. Whether increasing degree from $d=2$ to $d=4$ of SoS Relaxation, also improves the approximation, still remains unresolved question. ",
    "url": "/blog/max-cut-sdp/#generalizations",
    
    "relUrl": "/max-cut-sdp/#generalizations"
  },"71": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "Source code",
    "content": "Here is a simple implementation of Goemans-Williamson algorithm in Python: . import numpy as np import cvxpy as cp from scipy.linalg import sqrtm def gw(n, edges): '''Goemans-Williamson algorithm for MaxCut: Given a graph G(V=[n], E=edges), returns a vector x \\in {-1, 1}^n that corresponds to the chosen subset of vertices S of V. ''' ## SDP Relaxation X = cp.Variable((n, n), symmetric=True) constraints = [X &gt;&gt; 0] constraints += [ X[i, i] == 1 for i in range(n) ] objective = sum( 0.5*(1 - X[i, j]) for (i, j) in edges ) prob = cp.Problem(cp.Maximize(objective), constraints) prob.solve() ## Hyperplane Rounding Q = sqrtm(X.value).real r = np.random.randn(n) x = np.sign(Q @ r) return x def cut(x, edges): '''Given a vector x \\in {-1, 1}^n and edges of a graph G(V=[n], E=edges), returns the edges in cut(S) for the subset of vertices S of V represented by x.''' xcut = [] for i, j in edges: if np.sign(x[i]*x[j]) &lt; 0: xcut.append((i, j)) return xcut . To solve an SDP problem it uses CVXPY (Diamond &amp; Boyd, Version 1.2). Algorithm is randomized and returns a random cut, but guarantees that on average: cut size &gt;= 0.878 * maximum cut size. For the trivial example of a cycle on 5 vertices, the chosen subset and corresponding cut may vary, but the return cut will always be of size 4: . ## Define a cycle on 5 vertices n = 5 edges = [(0, 1), (1, 2), (2, 3), (3, 4), (0, 4)] ## Run Goemans-Williamson algorithm x = gw(n, edges) ## Find edges in the cut xcut = cut(x, edges) print('Chosen subset: %s' % np.where(x == 1)) print('Cut size: %i' % len(xcut) ) print('Edges of the cut: %s' % xcut ) # Chosen subset: [1 4] # Cut size: 4 # Edges of the cut: [(0, 1), (1, 2), (3, 4), (0, 4)] . ",
    "url": "/blog/max-cut-sdp/#source-code",
    
    "relUrl": "/max-cut-sdp/#source-code"
  },"72": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "References",
    "content": ". | Goemans, M. X., &amp; Williamson, D. P. (1995). Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming. J. ACM, 42(6), 1115–1145. https://doi.org/10.1145/227683.227684 | Karp, R. M. (1972). Reducibility among Combinatorial Problems. In R. E. Miller, J. W. Thatcher, &amp; J. D. Bohlinger (Eds.), Complexity of Computer Computations: Proceedings of a symposium on the Complexity of Computer Computations, held March 20--22, 1972, at the IBM Thomas J. Watson Research Center, Yorktown Heights, New York, and sponsored by the Office of Naval Research, Mathematics Program, IBM World Trade Corporation, and the IBM Research Mathematical Sciences Department (pp. 85–103). Springer US. https://doi.org/10.1007/978-1-4684-2001-2_9 | Erdős, P. (1967). On bipartite subgraphs of a graph. Matematika Lapok, 18, 283–288. | Michalek, M., &amp; Sturmfels, B. (2021). Invitation to nonlinear algebra, Chapter 12: Semidefinite Programming. American Mathematical Society. https://www.math.uni-konstanz.de/~michalek/june26.pdf | Mohar, B., &amp; Poljak, S. (1990). Eigenvalues and the max-cut problem. Czechoslovak Mathematical Journal, 40(2), 343–352. http://eudml.org/doc/13856 | Delorme, C., &amp; Poljak, S. (1993). Laplacian Eigenvalues and the Maximum Cut Problem. Math. Program., 62(1–3), 557–574. | Parrilo, P. A. (2000). Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization [PhD thesis, California Institute of Technology]. https://doi.org/10.7907/2K6Y-CH43 | Lasserre, J. B. (2001). Global Optimization with Polynomials and the Problem of Moments. SIAM Journal on Optimization, 11(3), 796–817. https://doi.org/10.1137/S1052623400366802 | Diamond, &amp; Boyd. (Version 1.2). CVXPY: A Python-embedded modeling language for convex optimization. https://www.cvxpy.org/ | . ",
    "url": "/blog/max-cut-sdp/#references",
    
    "relUrl": "/max-cut-sdp/#references"
  },"73": {
    "doc": "Maximum cut and Goemans-Williamson",
    "title": "Maximum cut and Goemans-Williamson",
    "content": "This is a short summary of approximation algorithms for the MaxCut problem of finding a maximum cut of a graph. After introducing the problem and trivial $\\frac{1}{2}$-approximation, we summarize the famous semidefinite programming relaxation and hyperplane rounding technique from (Goemans &amp; Williamson, 1995) that gives the best known approximation ratio for MaxCut. We then take a look at the dual problem and some previous results. Taking the dual approach further, using so-called Sum-of-Squares hierarchy framework with Gaussian rounding technique, we arrive at the same approximation ratio for MaxCut. Finally, we discuss some possible generalizations. ",
    "url": "/blog/max-cut-sdp/",
    
    "relUrl": "/max-cut-sdp/"
  }
}
