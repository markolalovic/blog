<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Marko's Blog</title>
  <meta name="description" content="        Figure 1: As the correlation between regressors increases, the OLS method becomes unstable. In the limit $|corr(x_{i}, x_{j})| \rightarrow 1$, the OL...">
  <script src="/blog/assets/css/main.css/assets/main.js"></script>
  <link rel="stylesheet" href="/blog/assets/css/main.css">
  <link rel="canonical" href="https://markolalovic.github.io/blog/ols-regression">
</head>


<body>

    <div class="page-wrapper">
        <header class="site-header">

    <link rel="apple-touch-icon" sizes="180x180" href="/docs/assets/img/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/docs/assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/docs/assets/img/favicon-16x16.png">
    <link rel="manifest" href="/docs/assets/img/site.webmanifest">
    <link rel="mask-icon" href="/docs/assets/img/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/docs/assets/img/favicon.ico">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-config" content="/docs/assets/img/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">

    <div class="wrapper">
        <a class="site-title" href="/blog">Marko Lalovic</a>
        <a class="site-nav" href="https://markolalovic.github.io">About</a>
        <a class="site-nav" href="/blog">Blog</a>
    </div>

</header>


        <div class="page-content">
            <div class="wrapper">
                <div class="post">

    <header class="post-header">
        <h1 class="post-title">Multicollinearity Effect on OLS Regression</h1>
        <h2 class="post-subtitle">The cause of inflation effect of highly correlated regressors on ordinary least squares estimator.</h2>
        <p class="post-meta">Jul 14, 2019 • Marko Lalovic • 3 min read</p>
    </header>

    <article class="post-content">
        <div class="images">
  <img src="/blog/assets/posts/ols-regression/problem.svg" />
  <div class="label">
    <strong>Figure 1:</strong> As the correlation between regressors increases, the OLS method becomes unstable. In the limit $|corr(x_{i}, x_{j})| \rightarrow 1$, the OLS objective function is no longer strictly convex and OLS solution is no longer unique.
  </div>
</div>

<p>In <em>ordinary least squares (OLS) regression</em>, for given $X: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$ and $y \in \mathbb{R}^{n}$, we minimize over $\beta \in \mathbb{R}^{p}$, the sum of squared residuals</p>

\[\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\begin{equation}
S(\beta) = \norm{ y - X\beta }_{2}^{2}
\end{equation}\]

<p>We first illustrate the problem with using ordinary least squares (OLS) method to estimate the unknown parameters $\beta$ in the case of highly correlated regressors on a simple example in R.</p>

<p>Suppose we have a model $y \sim \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2}$, more specifically, let</p>

<p>\begin{equation}
\beta_{0} = 3, \quad \beta_{1} = \beta_{2} = 1.
\end{equation}</p>

<p>and let the sample contain 100 elements</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span><span class="w">
</span></code></pre></div></div>

<p>We then introduce some highly correlated regressors</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">x1</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w">
</span><span class="n">x2</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>with correlation coefficient almost 1</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">0.999962365268769</span><span class="w">
</span></code></pre></div></div>

<p>Let’s run the OLS method 1000 times to get a sense of the effect of highly correlated regressors</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">intr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">))</span><span class="w">
</span><span class="n">nsim</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1000</span><span class="w">
</span><span class="n">betas</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">nsim</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">y</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
    </span><span class="n">xx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">solve</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w">
    </span><span class="n">beta.ols</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.vector</span><span class="p">(</span><span class="n">xx</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="n">beta.ols</span><span class="p">)</span><span class="w">
</span><span class="p">})</span><span class="w">
</span></code></pre></div></div>

<p>The estimator for $\beta$, obtained by the OLS method, is still unbiased</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">round</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">),</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">2.996</span><span class="w"> </span><span class="m">1.006</span><span class="w"> </span><span class="m">0.993</span><span class="w">
</span></code></pre></div></div>

<p>But the variance becomes to large</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">round</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="p">),</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w">  </span><span class="m">0.101</span><span class="w"> </span><span class="m">11.110</span><span class="w"> </span><span class="m">11.103</span><span class="w">
</span></code></pre></div></div>

<p>The estimated coefficients can become too large, some can even have the wrong sign</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">round</span><span class="p">(</span><span class="n">betas</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w">  </span><span class="m">3.002</span><span class="w"> </span><span class="m">-7.673</span><span class="w">  </span><span class="m">9.529</span><span class="w">
</span></code></pre></div></div>

<p>The problem can be seen by drawing a contour plot of the objective function $S(\beta)$</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ssr</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="nf">sum</span><span class="p">((</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">xlen</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span><span class="w">
</span><span class="n">ylen</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span><span class="w">
</span><span class="n">xgrid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-10.1</span><span class="p">,</span><span class="w"> </span><span class="m">10.1</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">xlen</span><span class="p">)</span><span class="w">
</span><span class="n">ygrid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-10.1</span><span class="p">,</span><span class="w"> </span><span class="m">10.1</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ylen</span><span class="p">)</span><span class="w">
</span><span class="n">zvals</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">xlen</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ylen</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">xlen</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">j</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">ylen</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">zvals</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ssr</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">xgrid</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">ygrid</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">xgrid</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ygrid</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">zvals</span><span class="p">,</span><span class="w">
        </span><span class="n">levels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1e3</span><span class="p">,</span><span class="w"> </span><span class="m">3e3</span><span class="p">,</span><span class="w"> </span><span class="m">6e3</span><span class="p">,</span><span class="w"> </span><span class="m">1e4</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>As shown in <strong>Figure 1</strong>, as the correlation between regressors increases, matrix $X$ becomes nearly singular and OLS method becomes unstable.</p>

<p>In the limit</p>

<p>\begin{equation}
|corr(x_{i}, x_{j})| \rightarrow 1
\end{equation}</p>

<p>the dimension of the column space decreases</p>

<p>\begin{equation}
\text{rank}(X) &lt; p
\end{equation}</p>

<p>the objective function $S(\beta)$ is no longer strictly convex, and there are infinitely many solutions of OLS.</p>

<p>Thinking about this in a different, more intuitive way, we would like to estimate the coefficient $\beta_{1}$ as the influence of $x_{1}$ on $y$ without the influence of $x_{2}$. Since the regressors $x_{1}$ and $x_{2}$ are highly correlated, they vary together and the coefficient $\beta_{1}$ is difficult to estimate.</p>

<p>The OLS method does not solve these problems, as it only minimizes the sum of squared residuals, i.e. the objective function $S(\beta)$.</p>

<h3 id="why-does-this-happen">Why does this happen?</h3>

<p>In general, we have a linear model</p>

<p>\begin{equation}
\label{eq: model}
y = X \beta + \epsilon
\end{equation}</p>

<p>and let for errors $\epsilon$ hold the assumption (Gauss–Markov)</p>

<p>\begin{equation}
\mathbb{E}[\epsilon \epsilon^{T}] = \sigma^{2} I
\end{equation}</p>

<p>the estimator according to the OLS method is</p>

<p>\begin{equation}
\label{eq: betahat}
\hat{\beta} = (X^{T}X)^{-1} X^{T} y.
\end{equation}</p>

<p>From \ref{eq: model} and \ref{eq: betahat} we get</p>

<p>\begin{equation}
\hat{\beta} - \beta = (X^{T}X)^{-1} X^{T} \epsilon
\end{equation}</p>

<p>therefore, the covariance matrix for $\hat{\beta}$ is</p>

\[\begin{align}
\mathbb{E}[(\hat{\beta} - \beta) (\hat{\beta} - \beta)^{T}] \nonumber
&amp;= \mathbb{E}[\left((X^{T}X)^{-1} X^{T} \epsilon\right) \left((X^{T}X)^{-1} X^{T} \epsilon\right)^{T}] \nonumber \\[1em]
&amp;= \mathbb{E}[(X^{T}X)^{-1} X^{T} \epsilon \epsilon^{T} X (X^{T}X)^{-1}] \nonumber \\[1em]
&amp;= (X^{T}X)^{-1} X^{T} \mathbb{E}[\epsilon \epsilon^{T}] X (X^{T}X)^{-1} \nonumber \\[1em]
&amp;= \sigma^{2} (X^{T}X)^{-1}
\label{eq: variance}
\end{align}\]

<p>where we took into account that the matrix $(X^{T}X)$ is symmetric and assumed that it is not stochastic and is independent of $\epsilon$. The variance for the coefficient $\hat{\beta}_{k}$ is the $(k, k)$-th element of the covariance matrix.</p>

<p>The average distance between the estimator $\hat{\beta}$ and the actual $\beta$ is</p>

\[\begin{align}
\mathbb{E}[(\hat{\beta} - \beta)^{T} (\hat{\beta} - \beta)] \nonumber
&amp;= \mathbb{E}[((X^{T}X)^{-1} X^{T} \epsilon)^{T} ((X^{T}X)^{-1} X^{T} \epsilon)] \nonumber \\[1em]
&amp;= \mathbb{E}[\epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} \epsilon] \nonumber \\[1em]
&amp;= \mathbb{E}[\text{tr}(\epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} \epsilon)] \nonumber \\[1em]
&amp;= \mathbb{E}[\text{tr}(\epsilon \epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} )] \nonumber \\[1em]
&amp;= \sigma^{2} \text{tr}( X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} )] \nonumber \\[1em]
&amp;= \sigma^{2} \text{tr}( X^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1})] \nonumber \\[1em]
&amp;= \sigma^{2} \text{tr}((X^{T}X)^{-1})
\label{eq: distance}
\end{align}\]

<p>where we took into account that the distance is a scalar, so its expected value will be equal to its trace. We then used the fact that the trace is invariant with respect to cyclic permutations</p>

\[\text{tr}(ABCD) = \text{tr}(DABC)\]

<p>From \ref{eq: variance} and \ref{eq: distance}, we see that both the variance of the estimator and the distance of the estimator from the actual $\beta$ depend on the matrix $(X^{T}X)^{-1}$.</p>

<p>The reason why the variance of the estimator and the distance of the estimator from the actual $\beta$ become large, can be shown conveniently with the help of singular value decomposition. Let</p>

<p>\begin{equation}
X = U \Sigma V^{T}
\end{equation}</p>

<p>be the singular value decomposition of $X$ where $\Sigma$ contains all the singular values</p>

<p>\begin{equation}
\sigma_{1} \geq \sigma_{2} \geq \dots \geq \sigma_{p} &gt; 0
\end{equation}</p>

<p>then</p>

\[\begin{align}
(X^{T}X)^{-1} &amp;= (V \Sigma^{T} \Sigma V^{T})^{-1} \nonumber \\[1em]
&amp;= (V^{T})^{-1} (\Sigma^{T} \Sigma)^{-1} V^{-1} \nonumber \\[1em]
&amp;= V (\Sigma^{T} \Sigma)^{-1} V^{T} \nonumber \\[1em]
&amp;= \sum_{j = 1}^{p} \frac{1}{\sigma_{j}^{2}} v_{j} v_{j}^{T}
\label{eq: svd}
\end{align}\]

<p>In the limit</p>

<p>\begin{equation}
| corr(x_{i}, x_{j}) | \rightarrow 1
\end{equation}</p>

<p>matrix $X$ becomes a singular and the smallest singular value vanishes</p>

<p>\begin{equation}
\sigma_{p} \rightarrow 0
\end{equation}</p>

<p>and, from \ref{eq: svd}, also</p>

<p>\begin{equation}
(X^{T}X)^{-1} \rightarrow \infty
\end{equation}</p>

<p>Therefore, from \ref{eq: variance} and \ref{eq: distance}, both the variance of the OLS estimator and the distance of the OLS estimator to the actual $\beta$ go to infinity.</p>

    </article>

    <div>
      <section class="comments" id="comment-section">
  <hr>
  
  <!-- Existing comments -->
  <div class="comments__existing">
    <h2>Comments</h2>
    
    
    <!-- List main comments in reverse date order, newest first. List replies in date order, oldest first. -->
    
    

<article id="comment-bb7cbf60-04fb-11ec-b695-df44408f21c8" class="js-comment comment" uid="bb7cbf60-04fb-11ec-b695-df44408f21c8">

  <div class="comment__author">
    Marko
    <span class="comment__date">•
        <a href="#comment-bb7cbf60-04fb-11ec-b695-df44408f21c8" title="Permalink to this comment">August 24th, 2021 18:52</a></span>
  </div>

  <div class="comment__body">
    <p><strong>Works</strong>
$X \rightarrow Y$</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-bb7cbf60-04fb-11ec-b695-df44408f21c8', 'respond', 'ols-regression', 'bb7cbf60-04fb-11ec-b695-df44408f21c8')">↪&#xFE0E; Reply to Marko</a>
    </div>
</article>
  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
    

<article id="comment-d30651d0-04f8-11ec-b695-df44408f21c8" class="js-comment comment" uid="d30651d0-04f8-11ec-b695-df44408f21c8">

  <div class="comment__author">
    Marko
    <span class="comment__date">•
        <a href="#comment-d30651d0-04f8-11ec-b695-df44408f21c8" title="Permalink to this comment">August 24th, 2021 18:32</a></span>
  </div>

  <div class="comment__body">
    <p>Testing</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-d30651d0-04f8-11ec-b695-df44408f21c8', 'respond', 'ols-regression', 'd30651d0-04f8-11ec-b695-df44408f21c8')">↪&#xFE0E; Reply to Marko</a>
    </div>
</article>
  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
  </div>
  

  <!-- New comment form -->
  <div id="respond" class="comment__new">
    <form class="js-form form" method="post" action="https://markos-staticman.herokuapp.com/v2/entry/markolalovic/blog/main/comments">
  <input type="hidden" name="options[origin]" value="https://markolalovic.github.io/blog/ols-regression">
  <input type="hidden" name="options[parent]" value="https://markolalovic.github.io/blog/ols-regression">
  <input type="hidden" id="comment-replying-to-uid" name="fields[replying_to_uid]" value="">
  <input type="hidden" name="options[slug]" value="ols-regression">
  

  <div class="textfield">
    <label for="comment-form-message"><h2>Add Comment<small><a rel="nofollow" id="cancel-comment-reply-link" href="https://markolalovic.github.io/blog/ols-regression#respond" style="display:none;">(cancel reply)</a></small></h2>
      <textarea class="textfield__input" name="fields[message]" type="text" id="comment-form-message" placeholder="Your comment (Markdown and TeX support within $...$ or $$...$$ delimiters)" required rows="6"></textarea>
    </label>
  </div>

    <div class="textfield narrowfield">
      <label for="comment-form-name">Name
        <input class="textfield__input" name="fields[name]" type="text" id="comment-form-name" placeholder="Your name (required)" required/>
      </label>
    </div>

    <div class="textfield narrowfield">
      <label for="comment-form-email">E-mail
        <input class="textfield__input" name="fields[email]" type="email" id="comment-form-email" placeholder="Your email (optional)"/>
      </label>
    </div>

    <div class="textfield narrowfield hp">
      <label for="hp">
        <input class="textfield__input" name="fields[hp]" id="hp" type="text" placeholder="Leave blank">
      </label>
    </div>

    

    <button class="button" type="button" onclick="previewComment()">Preview</button>
    <button class="button" id="comment-form-submit">Submit</button>

    <div class="textfield">
      <h1 id="comment-result-title"></h1>
      <p id="comment-result"></p>
    </div>

</form>

<article class="modal mdl-card mdl-shadow--2dp">
  <div>
    <h3 class="modal-title js-modal-title"></h3>
  </div>
  <div class="mdl-card__supporting-text js-modal-text"></div>
  <div class="mdl-card__actions mdl-card--border">
    <button class="button mdl-button--colored mdl-js-button mdl-js-ripple-effect js-close-modal">Close</button>
  </div>
</article>

<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="display:none" >
  <symbol id="icon-loading" viewBox="149.8 37.8 499.818 525"><path d="M557.8 187.8c13.8 0 24.601-10.8 24.601-24.6S571.6 138.6 557.8 138.6s-24.6 10.8-24.6 24.6c0 13.2 10.8 24.6 24.6 24.6zm61.2 90.6c-16.8 0-30.6 13.8-30.6 30.6s13.8 30.6 30.6 30.6 30.6-13.8 30.6-30.6c.6-16.8-13.2-30.6-30.6-30.6zm-61.2 145.2c-20.399 0-36.6 16.2-36.6 36.601 0 20.399 16.2 36.6 36.6 36.6 20.4 0 36.601-16.2 36.601-36.6C595 439.8 578.2 423.6 557.8 423.6zM409 476.4c-24 0-43.2 19.199-43.2 43.199s19.2 43.2 43.2 43.2 43.2-19.2 43.2-43.2S433 476.4 409 476.4zM260.8 411c-27 0-49.2 22.2-49.2 49.2s22.2 49.2 49.2 49.2 49.2-22.2 49.2-49.2-22.2-49.2-49.2-49.2zm-10.2-102c0-27.6-22.8-50.4-50.4-50.4-27.6 0-50.4 22.8-50.4 50.4 0 27.6 22.8 50.4 50.4 50.4 27.6 0 50.4-22.2 50.4-50.4zm10.2-199.8c-30 0-54 24-54 54s24 54 54 54 54-24 54-54-24.6-54-54-54zM409 37.8c-35.4 0-63.6 28.8-63.6 63.6S374.2 165 409 165s63.6-28.8 63.6-63.6-28.2-63.6-63.6-63.6z"/>
  </symbol>
</svg>



  </div>
</section>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="/blog/assets/main.js"></script>


    </div>
</div>

            </div>
        </div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <ul class="social-media-list">
      
      <li>
        <a href="https://github.com/markolalovic" target="_blank">
          <span class="icon  icon--github">
            <svg viewBox="0 0 16 16">
              <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
            </svg>
          </span>

          <span class="username">markolalovic</span>
        </a>
      </li>
      

      
      <li>
        <a href="https://twitter.com/MarkoLalovic" target="_blank">
          <span class="icon  icon--twitter">
            <svg viewBox="0 0 16 16">
              <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
              c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
            </svg>
          </span>

          <span class="username">MarkoLalovic</span>
        </a>
        <span class="post-meta" style="float: right">
          The website content is licensed <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC-BY 4.0</a>.
        </span>
      </li>
      
    </ul>
  </div>

</footer>


    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: {
          equationNumbers: { autoNumber: "AMS" },
          tagSide: "right"
        },
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    <script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

</body>

</html>
