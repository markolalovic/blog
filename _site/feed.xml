<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://markolalovic.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://markolalovic.github.io/blog/" rel="alternate" type="text/html" /><updated>2021-08-24T21:30:45+02:00</updated><id>https://markolalovic.github.io/blog/feed.xml</id><title type="html">Marko Lalovic</title><subtitle>This website contains my blog generated using Jekyll. View the README file on [GitHub](https://github.com/markolalovic/blog).</subtitle><entry><title type="html">Lasso Dual</title><link href="https://markolalovic.github.io/blog/lasso-dual" rel="alternate" type="text/html" title="Lasso Dual" /><published>2021-08-01T11:34:39+02:00</published><updated>2021-08-01T11:34:39+02:00</updated><id>https://markolalovic.github.io/blog/lasso-dual</id><content type="html" xml:base="https://markolalovic.github.io/blog/lasso-dual">&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/lasso-dual/transformation-3d.svg&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 1:&lt;/strong&gt; Illustration when $n=p=3$. (Drawn using &lt;a target=&quot;_blank&quot; href=&quot;https://docs.enthought.com/mayavi/mayavi/&quot;&gt;Mayavi library&lt;/a&gt;.)
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Let $A: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$ and $y \in \mathbb{R}^{p}$. Consider the Tikhonov regularization&lt;/p&gt;

\[\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\begin{equation}
\min_{x \in \mathbb{R}^{n}}
\frac{1}{2} \norm{Ax - y}_{2}^{2} + \alpha \norm{x}_{1}.
\label{eq:min-problem}
\end{equation}\]

&lt;p&gt;This type of penalized regression is called Lasso; see Tibshirani’s original paper [&lt;a href=&quot;http://statweb.stanford.edu/~tibs/lasso/lasso.pdf&quot; target=&quot;_blank&quot;&gt;1&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;In this post, we first derive the dual problem, then show that the solution $x^{*}$ can be determined with the help of a projection operator. Some nice and non-obvious properties of the solution $x^{*}$ follow from the geometry of the dual formulation. We finish with the observation that the Lasso solution is non-expansive as a function of $y$. This is not obvious and would probably be hard to show without the dual formulation.&lt;/p&gt;

&lt;!-- Section 1 --&gt;
&lt;h2 id=&quot;formulation-of-the-dual-problem&quot;&gt;Formulation of the dual problem&lt;/h2&gt;

&lt;p&gt;To derive the dual problem, we can introduce a dummy variable $z \in \mathbb{R}^{n}$&lt;/p&gt;

\[\begin{equation}\nonumber
z = Ax
\end{equation}\]

&lt;p&gt;and reformulate the minimization problem in \eqref{eq:min-problem} as a constrained problem&lt;/p&gt;

\[\begin{align*}
\label{eq:primal-problem}
\tag{P}
&amp;amp;\underset{z \in \mathbb{R}^{n}, x \in \mathbb{R}^{p}}{\text{minimize}} \quad
\frac{1}{2} \norm{y - z}_{2}^{2} + \alpha \norm{x}_{1} \\
&amp;amp;\text{subject to} \quad z = Ax
\end{align*}\]

&lt;p&gt;Then we can construct the Lagrangian by introducing the dual variable $p \in \mathbb{R}^{n}$ (containing $n$ Lagrange multipliers)&lt;/p&gt;

\[\begin{equation}\nonumber
L(x, z, p) = \frac{1}{2} \norm{y - z}_{2}^{2}
+ \alpha \norm{x}_{1} + p^{T} (z - Ax)
\end{equation}\]

&lt;p&gt;The dual objective function is&lt;/p&gt;

\[\begin{equation}\nonumber
g(p) = \min_{z \in \mathbb{R}^{n}, x \in \mathbb{R}^{p}}
\left\lbrace
\frac{1}{2} \norm{y - z}_{2}^{2}
+ \alpha \norm{x}_{1} + p^{T} (z - Ax)
\right\rbrace
\end{equation}\]

&lt;p&gt;We can split the terms depending on $z$ and $x$ and minimize each part separately&lt;/p&gt;

\[\begin{align}\nonumber
g(p)
&amp;amp;= \min_{z \in \mathbb{R}^{n}, x \in \mathbb{R}^{p}}
\left\lbrace
\frac{1}{2} \norm{y}_{2}^{2}
- y^{T}z
+ \frac{1}{2} \norm{z}_{2}^{2}
+ p^{T}z
+ \alpha \norm{x}_{1}
- p^{T}Ax
\right\rbrace \\[.3em]
&amp;amp;= \min_{z \in \mathbb{R}^{n}}\nonumber
\left\lbrace
\frac{1}{2} \norm{y}_{2}^{2}
- (y - p)^{T}z
+ \frac{1}{2} \norm{z}_{2}^{2}
\right\rbrace
+ \max_{x \in \mathbb{R}^{p}}
\left\lbrace
\alpha \norm{x}_{1}
- (A^{T}p)^{T}x
\right\rbrace
\end{align}\]

&lt;p&gt;We can use the stationarity condition, which says that at the optimal point, the subgradient of $L(x, z, p)$ with respect to $x$ and $z$ must contain 0.&lt;/p&gt;

&lt;p&gt;For the first part, since $L(x, z, p)$ is differentiable in $z$, the subgradient with respect to $z$  equals the gradient. By taking
$\frac{\partial}{\partial z} L(x, z, p)$ and setting it to $0$, we get the stationarity condition&lt;/p&gt;

\[\begin{equation}\nonumber
z = y - p^{*}
\end{equation}\]

&lt;p&gt;Plugging this into the first part, we get&lt;/p&gt;

\[\begin{align}\nonumber
&amp;amp;\frac{1}{2}\norm{y}_{2}^{2} - (y - p^{*})^{T}(y - p^{*})
+ \frac{1}{2}\norm{y - p^{*}}_{2}^{2} \\[.3em]
&amp;amp;=
\frac{1}{2}\norm{y}_{2}^{2}\nonumber
- \frac{1}{2}\norm{y - p^{*}}_{2}^{2}
\end{align}\]

&lt;p&gt;For the second part, because $\alpha \norm{x}_1$ is a non-differentiable function of $x$, we need to compute the subdifferential $\partial (\alpha \norm{x}_1)$.&lt;/p&gt;

&lt;p&gt;By using the rules for the subgradient of the maximum we can derive, see [&lt;a href=&quot;https://see.stanford.edu/materials/lsocoee364b/01-subgradients_notes.pdf&quot; target=&quot;_blank&quot;&gt;2&lt;/a&gt;], that the $\partial \norm{x}_{1}$ can be expressed as&lt;/p&gt;

\[\begin{equation}\nonumber
\partial (\norm{x}_{1})
= \lbrace g : \norm{g}_{\infty} \leq 1, g^{T} x = \norm{x}_{1}
\rbrace
\end{equation}\]

&lt;p&gt;and using the rule for scalar multiplication&lt;/p&gt;

\[\begin{equation}\nonumber
\partial (\alpha\norm{x}_{1})
= \lbrace g : \norm{g}_{\infty} \leq \alpha, g^{T} x = \alpha\norm{x}_{1}
\rbrace
\end{equation}\]

&lt;p&gt;Thus, we get the stationarity condition&lt;/p&gt;

\[\begin{equation}\nonumber
g = A^{T}p \in \partial \alpha \norm{x}_{1}
\end{equation}\]

&lt;p&gt;when&lt;/p&gt;

\[\begin{align} \nonumber
\norm{A^{T}p}_{\infty} \leq \alpha \\[.3em]
\alpha \norm{x}_{1} - (A^{T}p)^{T}x = 0\nonumber
\end{align}\]

&lt;p&gt;Therefore, the dual problem is&lt;/p&gt;

\[\begin{align*}
\label{eq:dual-problem}
\tag{D}
&amp;amp;\max_{p \in \mathbb{R}^{n}}
\frac{1}{2}\norm{y}_{2}^{2}
- \frac{1}{2}\norm{y - p}_{2}^{2} \\[.3em]
&amp;amp;\text{subject to} \norm{A^{T}p}_{\infty} \leq \alpha
\end{align*}\]

&lt;!-- Section 2 --&gt;
&lt;h2 id=&quot;solution-of-the-dual-problem&quot;&gt;Solution of the dual problem&lt;/h2&gt;
&lt;p&gt;The solution $p^{*}$ of the dual problem can be determined with the help of a projection operator.&lt;/p&gt;

&lt;p&gt;Looking at the dual problem formulation \eqref{eq:dual-problem}, we see that we can omit $\frac{1}{2}\norm{y}_{2}^{2}$, since this is constant. Then if we multiply it by 2 and flip the sign, we get an equivalent dual problem&lt;/p&gt;

\[\begin{align*}
\label{eq:dual-problem-prime}
\tag{D'}
&amp;amp;\min_{p \in \mathbb{R}^{n}}
\norm{y - p}_{2}^{2} \\[.3em]
&amp;amp;\text{subject to} \norm{A^{T}p}_{\infty} \leq \alpha
\end{align*}\]

&lt;p&gt;A &lt;em&gt;projection operator&lt;/em&gt; $P_{C}$ can be defined as&lt;/p&gt;

\[\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align}\nonumber
P_{C} \text{: } &amp;amp; \mathbb{R}^{n} \rightarrow C \\[.3em]
&amp;amp; y \mapsto P_{C}(y) := \argmin_{p \in C} \norm{y - p}_{2} \nonumber
\end{align}\]

&lt;p&gt;where $C \subset \mathbb{R}^{n}$ is closed and convex set. Looking at \eqref{eq:dual-problem-prime}, we see that $p^{*}$ is a projection&lt;/p&gt;

\[\begin{equation}\nonumber
p^{*} = P_{C}(y)
\end{equation}\]

&lt;p&gt;where $C$ is equal to&lt;/p&gt;

\[\begin{equation}\nonumber
C = \lbrace p \in \mathbb{R}^{n} :
\norm{A^{T}p}_{\infty} \leq \alpha
\rbrace
\end{equation}\]

&lt;p&gt;We notice that $C$ is indeed closed and convex. For example if $n=p=2$, and let&lt;/p&gt;

\[\begin{equation}\nonumber
A^{T} = \begin{pmatrix}
a_{11} &amp;amp; a_{21}\\
a_{12} &amp;amp; a_{22}
\end{pmatrix}
\end{equation}\]

&lt;p&gt;then&lt;/p&gt;

\[\begin{equation}\nonumber
C = \lbrace -\alpha \leq a_{11} p_{1} + a_{12} p_{2} \leq \alpha \rbrace
\cap
\lbrace -\alpha \leq a_{21} p_{1} + a_{22} p_{2} \leq \alpha \rbrace
\end{equation}\]

&lt;p&gt;Or we can express $C$ as&lt;/p&gt;

\[\begin{equation}\nonumber
C = (A^{T})^{-1}(D)
\end{equation}\]

&lt;p&gt;where $D$ is equal to&lt;/p&gt;

\[\begin{equation}\nonumber
D = \lbrace d \in \mathbb{R}^{p} : \norm{d}_{\infty} \leq \alpha \rbrace
\end{equation}\]

&lt;p&gt;Again for $p=2$&lt;/p&gt;

\[\begin{equation}\nonumber
D = \lbrace -\alpha \leq d_{1} \leq \alpha \rbrace
\cap
\lbrace -\alpha \leq d_{2} \leq \alpha \rbrace
\end{equation}\]

&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/lasso-dual/transformation-2d.svg&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 2:&lt;/strong&gt; Illustration when $n=p=2$.
  &lt;/div&gt;
&lt;/div&gt;

&lt;!-- Section 3 --&gt;
&lt;h2 id=&quot;solution-of-the-primal-problem&quot;&gt;Solution of the primal problem&lt;/h2&gt;
&lt;p&gt;From optimality condition of the dual problem, we can derive the primal solution under the assumption that $A^{-1}$ exists.&lt;/p&gt;

&lt;p&gt;During the formulation of the dual problem, we introduced the dummy variable $z = Ax$ and derived the stationary condition $z = y - p^{*}$.  From this, we get that every solution $x^{*}$ of \eqref{eq:primal-problem} should satisfy&lt;/p&gt;

\[\begin{equation}\nonumber
A x^{*} = y - p^{*}
\end{equation}\]

&lt;p&gt;where $p^{*}$ is a solution of the dual problem \eqref{eq:dual-problem}. Therefore, if $A^{-1}$ exists, the primal solution is&lt;/p&gt;

\[\begin{equation}\nonumber
x^{*} = A^{-1} \left( y - P_{C}(y) \right)
\end{equation}\]

&lt;p&gt;We notice that, since $P_{C}$ is a projection onto convex set $C$, it follows that $x^{*}$ is also non-expansive as a function of $y$; see &lt;strong&gt;Figure 3(a)&lt;/strong&gt; in contrast to &lt;strong&gt;Figure 3(b)&lt;/strong&gt; showing projection onto non-convex set $N$. This is not obvious and would probably be hard to show without the dual formulation.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/assets/posts/lasso-dual/convex.svg&quot; alt=&quot;convex.svg&quot; /&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/assets/posts/lasso-dual/bean.svg&quot; alt=&quot;bean.svg&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span class=&quot;subfig&quot;&gt;(a) Convex set $C$.&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span class=&quot;subfig&quot;&gt;(b) Non-convex set $N$.&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;images&quot;&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 3:&lt;/strong&gt; Lasso solution $x^{*}$ is non-expansive as a function of $y$.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B., Vol. 58, No. 1, pages 267-288).
&lt;a target=&quot;_blank&quot; href=&quot;http://statweb.stanford.edu/~tibs/lasso/lasso.pdf&quot;&gt;http://statweb.stanford.edu/~tibs/lasso/lasso.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] S. Boyd and L. Vandenberghe, “Subgradients Notes”, (2008), Stanford University
&lt;a target=&quot;_blank&quot; href=&quot;https://see.stanford.edu/materials/lsocoee364b/01-subgradients_notes.pdf&quot;&gt;https://see.stanford.edu/materials/lsocoee364b/01-subgradients_notes.pdf&lt;/a&gt;&lt;/p&gt;</content><author><name>Marko Lalovic</name></author><summary type="html">Figure 1: Illustration when $n=p=3$. (Drawn using Mayavi library.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://markolalovic.github.io/blog/assets/posts/lasso-dual/transformation-3d.jpg" /><media:content medium="image" url="https://markolalovic.github.io/blog/assets/posts/lasso-dual/transformation-3d.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Topological Features Applied to the MNIST Data Set</title><link href="https://markolalovic.github.io/blog/tda-digits" rel="alternate" type="text/html" title="Topological Features Applied to the MNIST Data Set" /><published>2020-09-25T22:26:36+02:00</published><updated>2020-09-25T22:26:36+02:00</updated><id>https://markolalovic.github.io/blog/tda-digits</id><content type="html" xml:base="https://markolalovic.github.io/blog/tda-digits">&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/tda-digits/intro-figure.jpg&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 1:&lt;/strong&gt; Illustration of the main ideas. (Torus was drawn using &lt;a target=&quot;_blank&quot; href=&quot;https://docs.enthought.com/mayavi/mayavi/&quot;&gt;Mayavi library&lt;/a&gt;.)
  &lt;/div&gt;
&lt;/div&gt;

&lt;!-- 8 min read --&gt;
&lt;p&gt;This is part of the project I made for Summer School on Computational Topology and Topological Data Analysis held in Ljubljana. I made publicly available all &lt;a href=&quot;https://github.com/markolalovic/tda-digits&quot; target=&quot;_blank&quot;&gt;scripts&lt;/a&gt; that I wrote for this tutorial including a processed version of the dataset. I am also using freely available computational topology package called &lt;a href=&quot;https://github.com/mrzv/dionysus&quot; target=&quot;_blank&quot;&gt;Dionysus 2&lt;/a&gt; for the computation of persistent homology.&lt;/p&gt;

&lt;h2 id=&quot;preface&quot;&gt;Preface&lt;/h2&gt;

&lt;p&gt;Topology applied to real-world data sets using persistent homology has begun to look for applications in machine learning, including deep learning [&lt;a href=&quot;https://arxiv.org/abs/1304.0530&quot; target=&quot;_blank&quot;&gt;2&lt;/a&gt;]. Topology is mainly used in a pre-processing step to provide robust features for learning. Our data is often a finite set of noisy samples from some underlying space. The developed topological techniques, mostly deal with point clouds, i.e. finite sets of data points in space. Point clouds are typically captured by a variety of imaging devices, such as MRI or CT scanners. With the greater availability of such devices, this type of data is being generated at an increasing rate. The data sets are often also very noisy and contain a lot of missing information, especially biological data sets. Our ability to analyze this data, both in terms of the amount and the nature of the data, is clearly out of step with the data we generate [&lt;a href=&quot;https://arxiv.org/pdf/1905.12200.pdf&quot; target=&quot;_blank&quot;&gt;3&lt;/a&gt;]. Topology can be used to make a useful contribution to the analysis of such data sets and it is especially helpful in studying them qualitatively.&lt;/p&gt;

&lt;p&gt;Persistent homology is a fascinating mathematical tool that continues to be studied, developed, and applied. The purpose of this article is to give a friendly introduction on how to use the persistent homology that does not require substantial knowledge of topological methods. To illustrate the use of persistent homology in machine learning we apply it to the MNIST data set of handwritten digits. A very similar approach can be applied to any point cloud data and it can easily be generalized to higher dimensions. &lt;strong&gt;Figure 1&lt;/strong&gt; illustrates the main ideas of the proposed technique. It shows a sample of handwritten digits on the left and extracted graph structure on the right. Underneath is a picture of a torus in 3D and the extracted point cloud with 2 colored loops on the right.&lt;/p&gt;

&lt;p&gt;First, we explain the extraction of topological features on a simple example
shown in &lt;strong&gt;Figure 2&lt;/strong&gt;. Second, we present and evaluate the empirical classification results on a subset of the MNIST database. The aim is to demonstrate the classification potential of the technique and not to outperform the existing models for the classification of handwritten digits. For a more interesting example of using this technique on a clinical data set to classify hepatic lesions, see [&lt;a href=&quot;https://arxiv.org/abs/1304.0530&quot; target=&quot;_blank&quot;&gt;1&lt;/a&gt;].&lt;/p&gt;

&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/tda-digits/anim-compressed.gif&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 2:&lt;/strong&gt; An example of applying this technique on the image of a handwritten digit 8.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;terminology&quot;&gt;Terminology&lt;/h3&gt;

&lt;p&gt;A list of short definitions which we will expand later if necessary:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Topology&lt;/em&gt; is a branch of mathematics that deals with qualitative geometric information. This includes the classification of loops and higher-dimensional surfaces.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Topological data analysis&lt;/em&gt; and &lt;em&gt;computational topology&lt;/em&gt; deal with the study of topology using a computer.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Persistent homology&lt;/em&gt; is an algebraic method for discerning topological features of data.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Connected component&lt;/em&gt; (or connected cluster of points) is a 0-dimensional feature and &lt;em&gt;cycle&lt;/em&gt; (or &lt;em&gt;loop&lt;/em&gt;) is a 1-dimensional feature.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Simplicial complex&lt;/em&gt; is a set composed of points, line segments, triangles, and their n-dimensional counterparts.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Filtration&lt;/em&gt; is the sequence of simplicial complexes, with an inclusion map from each simplicial complex to the next.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Barcode&lt;/em&gt; is a visual representation of the persistence of the topological features. Longer bars represent significant features of the data. Shorter bars are due to irregularities or noise.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;

&lt;p&gt;The main problem we are trying to solve is how to extract the topological features that can be used as an input to standard machine learning algorithms. We will use a similar approach as described in [&lt;a href=&quot;https://arxiv.org/abs/1304.0530&quot; target=&quot;_blank&quot;&gt;1&lt;/a&gt;]. From each image, we first construct a graph, where pixels of the image correspond to vertices of the graph and we add edges between adjacent pixels. A pure topological classification cannot distinguish between individual numbers, as the numbers are topologically too similar. For example numbers 1, 2, 3 are topologically the same if we use this style for writing numbers. Persistent homology, however, gives us more information.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overview.&lt;/strong&gt; We define a filtration on the vertices of the graph corresponding to the image pixels, adding vertices and edges as we sweep across the image in the vertical or horizontal direction. This adds spatial information to the topological features. For example, though 6 and 9 both have a single loop, it will appear at different locations in the filtration. We then compute the persistent homology given the simplex stream from the filtration to get the so-called Betti barcodes. &lt;em&gt;Betti&lt;/em&gt; $k$ &lt;em&gt;barcode&lt;/em&gt; is a finite set of intervals. Each interval represents the first filtration level where the topological feature of dimension $k$ appears and the filtration level where it disappears. These times are called &lt;em&gt;birth&lt;/em&gt; and &lt;em&gt;death&lt;/em&gt; of the topological feature respectively. We extract 4 features from the $k$-dimensional barcode from the invariants discussed in [&lt;a href=&quot;https://arxiv.org/abs/1304.0530&quot; target=&quot;_blank&quot;&gt;1&lt;/a&gt;]. For each of 4 sweep directions: top, bottom, right, left and dimensions: 0 and 1, we compute 4 features. This gives us a total of 32 features per image. On the extracted features from a set of images, we then apply a support vector machine (SVM) to classify the images.&lt;/p&gt;

&lt;h3 id=&quot;pre-processing&quot;&gt;Pre-Processing&lt;/h3&gt;

&lt;p&gt;The pre-processing steps used, shown in &lt;strong&gt;Figure 3&lt;/strong&gt;, are the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Load the MNIST image of a handwritten digit.&lt;/li&gt;
  &lt;li&gt;Produce the binary image by thresholding.&lt;/li&gt;
  &lt;li&gt;Reduce the binary image to a skeleton of 1-pixel width to expose its topology using the popular &lt;a href=&quot;https://github.com/linbojin/Skeletonization-by-Zhang-Suen-Thinning-Algorithm&quot; target=&quot;_blank&quot;&gt;Zhang-Suen thinning algorithm&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Transform the pixels of the skeleton to points in the plane.&lt;/li&gt;
  &lt;li&gt;Construct an embedded graph G in the plane where we treat the points as vertices and add edges between adjacent points similar and then remove all cycles of length 3. Intuitively, we connect the points while trying not to create new topological features.&lt;/li&gt;
  &lt;li&gt;Construct a simplex stream for computing the persistent homology using the filtration on the vertices of the graph G. Filtration is the following. We are adding the vertices and edges of the embedded graph G as we sweep across the plane. In this example, we sweep across the plane to the top in a vertical direction.&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/assets/posts/tda-digits/1_original-image.png&quot; alt=&quot;1_original-image.png&quot; /&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/assets/posts/tda-digits/2_binary-image.png&quot; alt=&quot;2_binary-image.png&quot; /&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/assets/posts/tda-digits/3_skeleton.png&quot; alt=&quot;3_skeleton.png&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span class=&quot;subfig&quot;&gt;(1) Original image&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span class=&quot;subfig&quot;&gt;(2) Binary image&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span class=&quot;subfig&quot;&gt;(3) Skeleton&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/assets/posts/tda-digits/4_points.png&quot; alt=&quot;4_points.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/assets/posts/tda-digits/5_embedded-graph.png&quot; alt=&quot;5_embedded-graph.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/blog/assets/posts/tda-digits/6_simplices.png&quot; alt=&quot;6_simplices.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span class=&quot;subfig&quot;&gt;(4) Points&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span class=&quot;subfig&quot;&gt;(5) Embedded graph&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span class=&quot;subfig&quot;&gt;(6) Simplices&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;images&quot;&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 3:&lt;/strong&gt; Pre-processing steps.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;extraction-of-topological-features&quot;&gt;Extraction of Topological Features&lt;/h3&gt;

&lt;p&gt;Given the simplex stream from the final pre-processing step, we compute the persistent homology to get the Betti barcodes shown in &lt;strong&gt;Figure 4&lt;/strong&gt;. See also &lt;strong&gt;Figure 2&lt;/strong&gt; which is using the same example.&lt;/p&gt;

&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/tda-digits/betti-barcodes.png&quot; width=&quot;500&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 4:&lt;/strong&gt; Betti barcodes from the sweep to the top of the image of digit 8.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Betti 0 barcode consists of one interval $[4.0, \infty)$, which clearly shows the single connected component with the birth time of 4 when the first point is detected. Betti 1 barcode consists of two intervals $[12, \infty)$ and $[20, \infty)$, with birth times 12 and 20 correspondingly to the births of 2 cycles (the value of y when the loop closes) in the drawing of digit 8 when we sweep to the top.&lt;/p&gt;

&lt;p&gt;Denote the endpoints of Betti barcode intervals with:&lt;/p&gt;

\[x_{1}, y_{1}, ..., x_{n}, y_{n}\]

&lt;p&gt;where $x_{i}$ represents the beginning and $y_{i}$ the end of each interval. From the endpoints we compute 4 features from the invariants discussed in [&lt;a href=&quot;https://arxiv.org/abs/1304.0530&quot; target=&quot;_blank&quot;&gt;1&lt;/a&gt;], that take into account all of the bars, lengths, and endpoints:&lt;/p&gt;

\[\begin{align}
&amp;amp;\sum_{i} x_{i} (y_{i} - x_{i}) \\
&amp;amp;\sum_{i} (y_{\max} - y_{i})(y_{i} - x_{i}) \\
&amp;amp;\sum_{i} (y_{\max} - y_{i})^{2} (y_{i} - x_{i})^{4} \\
&amp;amp;\sum_{i} x_{i}^{2} (y_{i} - x_{i})^{4} \\
\end{align}\]

&lt;p&gt;For each of the 4 sweep directions: top, bottom, right, left and for each $k$-dimensional barcode, for $k = 0, 1$, we compute the defined 4 features. This gives us a total of $4 \cdot 2 \cdot 4 = 32$ features per image.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;Data set consisted of extracted topological features of 10000 images of handwritten digits from MNIST database. Data set was split 50:50 in train and test set so that each had 5000 examples. SVM with RBF kernel was used for classification of the images based on the extracted topological features. The empirical classification results are as follows. Accuracy on the train set using 10-fold cross-validation was $0.88 (\pm 0.05)$. Accuracy on the test set was 0.89.&lt;/p&gt;

&lt;p&gt;We examine the common misclassifications.&lt;/p&gt;

&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/tda-digits/miss-2.png&quot; width=&quot;500&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 5:&lt;/strong&gt; Examples of number 2 being mistaken for number 0.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;There were 3 examples of the number 2 being mistaken for number 0, shown in &lt;strong&gt;Figure 5&lt;/strong&gt;. The reason is that the number 2 was written with a loop that appears in the region that is close to the loop in number 0.&lt;/p&gt;

&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/tda-digits/miss-5.png&quot; width=&quot;500&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 6:&lt;/strong&gt; Examples of number 5 being mistaken for number 2.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;For number 5 we got the lowest F1 score of 0.75. It was misclassified as number 2 in 32 examples in the test set. The first three examples are shown in &lt;strong&gt;Figure 6&lt;/strong&gt;. This was expected since these two numbers are topologically the same with no topological features (e.g. loops) appearing in different regions.&lt;/p&gt;

&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/tda-digits/miss-8.png&quot; width=&quot;500&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 7:&lt;/strong&gt; Examples of number 8 being mistaken for number 4.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Number 8 was misclassified as number 4 in 21 examples from the test set. The first three examples are shown in &lt;strong&gt;Figure 7&lt;/strong&gt;. We see the stylistic problems that caused the misclassifications. The top loop of number 8 was not closed which made it topologically more similar to number 4 written with a loop.&lt;/p&gt;

&lt;h2 id=&quot;source-code&quot;&gt;Source Code&lt;/h2&gt;

&lt;p&gt;The repository containing all Python scripts that I wrote for this tutorial including the processed version of the data set is available &lt;a href=&quot;https://github.com/markolalovic/tda-digits&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It’s dependencies are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Python (2 or 3);&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mrzv/dionysus&quot; target=&quot;_blank&quot;&gt;Dionysus 2&lt;/a&gt; for computing persistent homology;&lt;/li&gt;
  &lt;li&gt;Boost version 1.55 or higher for Dionysus 2;&lt;/li&gt;
  &lt;li&gt;Matplotlib for plotting the data;&lt;/li&gt;
  &lt;li&gt;NetworkX for plotting the graphs;&lt;/li&gt;
  &lt;li&gt;NumPy for loading data and computing;&lt;/li&gt;
  &lt;li&gt;Scikit-learn for machine learning algorithms;&lt;/li&gt;
  &lt;li&gt;Scikit-image for image pre-processing;&lt;/li&gt;
  &lt;li&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://docs.enthought.com/mayavi/mayavi/&quot;&gt;Mayavi library&lt;/a&gt; if you want to draw the torus from Figure 1 in 3D.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To generate the figures in the example of topological features extraction, run:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;scripts
./tda_digits
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For details on how to use the functions and classes see the Jupyter notebooks: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Example.ipynb&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Classification.ipynb&lt;/code&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scripts&lt;/code&gt; directory.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;This is part of the project I made for Summer School on Computational Topology and Topological Data Analysis held in Ljubljana. The project was presented by Primoz Skraba.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] A. Adcock, E. Carlsson, and G. Carlsson, “The Ring of Algebraic Functions on Persistence Bar Codes”, (2013), Homology, Homotopy and Applications
&lt;a target=&quot;_blank&quot; href=&quot;https://arxiv.org/abs/1304.0530&quot;&gt;https://arxiv.org/abs/1304.0530&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] R. Bruel-Gabrielsson, B. J. Nelson, A. Dwaraknath, P. Skraba, L. J. Guibas, and G. Carlsson, “A Topology Layer for Machine Learning”, (2020), Proceedings of Machine Learning Research
&lt;a target=&quot;_blank&quot; href=&quot;https://arxiv.org/pdf/1905.12200.pdf&quot;&gt;https://arxiv.org/pdf/1905.12200.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] G. Carlsson, “Topology and data”, (2009), Bulletin of the American Mathematical Society
&lt;a target=&quot;_blank&quot; href=&quot;https://www.ams.org/journals/bull/2009-46-02/S0273-0979-09-01249-X/S0273-0979-09-01249-X.pdf&quot;&gt;https://www.ams.org/journals/bull/2009-46-02/S0273-0979-09-01249-X/S0273-0979-09-01249-X.pdf&lt;/a&gt;&lt;/p&gt;</content><author><name>Marko Lalovic</name></author><summary type="html">Figure 1: Illustration of the main ideas. (Torus was drawn using Mayavi library.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://markolalovic.github.io/blog/assets/posts/tda-digits/intro-figure.jpg" /><media:content medium="image" url="https://markolalovic.github.io/blog/assets/posts/tda-digits/intro-figure.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Some Math Behind Water Towers</title><link href="https://markolalovic.github.io/blog/water-towers" rel="alternate" type="text/html" title="Some Math Behind Water Towers" /><published>2020-06-15T22:26:36+02:00</published><updated>2020-06-15T22:26:36+02:00</updated><id>https://markolalovic.github.io/blog/water-towers</id><content type="html" xml:base="https://markolalovic.github.io/blog/water-towers">&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/water-towers/water-towers.jpg&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 1:&lt;/strong&gt; Water towers in New York. (Source:  &lt;a target=&quot;_blank&quot; href=&quot;https://www.flickr.com/people/38782010@N00&quot;&gt;takomabibelot&lt;/a&gt;,
&lt;a target=&quot;_blank&quot; href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;&gt;CC-BY 2.0&lt;/a&gt;, edited by author.)
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The water towers are interesting to me because their structure reveals some math behind how they are built. In &lt;strong&gt;Figure 1&lt;/strong&gt;, we see that the water tanks are reinforced with metal rings to hold the cylindrical structure together. We also notice that the metal rings are closer to each other at the bottom of the tank where the pressure is greatest. But what is the optimal arrangement?&lt;/p&gt;

&lt;p&gt;Since I did not find a derivation anywhere on the internet, I will describe my solution here with a simple example.&lt;/p&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;p&gt;Say we have a wooden water tank that we want to reinforce with 5 equal metal rings so that the wood does not give in to the water pressure. How would you arrange the metal rings?&lt;/p&gt;

&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/water-towers/drawing.svg&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 2:&lt;/strong&gt; Two examples of arrangements of metal rings.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Two examples of arrangements are shown in &lt;strong&gt;Figure 2&lt;/strong&gt;. Let $d=3$ be the depth of the tank. Denote by $h_{k}$ for $k=1, …, 5​​$ the height of each metal ring from the bottom of the tank.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2A&lt;/strong&gt; shows a naïve straightforward approach, where the metal rings are spaced evenly at heights from the bottom of the tank:&lt;/p&gt;

\[h_{1} = 0.5, \quad h_{2} = 1, \quad h_{3} = 1.5, \quad h_{4} = 2,
\quad h_{5} = 2.5.\]

&lt;p&gt;The pressure that the water exerts increases with depth and the metal rings at the bottom of the tank have to hold a lot more pressure than the metal rings at the top of the tank. Therefore the structural strength of the lower part of the tank is lower than that of the upper part of the tank. Hence, this arrangement of metal rings is a poor choice.&lt;/p&gt;

&lt;h3 id=&quot;optimal-arrangement&quot;&gt;Optimal Arrangement&lt;/h3&gt;

&lt;p&gt;We derive the optimal arrangement of the metal rings shown in &lt;strong&gt;Figure 2B&lt;/strong&gt;. The influence of depth on water density is negligible. We also assume that the water temperature in the tank is the same and constant everywhere. The water pressure $p$ at depth $x$ is equal to:&lt;/p&gt;

\[p(x) = p_{0} + \rho \cdot g \cdot x\]

&lt;p&gt;where $p_{0}$ is the pressure at the surface, $\rho$ is the density of water and $g$ is the gravitational acceleration. The important part to notice for this derivation, is that the pressure exerted by a static liquid increases linearly with increasing depth:&lt;/p&gt;

\[p(x) \propto C \cdot x\]

&lt;p&gt;for some constant $C &amp;gt; 0$. We choose the constant $C = 2/9$ and define the density:&lt;/p&gt;

\[f_{X}(x) = \frac{2}{9} \cdot x\]

&lt;p&gt;of some continuously distributed random variable $X$ that represents the pressure in the tank, defined on the interval $[0, 3]$. The corresponding distribution function is then:&lt;/p&gt;

\[F_{X}(x) = \frac{1}{9} \cdot x^{2}.\]

&lt;p&gt;Define the limits of the interval $[0, 3]$:&lt;/p&gt;

\[x_{1} = 0, \quad x_{6} = 3\]

&lt;p&gt;and divide the interval $[0, 3]$ into 5 subintervals with boundaries:&lt;/p&gt;

\[x_{2} = x_{0.2}, \quad x_{3} = x_{0.4}, \quad x_{4} = x_{0.6}, \quad
x_{5} = x_{0.8}\]

&lt;p&gt;where $x_{q}$ are quantiles, that is:&lt;/p&gt;

\[F_{X}(x_{q}) = q.\]

&lt;p&gt;The boundaries are marked in dotted red in &lt;strong&gt;Figure 2B&lt;/strong&gt;. At each subinterval, the integral of the density is equal to 0.2. This method, of dividing a continuous distribution into subintervals with equal density, is called &lt;em&gt;Equal Frequency Discretization&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Finally, we arrange the metal rings so that each ring is exactly in the centroid $h_{k}$​ of density $f_{X}$ at the subinterval $k$:&lt;/p&gt;

\[h_{k} = \frac{\int_{x_{k}}^{x_{k+1}} x f(x) dx}{\int_{x_{k}}^{x_{k+1}} f(x) dx}.\]

&lt;p&gt;This way, each metal ring needs to hold the same amount of pressure. Therefore the structural strength is the same everywhere and this arrangement of metal rings is optimal.&lt;/p&gt;

&lt;p&gt;In the optimal arrangement, the calculated heights of the metal rings from the bottom of the tank are approximately:&lt;/p&gt;

\[h_{1} = 0.16, \quad h_{2} = 0.49, \quad h_{3} = 0.88, \quad
h_{4} = 1.36, \quad h_{5} = 2.11.\]

&lt;p&gt;The solution is shown in &lt;strong&gt;Figure 2b&lt;/strong&gt;.&lt;/p&gt;</content><author><name>Marko Lalovic</name></author><summary type="html">Figure 1: Water towers in New York. (Source: takomabibelot, CC-BY 2.0, edited by author.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://markolalovic.github.io/blog/assets/posts/water-towers/water-towers-small.jpg" /><media:content medium="image" url="https://markolalovic.github.io/blog/assets/posts/water-towers/water-towers-small.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Ridge Regression</title><link href="https://markolalovic.github.io/blog/ridge-regression" rel="alternate" type="text/html" title="Ridge Regression" /><published>2019-08-01T11:31:22+02:00</published><updated>2019-08-01T11:31:22+02:00</updated><id>https://markolalovic.github.io/blog/ridge-regression</id><content type="html" xml:base="https://markolalovic.github.io/blog/ridge-regression">&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/ridge-regression/problem.svg&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 1:&lt;/strong&gt; As the correlation between regressors increases the OLS method becomes unstable while the ridge regression method produces stable estimates regardless of the given data in $X$.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The ordinary least squares (OLS) method is not suitable to estimate the unknown parameters $\beta$ in the case of highly correlated regressors. As the correlation between regressors in $X$ increases the OLS method becomes unstable. In the limit $|corr(x_{i}, x_{j})| \rightarrow 1$, the OLS objective function is no longer strictly convex and there are infinitely many solutions of OLS problem. The matrix $X$ becomes singular and both the variance of the estimator and the distance of the estimator to the actual $\beta$ go to infinity; see my previous post on &lt;a href=&quot;https://markolalovic.com/blog/2019/07/14/ols-regression.html&quot;&gt;Multicollinearity Effect on OLS Regression&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here, after introducing penalized regression, we derive the ridge regression estimator. Ridge regression is an effective approach to solve such problems. We show that, regardless of data $X$, unique solution to ridge regression always exists. By adding the ridge (vector of $\alpha$’s) on the diagonal of $X$, the ridge regression method produces stable estimates of the coefficients in $\beta$. See &lt;strong&gt;Figure 1&lt;/strong&gt; for illustration.&lt;/p&gt;

&lt;p&gt;We illustrate the method on a simple example in R, explain the role of the penalty function and finish with the analysis of regularization parameter $\alpha$.&lt;/p&gt;

&lt;h2 id=&quot;penalized-regression&quot;&gt;Penalized regression&lt;/h2&gt;

&lt;p&gt;In &lt;em&gt;penalized regression&lt;/em&gt;, for $n &amp;gt; p$ and given $X: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$ and $y \in \mathbb{R}^{n}$, we minimize the functional&lt;/p&gt;

\[\begin{equation}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
J_{\alpha}(\beta) = \norm{ y - X\beta }_{2}^{2} + \alpha P(\beta)
\end{equation}\]

&lt;p&gt;over $\beta \in \mathbb{R}^{p}$, where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$J_{\alpha}: \mathbb{R}^{p} \rightarrow \mathbb{R}$ is the objective function;&lt;/li&gt;
  &lt;li&gt;$P: \mathbb{R}^{p} \rightarrow \mathbb{R}$ is a &lt;em&gt;penalty function&lt;/em&gt; that penalizes unrealistic values in $\beta$;&lt;/li&gt;
  &lt;li&gt;Parameter $\alpha &amp;gt; 0$ controls the trade-off between the penalty and the fit of the loss function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main idea that determines the choice of the penalty function is that we would prefer a simple model to a more complex one.&lt;/p&gt;

&lt;p&gt;There are many different possibilities for penalty function $P$. For example, if we want a smoother fit, then $P$ is a measure of the curvature.&lt;/p&gt;

&lt;p&gt;In the case of correlated regressors, the estimated coefficients can become too large and $P$ is a measure of the distance of the coefficients from the origin. In this case, the main penalty function to consider is&lt;/p&gt;

\[\begin{equation}
P(\beta) = \norm{\beta}_{2}^{2}
\end{equation}\]

&lt;p&gt;This type of penalized regression is called &lt;em&gt;Ridge regression&lt;/em&gt;; see original paper [&lt;a href=&quot;https://www.nrs.fs.fed.us/pubs/rn/rn_ne236.pdf&quot; target=&quot;_blank&quot;&gt;1&lt;/a&gt;].&lt;/p&gt;

&lt;h2 id=&quot;derivation-of-ridge-regression-estimator&quot;&gt;Derivation of ridge regression estimator&lt;/h2&gt;

&lt;p&gt;Here, in order to simplify the derivation, we will assume that $X: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$ is linear and continuous with full column rank $p$.&lt;/p&gt;

&lt;p&gt;The objective function we want to minimize, written in a matrix form, is&lt;/p&gt;

\[\begin{align}
\norm{ y - X \beta }_{2}^{2} + \alpha \norm{\beta}_{2}^2
&amp;amp;= (y - X \beta)^{T} (y - X \beta) + \alpha \beta^{T} \beta \nonumber \\[1em]
&amp;amp;= y^{T}y - 2 y^{T} X \beta + \beta^{T} X^{T}X \beta + \alpha \beta^{T} \beta
\end{align}\]

&lt;p&gt;By taking a partial derivative with respect to $\beta$ and setting it to zero
\begin{equation}
-2 X^{T} y + 2 X^{T} X \hat{\beta} + 2 \alpha \hat{\beta} = 0
\end{equation}&lt;/p&gt;

&lt;p&gt;we get a regularized normal equation
\begin{equation}
(X^{T}X + \alpha I) \hat{\beta} = X^{T}y
\end{equation}&lt;/p&gt;

&lt;p&gt;we can express $\hat{\beta}$ as
\begin{equation}
\hat{\beta} = (X^{T}X + \alpha I)^{-1} X^{T} y
\end{equation}&lt;/p&gt;

&lt;p&gt;and since $\text{rank}(X) = p$
\begin{equation}
X z \neq 0 \quad \text{for each} \quad z \neq 0
\end{equation}&lt;/p&gt;

&lt;p&gt;for the Hessian
\begin{equation}
2X^{T}X + 2 \alpha
\end{equation}&lt;/p&gt;

&lt;p&gt;it holds that&lt;/p&gt;

\[\begin{align}
2 z^{T} X^{T} X z + 2 \alpha z^{T} z
&amp;amp;= 2 (Xz)^{T} (Xz) + 2 \alpha z^{T} z \nonumber \\[1em]
&amp;amp;= 2 \norm{Xz}_{2}^{2} + 2 \alpha \norm{z}_{2}^{2} &amp;gt; 0
\quad \text{for each} \quad z \neq 0
\label{eq: positive-definite}
\end{align}\]

&lt;p&gt;therefore, the expressed $\hat{\beta}$ is really an estimator&lt;/p&gt;

&lt;p&gt;\begin{equation}
\hat{\beta}_{\text{RR}} = (X^{T}X + \alpha I)^{-1} X^{T} y.
\label{eq: rr}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;We can illustrate the ridge regression method to estimate the unknown parameters $\beta$ in the case of correlated regressors on a simple example in R.&lt;/p&gt;

&lt;p&gt;Suppose we have a model $y \sim \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2}$, more specifically, let&lt;/p&gt;

&lt;p&gt;\begin{equation}
\beta_{0} = 3, \quad \beta_{1} = \beta_{2} = 1.
\end{equation}&lt;/p&gt;

&lt;p&gt;and let the sample contain 100 elements&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We then introduce some highly correlated regressors&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;set.seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sd&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with correlation coefficient almost 1&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.999962365268769&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;into the model&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sd&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and calculate the estimate $\hat{\beta}_{\text{RR}}$ for $\alpha = 0.3$&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cbind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta.ridge&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta.ridge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2.98537494896842&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.815120466450887&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.04146900239714&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;properties-of-ridge-regression-estimator&quot;&gt;Properties of ridge regression estimator&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The unique solution \ref{eq: rr} of ridge regression $\hat{\beta}_{\text{RR}}$ always exists, since $X^{T}X + \alpha I$ is always rank $p$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We can derive a relationship between ridge and OLS estimators for the case when the matrix $X$ is orthogonal. Using $X^{T}X = I$ twice and since $\hat{\beta}_{\text{OLS}} = (X^{T}X)^{-1} X^{T} y$, we get the relation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[\begin{align}
\hat{\beta}_{\text{RR}} &amp;amp;= (X^{T}X + \alpha I)^{-1} X^{T} y \nonumber \\[1em]
&amp;amp;= (I + \alpha I)^{-1} X^{T} y \nonumber \\[1em]
&amp;amp;= (1 + \alpha)^{-1} I X^{T} y \nonumber \\[1em]
&amp;amp;= (1 + \alpha)^{-1} (X^{T}X)^{-1} X^{T} y \nonumber \\[1em]
&amp;amp;= (1 + \alpha)^{-1} \hat{\beta}_{\text{OLS}}
\end{align}\]

&lt;ul&gt;
  &lt;li&gt;Ridge regression estimator $\hat{\beta}_{\text{RR}}$ is biased since, for each value of $\alpha &amp;gt; 0$, its expected value is not equal to $\beta$&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align}
\mathbb{E}[\hat{\beta}_{ridge}]
&amp;amp;= \mathbb{E}[(X^{T}X + \alpha I)^{-1} X^{T} y] \nonumber \\[1em]
&amp;amp;= \mathbb{E}[(X^{T}X + \alpha I)^{-1} (X^{T}X) (X^{T}X)^{-1} X^{T} y] \nonumber \\[1em]
&amp;amp;= \mathbb{E}[(X^{T}X + \alpha I)^{-1} (X^{T}X) \hat{\beta}_{\text{OLS}}] \nonumber \\[1em]
&amp;amp;= (X^{T}X + \alpha I)^{-1} (X^{T}X) \mathbb{E}[\hat{\beta}_{\text{OLS}}] \nonumber \\[1em]
&amp;amp;= (X^{T}X + \alpha I)^{-1} (X^{T}X) \beta.
\end{align}\]

&lt;ul&gt;
  &lt;li&gt;Also, as $\alpha \rightarrow 0$, ridge estimator tends to OLS estimator, which can easiliy be seen from&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align}
\lim_{\alpha \to 0} \hat{\beta}_{\text{RR}}
&amp;amp;= \lim_{\alpha \to 0} (X^{T}X + \alpha I)^{-1} (X^{T}X) \hat{\beta}_{\text{OLS}} \nonumber \\[1em]
&amp;amp;= (X^{T}X)^{-1} (X^{T}X) \hat{\beta}_{\text{OLS}} \nonumber \\[1em]
&amp;amp;= \hat{\beta}_{\text{OLS}}
\end{align}\]

&lt;h2 id=&quot;the-role-of-the-penalty-function&quot;&gt;The role of the penalty function&lt;/h2&gt;
&lt;p&gt;The role of the penalty function can be shown conveniently with the help of singular value decomposition. Let&lt;/p&gt;

&lt;p&gt;\begin{equation}
X = U \Sigma V^{T}
\end{equation}&lt;/p&gt;

&lt;p&gt;be the singular value decomposition of $X$ where $\Sigma$ contains all the singular values&lt;/p&gt;

&lt;p&gt;\begin{equation}
\sigma_{1} \geq \sigma_{2} \geq \dots \geq \sigma_{p} &amp;gt; 0
\end{equation}&lt;/p&gt;

&lt;p&gt;The regularized normal equation
\(\begin{align}
( X^{T} X + \alpha I ) \hat{\beta} = X^{T} y
\end{align}\)&lt;/p&gt;

&lt;p&gt;can be rewritten as&lt;/p&gt;

\[\begin{align}
(V \Sigma^{T} U^{T}U \Sigma V^{T} + \alpha I) \hat{\beta}
= V \Sigma^{T} U^{T} y
\end{align}\]

&lt;p&gt;Then, since $U^{T}U = I$ and $V^{T}V = I$, we get&lt;/p&gt;

\[\begin{align}
(V \Sigma^{T} \Sigma V^{T} + \alpha V^{T}V) \hat{\beta}
&amp;amp;= V \Sigma^{T} U^{T} y \nonumber \\[1em]
V (\Sigma^{T} \Sigma + \alpha I) V^{T} \hat{\beta}
&amp;amp;= V \Sigma^{T} U^{T} y
\end{align}\]

&lt;p&gt;Furthermore, multiplying it by $V^{T}$ from the left and setting $z := V^{T} \hat{\beta}$, we get
\begin{equation}
(\Sigma^{T} \Sigma + \alpha I) z = \Sigma^{T} U^{T} y
\end{equation}&lt;/p&gt;

&lt;p&gt;Therefore&lt;/p&gt;

&lt;p&gt;\begin{equation}
z_{i} = \frac{\sigma_{i} (u_{i}^{T} y)}{\sigma_{i}^{2} + \alpha} \quad \text{for} \quad i = 1, \dots, p
\end{equation}&lt;/p&gt;

&lt;p&gt;And, for minimum norm solution, let
\begin{equation}
z_{i} = 0 \quad \text{for} \quad i = p + 1, \dots, n
\end{equation}&lt;/p&gt;

&lt;p&gt;Finally, from $\hat{\beta} = V z$ and since $V$ is orthogonal&lt;/p&gt;

\[\begin{equation}
\norm{\hat{\beta}} = \norm{VV^{T} \hat{\beta}}
= \norm{V^{T}\hat{\beta}}
= \norm{z}
\end{equation}\]

&lt;p&gt;we get&lt;/p&gt;

\[\begin{equation}
\hat{\beta}_{i}
= \frac{\sigma_{i} (u_{i}^{T} y)}{\sigma_{i}^{2} + \alpha} v_{i}
\label{eq: beta_i}
\end{equation}\]

&lt;p&gt;And from&lt;/p&gt;

\[\begin{equation}
\hat{\beta}_{i}
\approx
\begin{cases}
  0, &amp;amp; \text{if } \sigma_{i} &amp;lt;&amp;lt; \alpha \\
  \frac{u_{i}^{T} y}{\sigma_{i}}v_{i}, &amp;amp; \text{if } \sigma_{i} &amp;gt;&amp;gt; \alpha
\end{cases}
\end{equation}\]

&lt;p&gt;we can see that the penalty function $\alpha \norm{\beta}_{2}^{2}$ acts as a filter since the contributions&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;from $\sigma_{i}$ that is small relative to the regularization parameter $\alpha$ are almost eliminated;&lt;/li&gt;
  &lt;li&gt;from $\sigma_{i}$ that is large relative to the regularization parameter $\alpha$ are left almost unchanged.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By defining a filter&lt;/p&gt;

\[\begin{equation}
F_{\alpha}(\xi) = \frac{1}{\xi + \alpha}
\end{equation}\]

&lt;p&gt;the solution of ridge regression can be further expressed as&lt;/p&gt;

\[\begin{equation}
\hat{\beta}_{\text{RR}} = F_{\alpha}(X^{T}X) X^{T}y
\end{equation}\]

&lt;h2 id=&quot;regularization-parameter-alpha&quot;&gt;Regularization parameter $\alpha$&lt;/h2&gt;

&lt;p&gt;First, we notice that the solution of ridge regression is monotonically decreasing in $\alpha$.&lt;/p&gt;

&lt;p&gt;To show this, let&lt;/p&gt;

\[\begin{equation}
\psi(\alpha) = \norm{\hat{\beta}_{\text{RR}}}_{2}^{2}
\end{equation}\]

&lt;p&gt;Then, from derived equation for $\hat{\beta}_{i}$ in \ref{eq: beta_i}, we have that&lt;/p&gt;

\[\begin{equation}
\psi(\alpha) =
\sum_{i = 1}^{p} \frac{\sigma_{i}^{2} (u_{i}^{T} y)^{2}}{ (\sigma_{i}^{2} + \alpha)^{2} } v_{i}^{2}
\end{equation}\]

&lt;p&gt;and taking derivative on $\alpha$&lt;/p&gt;

\[\begin{equation}
\psi'(\alpha) =
-2 \sum_{i = 1}^{p} \frac{\sigma_{i}^{2} (u_{i}^{T} y)^{2}}{ (\sigma_{i}^{2} + \alpha)^{3} } v_{i}^{2} &amp;lt; 0
\end{equation}\]

&lt;p&gt;Second, as $\alpha \rightarrow \infty$ the solution of ridge regression goes to $\boldsymbol{0}$, since&lt;/p&gt;

\[\begin{equation}
\lim_{\alpha \rightarrow \infty} \psi(\alpha) =
\lim_{\alpha \rightarrow \infty} \sum_{i = 1}^{p} \frac{\sigma_{i}^{2} (u_{i}^{T} y)^{2}}{ (\sigma_{i}^{2} + \alpha)^{2} } v_{i}^{2} = 0
\end{equation}\]

&lt;p&gt;In the limit $\alpha \rightarrow 0$, as shown before, the solution of ridge regression goes to ordinary least squares solution. Furthermore, if $\sigma_{p} \rightarrow 0$ where $X$ is no longer full column rank, then $\psi(\alpha) \rightarrow \infty$.&lt;/p&gt;

&lt;p&gt;We can plot how the estimates $\beta_{0}, \beta_{1}, \beta_{2}$ change depending on the value of parameter $\alpha$ for the data of the &lt;a href=&quot;#example&quot;&gt;Example&lt;/a&gt;; shown in &lt;strong&gt;Figure 2&lt;/strong&gt; below&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta.ridge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latex2exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# for annotation&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;l&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lwd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;red&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TeX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'($\log(\alpha)$)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TeX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'($\hat{\beta}$)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cex.lab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cex.axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cex.main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cex.sub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;l&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lwd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;l&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lwd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;black&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7.73&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3.12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
           &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TeX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'($\hat{\beta}_{1}(\alpha)$)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
           &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TeX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'($\hat{\beta}_{2}(\alpha)$)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
           &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TeX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'($\hat{\beta}_{3}(\alpha)$)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;red&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;black&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lwd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/ridge-regression/ridge-solution-path.svg&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 2:&lt;/strong&gt; The solution of ridge regression as a function of the regularization parameter $\alpha$.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The selection of $\alpha$ is usually done by cross-validation. This means that we randomly partition the data into $K$ equally sized sets. For some value of $\alpha$ we then build a model (calculate estimates for the coefficients) on the data from $K - 1$ sets (learning set) and test it on the rest of the data (test set). Of course, we are interested in the mean square error (MSE). We then repeat the process for the remaining values of $\alpha$ and select the value of $\alpha$ where this average is the smallest. Typical values for $K$ are $5, 10$, and $n$ (sample size).&lt;/p&gt;

&lt;p&gt;Let’s find the optimal value of parameter $\alpha$ for the data of the &lt;a href=&quot;#example&quot;&gt;Example&lt;/a&gt; using 10-fold cross-validation&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;breaks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv.matrix&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test.i&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;br&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta.ridge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test.i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test.i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv.matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;br&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test.i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test.i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avgs&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv.matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best.alpha&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avgs&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best.alpha&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.246596963941606&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Hilt, Donald E.; Seegrist, Donald W. 1977. Ridge: a computer program for calculating ridge regression estimates. Research Note NE-236. Upper Darby, PA: U.S. Department of Agriculture, Forest Service, Northeastern Forest Experiment Station. 7p.
&lt;a target=&quot;_blank&quot; href=&quot;https://www.nrs.fs.fed.us/pubs/rn/rn_ne236.pdf&quot;&gt;https://www.nrs.fs.fed.us/pubs/rn/rn_ne236.pdf&lt;/a&gt;&lt;/p&gt;</content><author><name>Marko Lalovic</name></author><summary type="html">Figure 1: As the correlation between regressors increases the OLS method becomes unstable while the ridge regression method produces stable estimates regardless of the given data in $X$.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://markolalovic.github.io/blog/assets/posts/ridge-regression/problem-resized.jpg" /><media:content medium="image" url="https://markolalovic.github.io/blog/assets/posts/ridge-regression/problem-resized.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multicollinearity Effect on OLS Regression</title><link href="https://markolalovic.github.io/blog/ols-regression" rel="alternate" type="text/html" title="Multicollinearity Effect on OLS Regression" /><published>2019-07-14T22:41:01+02:00</published><updated>2019-07-14T22:41:01+02:00</updated><id>https://markolalovic.github.io/blog/ols-regression</id><content type="html" xml:base="https://markolalovic.github.io/blog/ols-regression">&lt;div class=&quot;images&quot;&gt;
  &lt;img src=&quot;/blog/assets/posts/ols-regression/problem.svg&quot; /&gt;
  &lt;div class=&quot;label&quot;&gt;
    &lt;strong&gt;Figure 1:&lt;/strong&gt; As the correlation between regressors increases, the OLS method becomes unstable. In the limit $|corr(x_{i}, x_{j})| \rightarrow 1$, the OLS objective function is no longer strictly convex and OLS solution is no longer unique.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In &lt;em&gt;ordinary least squares (OLS) regression&lt;/em&gt;, for given $X: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$ and $y \in \mathbb{R}^{n}$, we minimize over $\beta \in \mathbb{R}^{p}$, the sum of squared residuals&lt;/p&gt;

\[\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\begin{equation}
S(\beta) = \norm{ y - X\beta }_{2}^{2}
\end{equation}\]

&lt;p&gt;We first illustrate the problem with using ordinary least squares (OLS) method to estimate the unknown parameters $\beta$ in the case of highly correlated regressors on a simple example in R.&lt;/p&gt;

&lt;p&gt;Suppose we have a model $y \sim \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2}$, more specifically, let&lt;/p&gt;

&lt;p&gt;\begin{equation}
\beta_{0} = 3, \quad \beta_{1} = \beta_{2} = 1.
\end{equation}&lt;/p&gt;

&lt;p&gt;and let the sample contain 100 elements&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We then introduce some highly correlated regressors&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;set.seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sd&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with correlation coefficient almost 1&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.999962365268769&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s run the OLS method 1000 times to get a sense of the effect of highly correlated regressors&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cbind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intr&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nsim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sd&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta.ols&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta.ols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The estimator for $\beta$, obtained by the OLS method, is still unbiased&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2.996&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.006&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.993&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But the variance becomes to large&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.101&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;11.110&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;11.103&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The estimated coefficients can become too large, some can even have the wrong sign&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3.002&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-7.673&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;9.529&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem can be seen by drawing a contour plot of the objective function $S(\beta)$&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ssr&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlen&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylen&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgrid&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-10.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length.out&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ygrid&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-10.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length.out&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zvals&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncol&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zvals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ygrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ygrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zvals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;levels&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1e3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3e3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6e3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1e4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As shown in &lt;strong&gt;Figure 1&lt;/strong&gt;, as the correlation between regressors increases, matrix $X$ becomes nearly singular and OLS method becomes unstable.&lt;/p&gt;

&lt;p&gt;In the limit&lt;/p&gt;

&lt;p&gt;\begin{equation}
|corr(x_{i}, x_{j})| \rightarrow 1
\end{equation}&lt;/p&gt;

&lt;p&gt;the dimension of the column space decreases&lt;/p&gt;

&lt;p&gt;\begin{equation}
\text{rank}(X) &amp;lt; p
\end{equation}&lt;/p&gt;

&lt;p&gt;the objective function $S(\beta)$ is no longer strictly convex, and there are infinitely many solutions of OLS.&lt;/p&gt;

&lt;p&gt;Thinking about this in a different, more intuitive way, we would like to estimate the coefficient $\beta_{1}$ as the influence of $x_{1}$ on $y$ without the influence of $x_{2}$. Since the regressors $x_{1}$ and $x_{2}$ are highly correlated, they vary together and the coefficient $\beta_{1}$ is difficult to estimate.&lt;/p&gt;

&lt;p&gt;The OLS method does not solve these problems, as it only minimizes the sum of squared residuals, i.e. the objective function $S(\beta)$.&lt;/p&gt;

&lt;h3 id=&quot;why-does-this-happen&quot;&gt;Why does this happen?&lt;/h3&gt;

&lt;p&gt;In general, we have a linear model&lt;/p&gt;

&lt;p&gt;\begin{equation}
\label{eq: model}
y = X \beta + \epsilon
\end{equation}&lt;/p&gt;

&lt;p&gt;and let for errors $\epsilon$ hold the assumption (Gauss–Markov)&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbb{E}[\epsilon \epsilon^{T}] = \sigma^{2} I
\end{equation}&lt;/p&gt;

&lt;p&gt;the estimator according to the OLS method is&lt;/p&gt;

&lt;p&gt;\begin{equation}
\label{eq: betahat}
\hat{\beta} = (X^{T}X)^{-1} X^{T} y.
\end{equation}&lt;/p&gt;

&lt;p&gt;From \ref{eq: model} and \ref{eq: betahat} we get&lt;/p&gt;

&lt;p&gt;\begin{equation}
\hat{\beta} - \beta = (X^{T}X)^{-1} X^{T} \epsilon
\end{equation}&lt;/p&gt;

&lt;p&gt;therefore, the covariance matrix for $\hat{\beta}$ is&lt;/p&gt;

\[\begin{align}
\mathbb{E}[(\hat{\beta} - \beta) (\hat{\beta} - \beta)^{T}] \nonumber
&amp;amp;= \mathbb{E}[\left((X^{T}X)^{-1} X^{T} \epsilon\right) \left((X^{T}X)^{-1} X^{T} \epsilon\right)^{T}] \nonumber \\[1em]
&amp;amp;= \mathbb{E}[(X^{T}X)^{-1} X^{T} \epsilon \epsilon^{T} X (X^{T}X)^{-1}] \nonumber \\[1em]
&amp;amp;= (X^{T}X)^{-1} X^{T} \mathbb{E}[\epsilon \epsilon^{T}] X (X^{T}X)^{-1} \nonumber \\[1em]
&amp;amp;= \sigma^{2} (X^{T}X)^{-1}
\label{eq: variance}
\end{align}\]

&lt;p&gt;where we took into account that the matrix $(X^{T}X)$ is symmetric and assumed that it is not stochastic and is independent of $\epsilon$. The variance for the coefficient $\hat{\beta}_{k}$ is the $(k, k)$-th element of the covariance matrix.&lt;/p&gt;

&lt;p&gt;The average distance between the estimator $\hat{\beta}$ and the actual $\beta$ is&lt;/p&gt;

\[\begin{align}
\mathbb{E}[(\hat{\beta} - \beta)^{T} (\hat{\beta} - \beta)] \nonumber
&amp;amp;= \mathbb{E}[((X^{T}X)^{-1} X^{T} \epsilon)^{T} ((X^{T}X)^{-1} X^{T} \epsilon)] \nonumber \\[1em]
&amp;amp;= \mathbb{E}[\epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} \epsilon] \nonumber \\[1em]
&amp;amp;= \mathbb{E}[\text{tr}(\epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} \epsilon)] \nonumber \\[1em]
&amp;amp;= \mathbb{E}[\text{tr}(\epsilon \epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} )] \nonumber \\[1em]
&amp;amp;= \sigma^{2} \text{tr}( X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} )] \nonumber \\[1em]
&amp;amp;= \sigma^{2} \text{tr}( X^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1})] \nonumber \\[1em]
&amp;amp;= \sigma^{2} \text{tr}((X^{T}X)^{-1})
\label{eq: distance}
\end{align}\]

&lt;p&gt;where we took into account that the distance is a scalar, so its expected value will be equal to its trace. We then used the fact that the trace is invariant with respect to cyclic permutations&lt;/p&gt;

\[\text{tr}(ABCD) = \text{tr}(DABC)\]

&lt;p&gt;From \ref{eq: variance} and \ref{eq: distance}, we see that both the variance of the estimator and the distance of the estimator from the actual $\beta$ depend on the matrix $(X^{T}X)^{-1}$.&lt;/p&gt;

&lt;p&gt;The reason why the variance of the estimator and the distance of the estimator from the actual $\beta$ become large, can be shown conveniently with the help of singular value decomposition. Let&lt;/p&gt;

&lt;p&gt;\begin{equation}
X = U \Sigma V^{T}
\end{equation}&lt;/p&gt;

&lt;p&gt;be the singular value decomposition of $X$ where $\Sigma$ contains all the singular values&lt;/p&gt;

&lt;p&gt;\begin{equation}
\sigma_{1} \geq \sigma_{2} \geq \dots \geq \sigma_{p} &amp;gt; 0
\end{equation}&lt;/p&gt;

&lt;p&gt;then&lt;/p&gt;

\[\begin{align}
(X^{T}X)^{-1} &amp;amp;= (V \Sigma^{T} \Sigma V^{T})^{-1} \nonumber \\[1em]
&amp;amp;= (V^{T})^{-1} (\Sigma^{T} \Sigma)^{-1} V^{-1} \nonumber \\[1em]
&amp;amp;= V (\Sigma^{T} \Sigma)^{-1} V^{T} \nonumber \\[1em]
&amp;amp;= \sum_{j = 1}^{p} \frac{1}{\sigma_{j}^{2}} v_{j} v_{j}^{T}
\label{eq: svd}
\end{align}\]

&lt;p&gt;In the limit&lt;/p&gt;

&lt;p&gt;\begin{equation}
| corr(x_{i}, x_{j}) | \rightarrow 1
\end{equation}&lt;/p&gt;

&lt;p&gt;matrix $X$ becomes a singular and the smallest singular value vanishes&lt;/p&gt;

&lt;p&gt;\begin{equation}
\sigma_{p} \rightarrow 0
\end{equation}&lt;/p&gt;

&lt;p&gt;and, from \ref{eq: svd}, also&lt;/p&gt;

&lt;p&gt;\begin{equation}
(X^{T}X)^{-1} \rightarrow \infty
\end{equation}&lt;/p&gt;

&lt;p&gt;Therefore, from \ref{eq: variance} and \ref{eq: distance}, both the variance of the OLS estimator and the distance of the OLS estimator to the actual $\beta$ go to infinity.&lt;/p&gt;</content><author><name>Marko Lalovic</name></author><summary type="html">Figure 1: As the correlation between regressors increases, the OLS method becomes unstable. In the limit $|corr(x_{i}, x_{j})| \rightarrow 1$, the OLS objective function is no longer strictly convex and OLS solution is no longer unique.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://markolalovic.github.io/blog/assets/posts/ols-regression/problem-resized.jpg" /><media:content medium="image" url="https://markolalovic.github.io/blog/assets/posts/ols-regression/problem-resized.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>