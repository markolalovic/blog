<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/blog/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/blog/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav ul li a { background-image: none; } .site-nav > ul.nav-category-list > li > button svg { transform: rotate(-90deg); } .site-nav > ul.nav-category-list > li.nav-list-item > ul.nav-list { display: block; } </style> <script src="/blog/assets/js/vendor/lunr.min.js"></script> <script src="/blog/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="/blog/assets/icons/favicon.ico" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Multicollinearity effect on OLS regression | ML Blog</title> <meta name="generator" content="Jekyll v4.3.3" /> <meta property="og:title" content="Multicollinearity effect on OLS regression" /> <meta name="author" content="Marko Lalovic" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="In ordinary least squares (OLS) regression, for given $X: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$ and $y \in \mathbb{R}^{n}$, we minimize over $\beta \in \mathbb{R}^{p}$ the sum of squared residuals" /> <meta property="og:description" content="In ordinary least squares (OLS) regression, for given $X: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$ and $y \in \mathbb{R}^{n}$, we minimize over $\beta \in \mathbb{R}^{p}$ the sum of squared residuals" /> <link rel="canonical" href="http://localhost:4000/blog/ols-regression/" /> <meta property="og:url" content="http://localhost:4000/blog/ols-regression/" /> <meta property="og:site_name" content="ML Blog" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2019-07-14T00:00:00+02:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Multicollinearity effect on OLS regression" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Marko Lalovic"},"dateModified":"2019-07-14T00:00:00+02:00","datePublished":"2019-07-14T00:00:00+02:00","description":"In ordinary least squares (OLS) regression, for given $X: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{n}$ and $y \\in \\mathbb{R}^{n}$, we minimize over $\\beta \\in \\mathbb{R}^{p}$ the sum of squared residuals","headline":"Multicollinearity effect on OLS regression","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/ols-regression/"},"url":"http://localhost:4000/blog/ols-regression/"}</script> <!-- End Jekyll SEO tag --> <!-- Load dark color theme --> <link id="dark-theme" rel="stylesheet" href="/blog/assets/css/just-the-docs-dark.css" media="(prefers-color-scheme: dark)" /> <!-- Fonts: post-font-family: "Noto Sans" heading-font-family: "Lato" mono-font-family: "Fira Code" --> <link rel="preconnect" href="https://fonts.googleapis.com" /> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /> <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet"> <!-- MathJax config --> <script type="text/javascript"> window.MathJax = { tex: { packages: ["base", "ams", "newcommand", "boldsymbol"], inlineMath: [ ["$", "$"], ["\\(", "\\)"], ], tags: "ams" }, loader: { load: ["ui/menu", "[tex]/ams", "[tex]/newcommand", "[tex]/boldsymbol"], versionWarnings: false, }, }; </script> <!-- MathJax snippet --> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" ></script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div id="side-bar-component" class="side-bar"> <div class="site-header" role="banner"> <a href="/blog/" class="site-title lh-tight"> <p class="my-site-title">ML Blog</p> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"> <!-- Added `reversed` so that posts are sorted by date in reverse --><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --> <a href="/blog/" class="nav-list-link fs-5">Index</a><!-- changed typography: fw-400 fs-4 --></li></ul> <!-- Don't show my Posts collection name --><ul class="nav-list"> <!-- Added `reversed` so that posts are sorted by date in reverse --><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/max-cut-sdp/" class="nav-list-link fs-3 before-pseudo-class">Maximum cut and Goemans-Williamson</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/layout-optimization/" class="nav-list-link fs-3 before-pseudo-class">Layout optimization for a solar tower power plant</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/lasso-dual/" class="nav-list-link fs-3 before-pseudo-class">Lasso dual</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/tda-digits/" class="nav-list-link fs-3 before-pseudo-class">Topological features applied to the MNIST data set</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/water-towers/" class="nav-list-link fs-3 before-pseudo-class">Some math behind water towers</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/covid-calc/" class="nav-list-link fs-3 before-pseudo-class">Covid Calculator</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/direct-approach-to-fdr/" class="nav-list-link fs-3 before-pseudo-class">On Storey's direct approach to false discovery rates</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/ridge-regression/" class="nav-list-link fs-3 before-pseudo-class">Ridge regression</a></li><li class="nav-list-item"><!-- Added some >> pseudo class, but only to my Posts titles --> <!-- Changed typography: fw-400 --><!-- changed typography: fw-400 fs-4 --> <a href="/blog/ols-regression/" class="nav-list-link fs-3 before-pseudo-class">Multicollinearity effect on OLS regression</a></li></ul> </nav> <footer class="site-footer"></footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search ML Blog" aria-label="Search ML Blog" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <button id="toggle-theme-button" name="Swich Theme" class="toggle-btn site-button fw-500 fs-5" onclick="toggleTheme()"> <!-- Sun --> <svg width="24" height="24" id="light-icon" display="none"> <g transform="translate(0,1) scale( 0.9 )"> <circle cx="12" cy="12" r="6" fill="rgb(110, 110, 110)" /> <line id="ray" stroke="rgb(110, 110, 110)" stroke-width="2" stroke-linecap="round" x1="12" y1="1" x2="12" y2="3.3" ></line> <!-- rest of rays are just rotated top ray --> <use href="#ray" transform="rotate(45 12 12)" /> <use href="#ray" transform="rotate(90 12 12)" /> <use href="#ray" transform="rotate(135 12 12)" /> <use href="#ray" transform="rotate(180 12 12)" /> <use href="#ray" transform="rotate(225 12 12)" /> <use href="#ray" transform="rotate(270 12 12)" /> <use href="#ray" transform="rotate(315 12 12)" /> </g> </svg> <!-- Moon --> <svg width="24" height="24" id="dark-icon" display="none"> <g transform="translate(0,1) scale( 0.9 )"> <path fill="rgb(110, 110, 110)" d=" M 18, 4 A 10 10 0 1 0 18, 20 A 10 10 0 0 1 18, 4" /> </g> </svg> </button> </li> <li class="aux-nav-list-item"> <a href="https://lalovic.io" class="site-button fw-500 fs-5"> About </a> </li> </ul> </nav> <!-- Light and dark theme toggle --> <script> var currentTheme = ""; const toggleThemeButton = document.getElementById("toggle-theme-button"); const lightIcon = document.getElementById("light-icon"); const darkIcon = document.getElementById("dark-icon"); const defaultTheme = document.getElementsByTagName("link")[0]; const lightThemeHTML = '<link id="light-theme" rel="stylesheet" href="/blog/assets/css/just-the-docs-light.css"/>'; const darkThemeHTML = '<link id="dark-theme" rel="stylesheet" href="/blog/assets/css/just-the-docs-dark.css"/>'; if ( window.matchMedia && window.matchMedia("(prefers-color-scheme: dark)").matches ) { currentTheme = "dark"; toggleThemeButton.setAttribute("title", "Switch to light mode"); darkIcon.setAttribute("display", "none"); lightIcon.setAttribute("display", "block"); } else { currentTheme = "light"; toggleThemeButton.setAttribute("title", "Switch to dark mode"); darkIcon.setAttribute("display", "block"); lightIcon.setAttribute("display", "none"); } function removeBothThemes() { const lightTheme = document.getElementById("light-theme"); const darkTheme = document.getElementById("dark-theme"); if (lightTheme !== null) { lightTheme.remove(); } if (darkTheme !== null) { darkTheme.remove(); } } function darkOn() { removeBothThemes(); defaultTheme.insertAdjacentHTML("afterend", darkThemeHTML); currentTheme = "dark"; localStorage.setItem("currentThemeLS", "dark"); toggleThemeButton.setAttribute("title", "Switch to light mode"); lightIcon.setAttribute("display", "block"); darkIcon.setAttribute("display", "none"); } function lightOn() { removeBothThemes(); defaultTheme.insertAdjacentHTML("afterend", lightThemeHTML); currentTheme = "light"; localStorage.setItem("currentThemeLS", "light"); toggleThemeButton.setAttribute("title", "Switch to dark mode"); lightIcon.setAttribute("display", "none"); darkIcon.setAttribute("display", "block"); } function toggleTheme() { if (currentTheme == "dark") { lightOn(); } else { darkOn(); } } if (localStorage.getItem("currentThemeLS") !== null) { const currentThemeLS = localStorage.getItem("currentThemeLS"); if (currentThemeLS !== currentTheme) { if (currentThemeLS == "dark") { darkOn(); } else { lightOn(); } } } </script> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 class="fs-7">Multicollinearity effect on OLS regression</h1> <p class="my-post-meta"> Marko Lalovic &nbsp; Jul 14, 2019 </p> <p class="post-content"> <strong>TLDR:</strong> The cause of inflation effect of highly correlated regressors on ordinary least squares estimator. </p> <div class="post-content"> <p>In ordinary least squares (OLS) regression, for given $X: \mathbb{R}^{p} \rightarrow \mathbb{R}^{n}$ and $y \in \mathbb{R}^{n}$, we minimize over $\beta \in \mathbb{R}^{p}$ the sum of squared residuals</p> \[\newcommand{\norm}[1]{\left\lVert#1\right\rVert} S(\beta) = \norm{ y - X\beta }_{2}^{2}\] <p>This blog post illustrates the problem (<a href="#figure1">Figure 1</a>) of using OLS method to estimate the unknown parameters $\beta$ in the case of highly correlated regressors on a simple example using R.</p> <div id="figure1" class="imgcap-nbd"> <img src="/blog/assets/posts/ols-regression/drawing-ols.svg" width="90%" class="invertImg" /> <div class="thecap"> Figure 1: As the correlation between regressors increases, the OLS method becomes unstable. </div> </div> <p>Suppose we have a model</p> \[y \sim \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2}\] <p>where</p> \[\beta_{0} = 3, \quad \beta_{1} = \beta_{2} = 1.\] <p>Let the sample contain 100 elements:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span><span class="w">
</span></code></pre></div></div> <p>and let’s introduce some highly correlated regressors:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">x1</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w">
</span><span class="n">x2</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>with correlation coefficient almost 1:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">0.999962365268769</span><span class="w">
</span></code></pre></div></div> <p>We can run the OLS method 1000 times to get a sense of the effect of highly correlated regressors:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">intr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">))</span><span class="w">
</span><span class="n">nsim</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1000</span><span class="w">
</span><span class="n">betas</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">nsim</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">y</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
    </span><span class="n">xx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">solve</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w">
    </span><span class="n">beta.ols</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.vector</span><span class="p">(</span><span class="n">xx</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="n">beta.ols</span><span class="p">)</span><span class="w">
</span><span class="p">})</span><span class="w">
</span></code></pre></div></div> <p>The estimator for $\beta$, obtained by the OLS method, is still unbiased</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">round</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">),</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">2.996</span><span class="w"> </span><span class="m">1.006</span><span class="w"> </span><span class="m">0.993</span><span class="w">
</span></code></pre></div></div> <p>But the variance is large</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">round</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="p">),</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w">  </span><span class="m">0.101</span><span class="w"> </span><span class="m">11.110</span><span class="w"> </span><span class="m">11.103</span><span class="w">
</span></code></pre></div></div> <p>The estimated coefficients can be very large, some can even have the wrong sign:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">round</span><span class="p">(</span><span class="n">betas</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w">  </span><span class="m">3.002</span><span class="w"> </span><span class="m">-7.673</span><span class="w">  </span><span class="m">9.529</span><span class="w">
</span></code></pre></div></div> <p>The problem can be seen by drawing a contour plot (shown in <a href="#figure1">Figure 1</a>) of the objective function $S(\beta)$:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ssr</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="nf">sum</span><span class="p">((</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">xlen</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span><span class="w">
</span><span class="n">ylen</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span><span class="w">
</span><span class="n">xgrid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-10.1</span><span class="p">,</span><span class="w"> </span><span class="m">10.1</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">xlen</span><span class="p">)</span><span class="w">
</span><span class="n">ygrid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-10.1</span><span class="p">,</span><span class="w"> </span><span class="m">10.1</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ylen</span><span class="p">)</span><span class="w">
</span><span class="n">zvals</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">xlen</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ylen</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">xlen</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">j</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">ylen</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">zvals</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ssr</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">xgrid</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">ygrid</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">xgrid</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ygrid</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">zvals</span><span class="p">,</span><span class="w">
        </span><span class="n">levels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1e3</span><span class="p">,</span><span class="w"> </span><span class="m">3e3</span><span class="p">,</span><span class="w"> </span><span class="m">6e3</span><span class="p">,</span><span class="w"> </span><span class="m">1e4</span><span class="p">))</span><span class="w">
</span></code></pre></div></div> <p>As the correlation between regressors increases, matrix $X$ becomes nearly singular and OLS method becomes unstable. In the limit</p> \[|corr(x_{i}, x_{j})| \rightarrow 1\] <p>the dimension of the column space decreases</p> \[\text{rank}(X) &lt; p.\] <p>The objective function $S(\beta)$ is no longer strictly convex, and there are infinitely many solutions of OLS.</p> <p>Intuitively, we would like to estimate the coefficient $\beta_{1}$ as the influence of $x_{1}$ on $y$ without the influence of $x_{2}$. Since the regressors $x_{1}$ and $x_{2}$ are highly correlated, they vary together and the coefficient $\beta_{1}$ is difficult to estimate. The OLS method does not accomplish this, as it only minimizes the sum of squared residuals, i.e. the objective function $S(\beta)$.</p> <h2 id="why-does-this-happen">Why does this happen?</h2> <p>In general, we have a linear model</p> <p>\begin{equation} \label{eq: model} y = X \beta + \epsilon \end{equation}</p> <p>and let for errors $\epsilon$ hold the assumption (Gauss–Markov):</p> \[\mathbb{E}[\epsilon \epsilon^{T}] = \sigma^{2} I.\] <p>The estimator according to the OLS method is</p> <p>\begin{equation} \label{eq: betahat} \hat{\beta} = (X^{T}X)^{-1} X^{T} y. \end{equation}</p> <p>From \ref{eq: model} and \ref{eq: betahat}, we get</p> \[\hat{\beta} - \beta = (X^{T}X)^{-1} X^{T} \epsilon\] <p>Therefore, the covariance matrix for $\hat{\beta}$ is</p> \[\begin{align} \mathbb{E}[(\hat{\beta} - \beta) (\hat{\beta} - \beta)^{T}] \nonumber &amp;= \mathbb{E}[\left((X^{T}X)^{-1} X^{T} \epsilon\right) \left((X^{T}X)^{-1} X^{T} \epsilon\right)^{T}] \nonumber \\[1em] &amp;= \mathbb{E}[(X^{T}X)^{-1} X^{T} \epsilon \epsilon^{T} X (X^{T}X)^{-1}] \nonumber \\[1em] &amp;= (X^{T}X)^{-1} X^{T} \mathbb{E}[\epsilon \epsilon^{T}] X (X^{T}X)^{-1} \nonumber \\[1em] &amp;= \sigma^{2} (X^{T}X)^{-1} \label{eq: variance} \end{align}\] <p>In the derivation, we took into account that the matrix $X^{T}X$ is symmetric and assumed that it is not stochastic and it is independent of $\epsilon$. The variance for the coefficient $\hat{\beta}_{k}$ is the $(k, k)$-th element of the covariance matrix.</p> <p>The average distance between the estimator $\hat{\beta}$ and the actual $\beta$ is</p> \[\begin{align} \mathbb{E}[(\hat{\beta} - \beta)^{T} (\hat{\beta} - \beta)] \nonumber &amp;= \mathbb{E}[((X^{T}X)^{-1} X^{T} \epsilon)^{T} ((X^{T}X)^{-1} X^{T} \epsilon)] \nonumber \\[1em] &amp;= \mathbb{E}[\epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} \epsilon] \nonumber \\[1em] &amp;= \mathbb{E}[\text{tr}(\epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} \epsilon)] \nonumber \\[1em] &amp;= \mathbb{E}[\text{tr}(\epsilon \epsilon^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} )] \nonumber \\[1em] &amp;= \sigma^{2} \text{tr}( X (X^{T}X)^{-1} (X^{T}X)^{-1} X^{T} )] \nonumber \\[1em] &amp;= \sigma^{2} \text{tr}( X^{T} X (X^{T}X)^{-1} (X^{T}X)^{-1})] \nonumber \\[1em] &amp;= \sigma^{2} \text{tr}((X^{T}X)^{-1}) \label{eq: distance} \end{align}\] <p>In the derivation, we used that the distance is a scalar, so its expected value will be equal to its trace. We then used the fact that the trace is invariant with respect to cyclic permutations:</p> \[\text{tr}(ABCD) = \text{tr}(DABC).\] <p>From \ref{eq: variance} and \ref{eq: distance}, we see that both the variance of the estimator and the distance of the estimator from the actual $\beta$ depend on the matrix $(X^{T}X)^{-1}$.</p> <p>The reason why the variance of the estimator and the distance of the estimator from the actual $\beta$ become large, can be shown conveniently using singular value decomposition. Let</p> \[X = U \Sigma V^{T}\] <p>be the singular value decomposition of $X$ where $\Sigma$ contains all the singular values:</p> \[\sigma_{1} \geq \sigma_{2} \geq \dots \geq \sigma_{p} &gt; 0.\] <p>Then</p> \[\begin{align} (X^{T}X)^{-1} &amp;= (V \Sigma^{T} \Sigma V^{T})^{-1} \nonumber \\[1em] &amp;= (V^{T})^{-1} (\Sigma^{T} \Sigma)^{-1} V^{-1} \nonumber \\[1em] &amp;= V (\Sigma^{T} \Sigma)^{-1} V^{T} \nonumber \\[1em] &amp;= \sum_{j = 1}^{p} \frac{1}{\sigma_{j}^{2}} v_{j} v_{j}^{T} \label{eq: svd} \end{align}\] <p>In the limit</p> \[| corr(x_{i}, x_{j}) | \rightarrow 1\] <p>matrix $X$ becomes singular and the smallest singular value vanishes:</p> \[\sigma_{p} \rightarrow 0\] <p>and from \ref{eq: svd}:</p> \[(X^{T}X)^{-1} \rightarrow \infty\] <p>Therefore, from \ref{eq: variance} and \ref{eq: distance}, both the variance of the $\hat{\beta}$ and the distance of $\hat{\beta}$ to the actual $\beta$ go to infinity as \(| corr(x_{i}, x_{j}) | \rightarrow 1.\)</p> </div> <div> <section class="comments"> <!-- Add comment form --> <div id="respond" class="comment-new"> <h2>Leave a comment</h2> <form id="input-form" method="post" action="https://api.staticforms.xyz/submit"> <input type="hidden" name="accessKey" value="4e5a5ee1-5fd9-47ed-9b85-aecdabd0c006"> <input type="hidden" name="$Date" value="1716071731"> <input type="hidden" name="$BlogPost" value="Multicollinearity effect on OLS regression"> <input type="hidden" name="subject" value="Comment to blog post Sun, 19 May 2024 00:35:31 +0200"> <!-- DEBUG: hard coded redirect_url --> <input type="hidden" name="redirectTo" value="https://lalovic.io/blog/thanks"> <textarea class="wide-field" id="message" name="message" rows="8" cols="68" placeholder="Your comment (LaTeX markup accepted)" required></textarea> <div style="margin-top: 5px; margin-bottom: 5px;"> <input class="input-field" id="name" name="name" type="text" placeholder="Your name (optional)" /> <button class="btn btn-blue" id="submit-button" type="submit">Add Comment</button> </div> </form> <div id="preview-div"></div> <!-- Comment preview and rerenderMath function to apply MathJax to preview --> <script> function previewComment() { const content = document.getElementById("message").value; document.getElementById("comment-result-title").innerText = "Your comment:"; document.getElementById("comment-result").innerText = content; rerenderMath(); } function rerenderMath() { MathJax.typeset(); } var textareaInput = document.getElementById("message"); textareaInput.onkeyup = textareaInput.onkeypress = function() { document.getElementById("preview-div").innerText = this.value; rerenderMath(); } </script> </div> </section> </div> </main> <hr> <footer class="my-footer"> <p><a class="fs-5" href="#top" id="back-to-top">Back to top</a></p> <p class="fs-3 text-grey-dk-000"> CC-BY 2024, Marko Lalovic. Theme is based on <a href="https://just-the-docs.com/">Just the Docs</a>. </p> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
